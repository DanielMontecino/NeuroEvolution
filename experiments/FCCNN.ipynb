{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.codification_cnn import CNNLayer, NNLayer, ChromosomeCNN, FitnessCNN, FitnessCNNParallel\n",
    "from utils.datamanager import DataManager\n",
    "from utils.lr_finder import LRFinder\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_cnn = FitnessCNN()\n",
    "\n",
    "# dataset params:\n",
    "data_folder = '/home/daniel/datasets/MNIST_variations'\n",
    "classes = []\n",
    "\n",
    "# Fitness params\n",
    "epochs = 15\n",
    "batch_size = 128\n",
    "verbose = 1\n",
    "redu_plat = False\n",
    "early_stop = 0\n",
    "warm_up_epochs= 5\n",
    "base_lr = 0.002\n",
    "smooth = 0.1\n",
    "cosine_dec = True\n",
    "lr_find = True\n",
    "\n",
    "dataset = 'MRDBI'\n",
    "dm = DataManager(dataset, clases=classes, folder_var_mnist=data_folder)\n",
    "data = dm.load_data()\n",
    "print(data[0][0].shape)\n",
    "\n",
    "fitness_cnn.set_params(data=data, verbose=verbose, batch_size=batch_size, reduce_plateau=redu_plat,\n",
    "                   epochs=epochs, cosine_decay=cosine_dec, early_stop=early_stop, \n",
    "                   warm_epochs=warm_up_epochs, base_lr=base_lr, smooth_label=smooth, find_lr=lr_find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_2 = CNNLayer(86, (3,5), 'leakyreLu', 0.262, 1)\n",
    "l2_2 = CNNLayer(84, (5,3), 'leakyreLu', 0.319, 1)\n",
    "l3_2 = CNNLayer(243, (1,3), 'prelu', 0.322, 1)\n",
    "l4_2 = NNLayer(948, 'sigmoid', 0.467)\n",
    "l5_2 = NNLayer(780, 'sigmoid', 0.441)\n",
    "base_model = ChromosomeCNN([l1_2, l2_2, l3_2], [l4_2, l5_2], fitness_cnn)\n",
    "\n",
    "l1_1 = CNNLayer(32, (3,3), 'relu', 0.2, 1)\n",
    "l2_1 = CNNLayer(64, (3,3), 'relu', 0.2, 1)\n",
    "l3_1 = NNLayer(512, 'relu', 0.5)\n",
    "l4_1 = NNLayer(512, 'relu', 0.5)\n",
    "simple_model = ChromosomeCNN([l1_1, l2_1], [l3_1, l4_1], fitness_cnn)\n",
    "\n",
    "models = {'simple': simple_model, 'base':base_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.layers import Conv2D, PReLU, LeakyReLU, Dropout, MaxPooling2D, Flatten, Dense, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "from utils.utils import smooth_labels, WarmUpCosineDecayScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "init_conv = True\n",
    "k_sizes_0 = [1,3,5]\n",
    "print(data[0][0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 28, 28, 32)   64          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 28, 28, 64)   2112        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 28, 28, 64)   18496       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 28, 28, 64)   51264       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 28, 28, 64)   100416      conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 28, 28, 256)  0           conv2d_2[0][0]                   \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 28, 28, 256)  1024        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 14, 14, 256)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 14, 14, 256)  0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 14, 14, 128)  295040      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 14, 14, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 7, 7, 128)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 7, 7, 128)    0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 6272)         0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          1605888     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           2570        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,077,386\n",
      "Trainable params: 2,076,618\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "inp = Input(shape=data[0][0][0].shape)\n",
    "x = inp\n",
    "if init_conv:\n",
    "    x = Conv2D(32, 1, activation='relu', padding='same')(x)\n",
    "layer_0 = []\n",
    "for k in k_sizes_0:\n",
    "    layer_0.append(Conv2D(32, k, activation='relu', padding='same')(x))\n",
    "x = concatenate(layer_0)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "layer_1 = []\n",
    "for k in k_sizes_0:\n",
    "    layer_1.append(Conv2D(64, k, activation='relu', padding='same')(x))\n",
    "x = concatenate(layer_1)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "#x = Conv2D(256, 3, activation='relu', padding='same')(x)\n",
    "#x = BatchNormalization()(x)\n",
    "#x = MaxPooling2D()(x)\n",
    "#x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.45)(x)\n",
    "x = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9600 samples, validate on 2400 samples\n",
      "Epoch 1/75\n",
      "9600/9600 [==============================] - 21s 2ms/step - loss: 3.4242 - acc: 0.1284 - val_loss: 2.2332 - val_acc: 0.1837\n",
      "Epoch 2/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 2.2429 - acc: 0.1742 - val_loss: 2.0816 - val_acc: 0.2729\n",
      "Epoch 3/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 2.1134 - acc: 0.2470 - val_loss: 1.8663 - val_acc: 0.3350\n",
      "Epoch 4/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 2.0064 - acc: 0.2984 - val_loss: 1.7979 - val_acc: 0.3396\n",
      "Epoch 5/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 1.9422 - acc: 0.3362 - val_loss: 1.6854 - val_acc: 0.3962\n",
      "Epoch 6/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 1.8652 - acc: 0.3755 - val_loss: 1.5668 - val_acc: 0.4492\n",
      "Epoch 7/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 1.7767 - acc: 0.4288 - val_loss: 1.5249 - val_acc: 0.4633\n",
      "Epoch 8/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 1.6842 - acc: 0.4815 - val_loss: 1.8004 - val_acc: 0.3937\n",
      "Epoch 9/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 1.6136 - acc: 0.5140 - val_loss: 1.5857 - val_acc: 0.4683\n",
      "Epoch 10/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 1.5382 - acc: 0.5567 - val_loss: 1.5333 - val_acc: 0.5087\n",
      "Epoch 11/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 1.4708 - acc: 0.5926 - val_loss: 1.4500 - val_acc: 0.5350\n",
      "Epoch 12/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 1.4052 - acc: 0.6300 - val_loss: 1.2617 - val_acc: 0.6208\n",
      "Epoch 13/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 1.3547 - acc: 0.6566 - val_loss: 1.1744 - val_acc: 0.6600\n",
      "Epoch 14/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 1.2988 - acc: 0.6835 - val_loss: 1.1723 - val_acc: 0.6708\n",
      "Epoch 15/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 1.2397 - acc: 0.7118 - val_loss: 1.0267 - val_acc: 0.7192\n",
      "Epoch 16/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 1.2169 - acc: 0.7217 - val_loss: 1.0598 - val_acc: 0.7188\n",
      "Epoch 17/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 1.1686 - acc: 0.7509 - val_loss: 0.8952 - val_acc: 0.7421\n",
      "Epoch 18/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 1.1335 - acc: 0.7638 - val_loss: 0.8625 - val_acc: 0.7658\n",
      "Epoch 19/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 1.0975 - acc: 0.7877 - val_loss: 0.8073 - val_acc: 0.7792\n",
      "Epoch 20/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 1.0660 - acc: 0.7976 - val_loss: 0.7325 - val_acc: 0.7821\n",
      "Epoch 21/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 1.0384 - acc: 0.8107 - val_loss: 0.7400 - val_acc: 0.7954\n",
      "Epoch 22/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 1.0157 - acc: 0.8260 - val_loss: 0.7319 - val_acc: 0.7975\n",
      "Epoch 23/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 0.9917 - acc: 0.8358 - val_loss: 0.6981 - val_acc: 0.7983\n",
      "Epoch 24/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 0.9584 - acc: 0.8522 - val_loss: 0.6836 - val_acc: 0.7975\n",
      "Epoch 25/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 0.9470 - acc: 0.8595 - val_loss: 0.6563 - val_acc: 0.8071\n",
      "Epoch 26/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 0.9347 - acc: 0.8635 - val_loss: 0.7248 - val_acc: 0.8121\n",
      "Epoch 27/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 0.9173 - acc: 0.8726 - val_loss: 0.6597 - val_acc: 0.8067\n",
      "Epoch 28/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 0.8954 - acc: 0.8795 - val_loss: 0.6534 - val_acc: 0.8133\n",
      "Epoch 29/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 0.8788 - acc: 0.8949 - val_loss: 0.6420 - val_acc: 0.8154\n",
      "Epoch 30/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 0.8698 - acc: 0.8975 - val_loss: 0.6699 - val_acc: 0.8200\n",
      "Epoch 31/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.8461 - acc: 0.9115 - val_loss: 0.6603 - val_acc: 0.8183\n",
      "Epoch 32/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.8365 - acc: 0.9150 - val_loss: 0.6420 - val_acc: 0.8188\n",
      "Epoch 33/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.8249 - acc: 0.9204 - val_loss: 0.6431 - val_acc: 0.8142\n",
      "Epoch 34/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.8183 - acc: 0.9216 - val_loss: 0.6674 - val_acc: 0.8175\n",
      "Epoch 35/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.8095 - acc: 0.9291 - val_loss: 0.6404 - val_acc: 0.8163\n",
      "Epoch 36/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.8024 - acc: 0.9299 - val_loss: 0.6148 - val_acc: 0.8233\n",
      "Epoch 37/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 0.7907 - acc: 0.9400 - val_loss: 0.6393 - val_acc: 0.8183\n",
      "Epoch 38/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.7842 - acc: 0.9406 - val_loss: 0.6342 - val_acc: 0.8250\n",
      "Epoch 39/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.7805 - acc: 0.9411 - val_loss: 0.6316 - val_acc: 0.8217\n",
      "Epoch 40/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.7726 - acc: 0.9463 - val_loss: 0.6339 - val_acc: 0.8188\n",
      "Epoch 41/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.7690 - acc: 0.9459 - val_loss: 0.6323 - val_acc: 0.8246\n",
      "Epoch 42/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.7622 - acc: 0.9506 - val_loss: 0.6396 - val_acc: 0.8225\n",
      "Epoch 43/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.7567 - acc: 0.9496 - val_loss: 0.6606 - val_acc: 0.8233\n",
      "Epoch 44/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.7501 - acc: 0.9527 - val_loss: 0.6252 - val_acc: 0.8279\n",
      "Epoch 45/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.7471 - acc: 0.9550 - val_loss: 0.6469 - val_acc: 0.8196\n",
      "Epoch 46/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.7452 - acc: 0.9568 - val_loss: 0.6408 - val_acc: 0.8263\n",
      "Epoch 47/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.7365 - acc: 0.9610 - val_loss: 0.6280 - val_acc: 0.8296\n",
      "Epoch 48/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.7323 - acc: 0.9631 - val_loss: 0.6291 - val_acc: 0.8329\n",
      "Epoch 49/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.7306 - acc: 0.9633 - val_loss: 0.6226 - val_acc: 0.8296\n",
      "Epoch 50/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.7323 - acc: 0.9624 - val_loss: 0.6220 - val_acc: 0.8337\n",
      "Epoch 51/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.7266 - acc: 0.9615 - val_loss: 0.6344 - val_acc: 0.8337\n",
      "Epoch 52/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 0.7212 - acc: 0.9667 - val_loss: 0.6242 - val_acc: 0.8329\n",
      "Epoch 53/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 0.7209 - acc: 0.9671 - val_loss: 0.6273 - val_acc: 0.8300\n",
      "Epoch 54/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.7152 - acc: 0.9682 - val_loss: 0.6243 - val_acc: 0.8308\n",
      "Epoch 55/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 0.7151 - acc: 0.9675 - val_loss: 0.6242 - val_acc: 0.8292\n",
      "Epoch 56/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.7106 - acc: 0.9710 - val_loss: 0.6239 - val_acc: 0.8313\n",
      "Epoch 57/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.7087 - acc: 0.9712 - val_loss: 0.6186 - val_acc: 0.8337\n",
      "Epoch 58/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 0.7088 - acc: 0.9698 - val_loss: 0.6251 - val_acc: 0.8317\n",
      "Epoch 59/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.7084 - acc: 0.9710 - val_loss: 0.6157 - val_acc: 0.8321\n",
      "Epoch 60/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.7095 - acc: 0.9703 - val_loss: 0.6187 - val_acc: 0.8333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/75\n",
      "9600/9600 [==============================] - 15s 2ms/step - loss: 0.7068 - acc: 0.9715 - val_loss: 0.6222 - val_acc: 0.8329\n",
      "Epoch 62/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 0.7035 - acc: 0.9746 - val_loss: 0.6200 - val_acc: 0.8342\n",
      "Epoch 63/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.7070 - acc: 0.9714 - val_loss: 0.6184 - val_acc: 0.8346\n",
      "Epoch 64/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.7041 - acc: 0.9739 - val_loss: 0.6209 - val_acc: 0.8350\n",
      "Epoch 65/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.7012 - acc: 0.9756 - val_loss: 0.6237 - val_acc: 0.8338\n",
      "Epoch 66/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.6982 - acc: 0.9766 - val_loss: 0.6182 - val_acc: 0.8350\n",
      "Epoch 67/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.6988 - acc: 0.9727 - val_loss: 0.6237 - val_acc: 0.8338\n",
      "Epoch 68/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 0.7002 - acc: 0.9761 - val_loss: 0.6184 - val_acc: 0.8358\n",
      "Epoch 69/75\n",
      "9600/9600 [==============================] - 14s 1ms/step - loss: 0.7009 - acc: 0.9725 - val_loss: 0.6169 - val_acc: 0.8358\n",
      "Epoch 70/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.6982 - acc: 0.9757 - val_loss: 0.6159 - val_acc: 0.8346\n",
      "Epoch 71/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.6980 - acc: 0.9747 - val_loss: 0.6170 - val_acc: 0.8350\n",
      "Epoch 72/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.7015 - acc: 0.9734 - val_loss: 0.6174 - val_acc: 0.8346\n",
      "Epoch 73/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.6965 - acc: 0.9742 - val_loss: 0.6177 - val_acc: 0.8342\n",
      "Epoch 74/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.6992 - acc: 0.9759 - val_loss: 0.6178 - val_acc: 0.8338\n",
      "Epoch 75/75\n",
      "9600/9600 [==============================] - 13s 1ms/step - loss: 0.6988 - acc: 0.9741 - val_loss: 0.6178 - val_acc: 0.8338\n",
      "Test: 0.82778\n"
     ]
    }
   ],
   "source": [
    "epochs = 75\n",
    "warmup_epochs = 5\n",
    "batch_size=256\n",
    "cosine_decay = True\n",
    "\n",
    "callbacks = []\n",
    "# Create the Learning rate scheduler.\n",
    "total_steps = int(epochs * data[0][1].shape[0] / batch_size)\n",
    "warm_up_steps = int(warmup_epochs * data[0][1].shape[0] /batch_size)\n",
    "base_steps = total_steps * (not cosine_decay)\n",
    "warm_up_lr = WarmUpCosineDecayScheduler(learning_rate_base=0.001,\n",
    "                                        total_steps=total_steps,\n",
    "                                        warmup_learning_rate=0.0,\n",
    "                                        warmup_steps=warm_up_steps,\n",
    "                                        hold_base_rate_steps=base_steps)\n",
    "callbacks.append(warm_up_lr)\n",
    "\n",
    "\n",
    "h = model.fit(data[0][0], data[0][1],\n",
    "      batch_size=batch_size,\n",
    "      epochs=epochs,\n",
    "      verbose=1,\n",
    "      validation_data=(data[2][0], data[2][1]),\n",
    "             callbacks=callbacks)\n",
    "print(\"Test:\", model.evaluate(data[1][0], data[1][1], verbose=0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
