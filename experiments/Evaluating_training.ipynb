{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.utils import WarmUpCosineDecayScheduler, CLRScheduler, smooth_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
    "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
    "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
    "# ----------------------------------------------------------------------------\n",
    "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
    "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
    "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
    "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
    "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
    "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
    "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
    "# ---------------------------------------------------------------------------\n",
    "n = 3\n",
    "version = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(valid_split=True, smooth=0):\n",
    "    # Load the CIFAR10 data.\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "    # Input image dimensions.\n",
    "    input_shape = x_train.shape[1:]\n",
    "\n",
    "    # Normalize data.\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "    x_test = x_test.astype('float32') / 255\n",
    "\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    #x_train -= x_train_mean\n",
    "    #x_test -= x_train_mean\n",
    "\n",
    "    # Convert class vectors to binary class matrices.\n",
    "    y_train = keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "    if valid_split:\n",
    "        L = x_train.shape[0]\n",
    "        p = np.arange(L)\n",
    "        np.random.shuffle(p)\n",
    "        valid_fraction = int(L * 0.2)\n",
    "        x_val, y_val     = x_train[0:valid_fraction,...], y_train[0:valid_fraction,...]\n",
    "        x_train, y_train = x_train[valid_fraction::,...], y_train[valid_fraction::,...]\n",
    "    else:\n",
    "        x_val, y_val = x_test, y_test\n",
    "        print(\"Validation set is the test set\")\n",
    "\n",
    "\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print('y_train shape:', y_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "    print(x_val.shape[0], 'validation samples')\n",
    "\n",
    "    # Smooth labels\n",
    "    y_train = smooth_labels(y_train, smooth)\n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer=VarianceScaling(),\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet_v2(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 2 Model builder [b]\n",
    "\n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
    "    bottleneck layer\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same filter map sizes.\n",
    "    Features maps sizes:\n",
    "    conv1  : 32x32,  16\n",
    "    stage 0: 32x32,  64\n",
    "    stage 1: 16x16, 128\n",
    "    stage 2:  8x8,  256\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # Start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def get_model(input_shape):\n",
    "    # Computed depth from supplied model parameter n \n",
    "    if version == 2:\n",
    "        depth = n * 9 + 2\n",
    "        model = resnet_v2(input_shape=input_shape, depth=depth)\n",
    "    else:\n",
    "        depth = n * 6 + 2\n",
    "        model = resnet_v1(input_shape=input_shape, depth=depth)\n",
    "    return model, depth\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = base_lr\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "def get_callbacks(lr_sc, base_lr, train_examples, filepath):\n",
    "    # Prepare callbacks for model saving and for learning rate adjustment.\n",
    "    checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                                 monitor='val_acc',\n",
    "                                 verbose=1,\n",
    "                                 save_best_only=True)\n",
    "    callbacks = [checkpoint]\n",
    "\n",
    "    if lr_sc == 'lr_schedule':\n",
    "        schedule = LearningRateScheduler(lr_schedule)\n",
    "        lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                                       cooldown=0,\n",
    "                                       patience=5,\n",
    "                                       min_lr=0.5e-6)\n",
    "        callbacks += [lr_reducer, schedule]\n",
    "\n",
    "    elif lr_sc == 'cosine':\n",
    "        total_steps = int(epochs * train_examples / batch_size)\n",
    "        warm_up_steps = 0\n",
    "        schedule = WarmUpCosineDecayScheduler(learning_rate_base=base_lr,\n",
    "                                                  total_steps=total_steps,\n",
    "                                                  warmup_learning_rate=0.0,\n",
    "                                                  warmup_steps=warm_up_steps,\n",
    "                                                  hold_base_rate_steps=0)\n",
    "\n",
    "    elif lr_sc == 'warmup_cosine':\n",
    "        total_steps = int(epochs * train_examples / batch_size)\n",
    "        warm_up_steps = int(total_steps / 10)\n",
    "        schedule = WarmUpCosineDecayScheduler(learning_rate_base=base_lr,\n",
    "                                                  total_steps=total_steps,\n",
    "                                                  warmup_learning_rate=0.0,\n",
    "                                                  warmup_steps=warm_up_steps,\n",
    "                                                  hold_base_rate_steps=0)\n",
    "\n",
    "    elif lr_sc == 'clrs':\n",
    "        total_steps = int(epochs * train_examples / batch_size)\n",
    "        schedule = CLRScheduler(max_lr=base_lr,\n",
    "                                    min_lr=0.00002,\n",
    "                                    total_steps=total_steps)\n",
    "    else:\n",
    "        print(lr_sc)\n",
    "\n",
    "    callbacks.append(schedule)\n",
    "    return callbacks\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, callbacks, data, data_augmentation, epochs, batch_size):\n",
    "    (x_train, y_train), (x_val, y_val), (x_test, y_test) = data\n",
    "    # Run training, with or without data augmentation.\n",
    "    if not data_augmentation:\n",
    "        print('Not using data augmentation.')\n",
    "        model.fit(x_train, y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(x_val, y_val),\n",
    "                  shuffle=True,\n",
    "                  callbacks=callbacks)\n",
    "    else:\n",
    "        print('Using real-time data augmentation.')\n",
    "        # This will do preprocessing and realtime data augmentation:\n",
    "        datagen = ImageDataGenerator(\n",
    "            # set input mean to 0 over the dataset\n",
    "            featurewise_center=False,\n",
    "            # set each sample mean to 0\n",
    "            samplewise_center=False,\n",
    "            # divide inputs by std of dataset\n",
    "            featurewise_std_normalization=False,\n",
    "            # divide each input by its std\n",
    "            samplewise_std_normalization=False,\n",
    "            # apply ZCA whitening\n",
    "            zca_whitening=False,\n",
    "            # epsilon for ZCA whitening\n",
    "            zca_epsilon=1e-06,\n",
    "            # randomly rotate images in the range (deg 0 to 180)\n",
    "            rotation_range=0,\n",
    "            # randomly shift images horizontally\n",
    "            width_shift_range=0.1,\n",
    "            # randomly shift images vertically\n",
    "            height_shift_range=0.1,\n",
    "            # set range for random shear\n",
    "            shear_range=0.,\n",
    "            # set range for random zoom\n",
    "            zoom_range=0.,\n",
    "            # set range for random channel shifts\n",
    "            channel_shift_range=0.,\n",
    "            # set mode for filling points outside the input boundaries\n",
    "            fill_mode='nearest',\n",
    "            # value used for fill_mode = \"constant\"\n",
    "            cval=0.,\n",
    "            # randomly flip images\n",
    "            horizontal_flip=True,\n",
    "            # randomly flip images\n",
    "            vertical_flip=False,\n",
    "            # set rescaling factor (applied before any other transformation)\n",
    "            rescale=None,\n",
    "            # set function that will be applied on each input\n",
    "            preprocessing_function=None,\n",
    "            # image data format, either \"channels_first\" or \"channels_last\"\n",
    "            data_format=None,\n",
    "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "            validation_split=0.0)\n",
    "\n",
    "        # Compute quantities required for featurewise normalization\n",
    "        # (std, mean, and principal components if ZCA whitening is applied).\n",
    "        datagen.fit(x_train)\n",
    "\n",
    "        # Fit the model on the batches generated by datagen.flow().\n",
    "        model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                            validation_data=(x_test, y_test),\n",
    "                            epochs=epochs, verbose=1, workers=4,\n",
    "                            callbacks=callbacks,\n",
    "                            steps_per_epoch=int(x_train.shape[0] / batch_size))\n",
    "\n",
    "    # Score trained model.\n",
    "    scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "    return model, callbacks, scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_training(valid_split, data_augmentation, smooth, lr_sc, batch_size, epochs, base_lr, optimizer='Adam'):\n",
    "    keras.backend.clear_session()\n",
    "    data = get_dataset(valid_split=valid_split, smooth=smooth)\n",
    "    model, depth = get_model(data[0][0].shape[1:])\n",
    "    if optimizer == 'Adam':\n",
    "        optimizer = Adam(lr=base_lr)\n",
    "    elif optimizer == 'RMSProp':\n",
    "        optimizer = RMSprop(learning_rate=base_lr)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    # Model name, depth and version\n",
    "    model_type = 'ResNet%dv%d' % (depth, version)\n",
    "    print(model_type)\n",
    "\n",
    "    # Prepare model model saving directory.\n",
    "    save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "    model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "    callbacks = get_callbacks(lr_sc, base_lr, data[0][0].shape[0], filepath)\n",
    "    model, callbacks, score = train_model(model, callbacks, data, data_augmentation, epochs, batch_size)\n",
    "    return model, callbacks, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_split = True\n",
    "data_augmentation = True\n",
    "lr_sc = ['lr_schedule', 'cosine', 'warmup_cosine', 'clrs', 'nasbench'][2]\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 256  # orig paper trained all networks with batch_size=128\n",
    "epochs = 108\n",
    "base_lr = 0.01\n",
    "smooth = 0.1\n",
    "optimizer = 'Adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (40000, 32, 32, 3)\n",
      "y_train shape: (40000, 10)\n",
      "40000 train samples\n",
      "10000 test samples\n",
      "10000 validation samples\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "ResNet20v1\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/108\n",
      "156/156 [==============================] - 13s 86ms/step - loss: 3.6091 - accuracy: 0.1398 - val_loss: 3.0406 - val_accuracy: 0.0999\n",
      "Epoch 2/108\n",
      "  1/156 [..............................] - ETA: 11s - loss: 2.2522 - accuracy: 0.1758"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheverazo/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 10s 61ms/step - loss: 2.2450 - accuracy: 0.1851 - val_loss: 2.6229 - val_accuracy: 0.1745\n",
      "Epoch 3/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 2.1184 - accuracy: 0.2109 - val_loss: 2.2318 - val_accuracy: 0.1495\n",
      "Epoch 4/108\n",
      "156/156 [==============================] - 10s 63ms/step - loss: 2.0672 - accuracy: 0.2356 - val_loss: 2.7168 - val_accuracy: 0.1822\n",
      "Epoch 5/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 2.0176 - accuracy: 0.2535 - val_loss: 5.9020 - val_accuracy: 0.1000\n",
      "Epoch 6/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.9710 - accuracy: 0.2776 - val_loss: 19.7062 - val_accuracy: 0.1241\n",
      "Epoch 7/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.9423 - accuracy: 0.2974 - val_loss: 3.8211 - val_accuracy: 0.2275\n",
      "Epoch 8/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.9099 - accuracy: 0.3125 - val_loss: 2.4496 - val_accuracy: 0.2285\n",
      "Epoch 9/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.9041 - accuracy: 0.3138 - val_loss: 3.4362 - val_accuracy: 0.2002\n",
      "Epoch 10/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.8934 - accuracy: 0.3186 - val_loss: 2.0575 - val_accuracy: 0.2629\n",
      "Epoch 11/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.8890 - accuracy: 0.3207 - val_loss: 3.5680 - val_accuracy: 0.1722\n",
      "Epoch 12/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.8877 - accuracy: 0.3229 - val_loss: 3.6210 - val_accuracy: 0.1603\n",
      "Epoch 13/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8767 - accuracy: 0.3241 - val_loss: 2.3983 - val_accuracy: 0.2309\n",
      "Epoch 14/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.8798 - accuracy: 0.3237 - val_loss: 1.9849 - val_accuracy: 0.2561\n",
      "Epoch 15/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8673 - accuracy: 0.3320 - val_loss: 2.2381 - val_accuracy: 0.2661\n",
      "Epoch 16/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.8601 - accuracy: 0.3379 - val_loss: 4.2333 - val_accuracy: 0.1002\n",
      "Epoch 17/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.8551 - accuracy: 0.3421 - val_loss: 4.2632 - val_accuracy: 0.1371\n",
      "Epoch 18/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.8389 - accuracy: 0.3461 - val_loss: 5.4752 - val_accuracy: 0.1355\n",
      "Epoch 19/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8378 - accuracy: 0.3516 - val_loss: 2.3559 - val_accuracy: 0.2400\n",
      "Epoch 20/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.8287 - accuracy: 0.3539 - val_loss: 2.3762 - val_accuracy: 0.2221\n",
      "Epoch 21/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.8221 - accuracy: 0.3580 - val_loss: 2.5175 - val_accuracy: 0.1956\n",
      "Epoch 22/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8186 - accuracy: 0.3585 - val_loss: 2.8620 - val_accuracy: 0.2164\n",
      "Epoch 23/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8115 - accuracy: 0.3650 - val_loss: 10.8167 - val_accuracy: 0.1108\n",
      "Epoch 24/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.7978 - accuracy: 0.3705 - val_loss: 2.0330 - val_accuracy: 0.2852\n",
      "Epoch 25/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.7943 - accuracy: 0.3682 - val_loss: 3.9118 - val_accuracy: 0.1423\n",
      "Epoch 26/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.7847 - accuracy: 0.3741 - val_loss: 2.5977 - val_accuracy: 0.2609\n",
      "Epoch 27/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7872 - accuracy: 0.3718 - val_loss: 1.9920 - val_accuracy: 0.3289\n",
      "Epoch 28/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.7703 - accuracy: 0.3786 - val_loss: 1.9127 - val_accuracy: 0.3055\n",
      "Epoch 29/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.7738 - accuracy: 0.3785 - val_loss: 3.0854 - val_accuracy: 0.2373\n",
      "Epoch 30/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.7632 - accuracy: 0.3806 - val_loss: 2.9727 - val_accuracy: 0.1954\n",
      "Epoch 31/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.7599 - accuracy: 0.3850 - val_loss: 3.5332 - val_accuracy: 0.1796\n",
      "Epoch 32/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.7516 - accuracy: 0.3855 - val_loss: 3.3674 - val_accuracy: 0.1472\n",
      "Epoch 33/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7457 - accuracy: 0.3864 - val_loss: 3.1552 - val_accuracy: 0.1423\n",
      "Epoch 34/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7408 - accuracy: 0.3910 - val_loss: 2.9336 - val_accuracy: 0.2574\n",
      "Epoch 35/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7371 - accuracy: 0.3903 - val_loss: 3.6563 - val_accuracy: 0.1330\n",
      "Epoch 36/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7333 - accuracy: 0.3902 - val_loss: 1.8951 - val_accuracy: 0.3521\n",
      "Epoch 37/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7240 - accuracy: 0.3974 - val_loss: 4.1773 - val_accuracy: 0.2186\n",
      "Epoch 38/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7157 - accuracy: 0.3948 - val_loss: 2.2135 - val_accuracy: 0.3049\n",
      "Epoch 39/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7207 - accuracy: 0.3946 - val_loss: 2.2122 - val_accuracy: 0.2890\n",
      "Epoch 40/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7072 - accuracy: 0.3966 - val_loss: 2.1648 - val_accuracy: 0.2980\n",
      "Epoch 41/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7011 - accuracy: 0.3979 - val_loss: 2.3423 - val_accuracy: 0.2953\n",
      "Epoch 42/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7004 - accuracy: 0.4051 - val_loss: 3.2663 - val_accuracy: 0.2166\n",
      "Epoch 43/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6939 - accuracy: 0.4038 - val_loss: 2.6790 - val_accuracy: 0.2259\n",
      "Epoch 44/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6815 - accuracy: 0.4070 - val_loss: 2.5796 - val_accuracy: 0.1720\n",
      "Epoch 45/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6756 - accuracy: 0.4107 - val_loss: 2.5225 - val_accuracy: 0.2495\n",
      "Epoch 46/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6714 - accuracy: 0.4102 - val_loss: 1.7869 - val_accuracy: 0.3880\n",
      "Epoch 47/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6668 - accuracy: 0.4139 - val_loss: 2.3781 - val_accuracy: 0.2926\n",
      "Epoch 48/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6551 - accuracy: 0.4183 - val_loss: 1.7494 - val_accuracy: 0.3851\n",
      "Epoch 49/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6548 - accuracy: 0.4149 - val_loss: 2.3199 - val_accuracy: 0.2953\n",
      "Epoch 50/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6547 - accuracy: 0.4157 - val_loss: 1.7262 - val_accuracy: 0.3966\n",
      "Epoch 51/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6449 - accuracy: 0.4163 - val_loss: 1.8474 - val_accuracy: 0.3478\n",
      "Epoch 52/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6360 - accuracy: 0.4206 - val_loss: 1.7823 - val_accuracy: 0.3802\n",
      "Epoch 53/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6328 - accuracy: 0.4218 - val_loss: 2.2069 - val_accuracy: 0.2558\n",
      "Epoch 54/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6268 - accuracy: 0.4245 - val_loss: 2.7164 - val_accuracy: 0.2685\n",
      "Epoch 55/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6126 - accuracy: 0.4282 - val_loss: 3.3287 - val_accuracy: 0.1875\n",
      "Epoch 56/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6069 - accuracy: 0.4288 - val_loss: 2.2630 - val_accuracy: 0.3025\n",
      "Epoch 57/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6094 - accuracy: 0.4261 - val_loss: 1.8465 - val_accuracy: 0.3575\n",
      "Epoch 58/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6011 - accuracy: 0.4310 - val_loss: 1.6519 - val_accuracy: 0.4029\n",
      "Epoch 59/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5916 - accuracy: 0.4328 - val_loss: 1.9506 - val_accuracy: 0.3352\n",
      "Epoch 60/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5850 - accuracy: 0.4360 - val_loss: 9.9532 - val_accuracy: 0.1288\n",
      "Epoch 61/108\n",
      "156/156 [==============================] - 9s 56ms/step - loss: 1.5829 - accuracy: 0.4333 - val_loss: 1.6629 - val_accuracy: 0.3999\n",
      "Epoch 62/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5739 - accuracy: 0.4384 - val_loss: 2.7868 - val_accuracy: 0.2483\n",
      "Epoch 63/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5673 - accuracy: 0.4405 - val_loss: 5.6694 - val_accuracy: 0.1690\n",
      "Epoch 64/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5571 - accuracy: 0.4445 - val_loss: 1.7131 - val_accuracy: 0.3907\n",
      "Epoch 65/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5503 - accuracy: 0.4446 - val_loss: 9.5415 - val_accuracy: 0.1387\n",
      "Epoch 66/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5473 - accuracy: 0.4441 - val_loss: 1.5637 - val_accuracy: 0.4443\n",
      "Epoch 67/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5403 - accuracy: 0.4529 - val_loss: 1.8004 - val_accuracy: 0.3760\n",
      "Epoch 68/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5288 - accuracy: 0.4534 - val_loss: 2.8024 - val_accuracy: 0.2614\n",
      "Epoch 69/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5348 - accuracy: 0.4533 - val_loss: 3.1082 - val_accuracy: 0.2615\n",
      "Epoch 70/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5119 - accuracy: 0.4568 - val_loss: 1.6377 - val_accuracy: 0.4239\n",
      "Epoch 71/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5116 - accuracy: 0.4597 - val_loss: 1.6402 - val_accuracy: 0.4256\n",
      "Epoch 72/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5053 - accuracy: 0.4630 - val_loss: 1.7806 - val_accuracy: 0.3895\n",
      "Epoch 73/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5028 - accuracy: 0.4606 - val_loss: 2.2180 - val_accuracy: 0.3240\n",
      "Epoch 74/108\n",
      "156/156 [==============================] - 9s 56ms/step - loss: 1.5005 - accuracy: 0.4605 - val_loss: 2.5866 - val_accuracy: 0.2644\n",
      "Epoch 75/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4869 - accuracy: 0.4687 - val_loss: 3.5116 - val_accuracy: 0.1954\n",
      "Epoch 76/108\n",
      "156/156 [==============================] - 9s 56ms/step - loss: 1.4903 - accuracy: 0.4677 - val_loss: 1.5468 - val_accuracy: 0.4516\n",
      "Epoch 77/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.4757 - accuracy: 0.4682 - val_loss: 1.5429 - val_accuracy: 0.4438\n",
      "Epoch 78/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4762 - accuracy: 0.4733 - val_loss: 2.3977 - val_accuracy: 0.2667\n",
      "Epoch 79/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4580 - accuracy: 0.4762 - val_loss: 1.5651 - val_accuracy: 0.4478\n",
      "Epoch 80/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.4613 - accuracy: 0.4755 - val_loss: 1.7241 - val_accuracy: 0.3944\n",
      "Epoch 81/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4537 - accuracy: 0.4773 - val_loss: 1.6753 - val_accuracy: 0.4146\n",
      "Epoch 82/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4573 - accuracy: 0.4788 - val_loss: 1.5067 - val_accuracy: 0.4721\n",
      "Epoch 83/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4384 - accuracy: 0.4849 - val_loss: 2.1423 - val_accuracy: 0.2998\n",
      "Epoch 84/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4423 - accuracy: 0.4819 - val_loss: 1.5421 - val_accuracy: 0.4402\n",
      "Epoch 85/108\n",
      "156/156 [==============================] - 9s 56ms/step - loss: 1.4312 - accuracy: 0.4881 - val_loss: 1.6619 - val_accuracy: 0.4045\n",
      "Epoch 86/108\n",
      "156/156 [==============================] - 9s 56ms/step - loss: 1.4193 - accuracy: 0.4909 - val_loss: 1.4539 - val_accuracy: 0.4701\n",
      "Epoch 87/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4274 - accuracy: 0.4867 - val_loss: 1.4505 - val_accuracy: 0.4812\n",
      "Epoch 88/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4226 - accuracy: 0.4869 - val_loss: 1.4031 - val_accuracy: 0.4916\n",
      "Epoch 89/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4063 - accuracy: 0.4956 - val_loss: 2.2648 - val_accuracy: 0.3047\n",
      "Epoch 90/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4132 - accuracy: 0.4944 - val_loss: 1.4724 - val_accuracy: 0.4747\n",
      "Epoch 91/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.4043 - accuracy: 0.4917 - val_loss: 2.0004 - val_accuracy: 0.3661\n",
      "Epoch 92/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4008 - accuracy: 0.4944 - val_loss: 1.4514 - val_accuracy: 0.4851\n",
      "Epoch 93/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4000 - accuracy: 0.4940 - val_loss: 1.4752 - val_accuracy: 0.4729\n",
      "Epoch 94/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.3882 - accuracy: 0.5036 - val_loss: 1.4449 - val_accuracy: 0.4814\n",
      "Epoch 95/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.3928 - accuracy: 0.4995 - val_loss: 1.3703 - val_accuracy: 0.5047\n",
      "Epoch 96/108\n",
      "156/156 [==============================] - 9s 56ms/step - loss: 1.3828 - accuracy: 0.5042 - val_loss: 1.7047 - val_accuracy: 0.4110\n",
      "Epoch 97/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.3906 - accuracy: 0.5014 - val_loss: 1.3714 - val_accuracy: 0.5051\n",
      "Epoch 98/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.3744 - accuracy: 0.5029 - val_loss: 1.4733 - val_accuracy: 0.4762\n",
      "Epoch 99/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.3772 - accuracy: 0.5068 - val_loss: 1.4287 - val_accuracy: 0.4808\n",
      "Epoch 100/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.3824 - accuracy: 0.5029 - val_loss: 1.3531 - val_accuracy: 0.5155\n",
      "Epoch 101/108\n",
      "156/156 [==============================] - 9s 56ms/step - loss: 1.3699 - accuracy: 0.5082 - val_loss: 1.3440 - val_accuracy: 0.5173\n",
      "Epoch 102/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.3812 - accuracy: 0.5017 - val_loss: 1.3446 - val_accuracy: 0.5140\n",
      "Epoch 103/108\n",
      "156/156 [==============================] - 9s 56ms/step - loss: 1.3710 - accuracy: 0.5081 - val_loss: 1.3441 - val_accuracy: 0.5170\n",
      "Epoch 104/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.3684 - accuracy: 0.5049 - val_loss: 1.3493 - val_accuracy: 0.5153\n",
      "Epoch 105/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.3729 - accuracy: 0.5080 - val_loss: 1.3542 - val_accuracy: 0.5152\n",
      "Epoch 106/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.3694 - accuracy: 0.5088 - val_loss: 1.3471 - val_accuracy: 0.5167\n",
      "Epoch 107/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.3725 - accuracy: 0.5067 - val_loss: 1.3473 - val_accuracy: 0.5148\n",
      "Epoch 108/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.3629 - accuracy: 0.5092 - val_loss: 1.3476 - val_accuracy: 0.5153\n",
      "10000/10000 [==============================] - 3s 279us/step\n",
      "Test loss: 1.3476186244964599\n",
      "Test accuracy: 0.5152999758720398\n",
      "x_train shape: (40000, 32, 32, 3)\n",
      "y_train shape: (40000, 10)\n",
      "40000 train samples\n",
      "10000 test samples\n",
      "10000 validation samples\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "ResNet20v1\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/108\n",
      "156/156 [==============================] - 13s 82ms/step - loss: 3.7988 - accuracy: 0.1326 - val_loss: 3.4310 - val_accuracy: 0.1464\n",
      "Epoch 2/108\n",
      "  1/156 [..............................] - ETA: 7s - loss: 2.1600 - accuracy: 0.2461"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheverazo/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 10s 61ms/step - loss: 2.2557 - accuracy: 0.2047 - val_loss: 2.4319 - val_accuracy: 0.1807\n",
      "Epoch 3/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 2.1628 - accuracy: 0.2291 - val_loss: 3.3602 - val_accuracy: 0.0982\n",
      "Epoch 4/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 2.0834 - accuracy: 0.2652 - val_loss: 11.0069 - val_accuracy: 0.1020\n",
      "Epoch 5/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 2.0115 - accuracy: 0.2911 - val_loss: 2.3078 - val_accuracy: 0.2061\n",
      "Epoch 6/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.9514 - accuracy: 0.3093 - val_loss: 25.9166 - val_accuracy: 0.1181\n",
      "Epoch 7/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.9338 - accuracy: 0.3145 - val_loss: 2.5549 - val_accuracy: 0.2324\n",
      "Epoch 8/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8976 - accuracy: 0.3316 - val_loss: 1.9684 - val_accuracy: 0.3230\n",
      "Epoch 9/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9021 - accuracy: 0.3420 - val_loss: 2.4432 - val_accuracy: 0.2475\n",
      "Epoch 10/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8788 - accuracy: 0.3472 - val_loss: 3.9125 - val_accuracy: 0.2001\n",
      "Epoch 11/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8576 - accuracy: 0.3531 - val_loss: 2.5555 - val_accuracy: 0.2713\n",
      "Epoch 12/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8493 - accuracy: 0.3556 - val_loss: 2.8427 - val_accuracy: 0.2386\n",
      "Epoch 13/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8361 - accuracy: 0.3614 - val_loss: 3.4870 - val_accuracy: 0.2105\n",
      "Epoch 14/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8266 - accuracy: 0.3720 - val_loss: 1.8488 - val_accuracy: 0.3583\n",
      "Epoch 15/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8245 - accuracy: 0.3699 - val_loss: 4.8108 - val_accuracy: 0.1595\n",
      "Epoch 16/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8178 - accuracy: 0.3720 - val_loss: 8.9216 - val_accuracy: 0.1591\n",
      "Epoch 17/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8169 - accuracy: 0.3740 - val_loss: 1.9148 - val_accuracy: 0.3422\n",
      "Epoch 18/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8075 - accuracy: 0.3753 - val_loss: 1.8072 - val_accuracy: 0.3871\n",
      "Epoch 19/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8053 - accuracy: 0.3805 - val_loss: 4.5872 - val_accuracy: 0.1710\n",
      "Epoch 20/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7954 - accuracy: 0.3772 - val_loss: 2.2348 - val_accuracy: 0.3184\n",
      "Epoch 21/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7853 - accuracy: 0.3831 - val_loss: 1.7998 - val_accuracy: 0.3823\n",
      "Epoch 22/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7899 - accuracy: 0.3815 - val_loss: 3.0559 - val_accuracy: 0.2209\n",
      "Epoch 23/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7860 - accuracy: 0.3818 - val_loss: 4.9192 - val_accuracy: 0.1089\n",
      "Epoch 24/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.7787 - accuracy: 0.3861 - val_loss: 2.3344 - val_accuracy: 0.2684\n",
      "Epoch 25/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7819 - accuracy: 0.3812 - val_loss: 2.5529 - val_accuracy: 0.1988\n",
      "Epoch 26/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7710 - accuracy: 0.3878 - val_loss: 3.1261 - val_accuracy: 0.1556\n",
      "Epoch 27/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.7850 - accuracy: 0.3857 - val_loss: 6.8131 - val_accuracy: 0.1000\n",
      "Epoch 28/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7575 - accuracy: 0.3923 - val_loss: 5.1421 - val_accuracy: 0.1001\n",
      "Epoch 29/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7563 - accuracy: 0.3918 - val_loss: 8.5360 - val_accuracy: 0.1697\n",
      "Epoch 30/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7535 - accuracy: 0.3929 - val_loss: 3.4603 - val_accuracy: 0.2057\n",
      "Epoch 31/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7448 - accuracy: 0.3940 - val_loss: 27.7107 - val_accuracy: 0.1000\n",
      "Epoch 32/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7483 - accuracy: 0.3945 - val_loss: 6.4096 - val_accuracy: 0.1000\n",
      "Epoch 33/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7407 - accuracy: 0.3955 - val_loss: 10.4007 - val_accuracy: 0.1000\n",
      "Epoch 34/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7385 - accuracy: 0.3960 - val_loss: 2.9544 - val_accuracy: 0.1060\n",
      "Epoch 35/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7364 - accuracy: 0.3967 - val_loss: 3.6455 - val_accuracy: 0.1226\n",
      "Epoch 36/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7211 - accuracy: 0.4019 - val_loss: 6.1262 - val_accuracy: 0.1006\n",
      "Epoch 37/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7310 - accuracy: 0.3961 - val_loss: 3.5406 - val_accuracy: 0.1000\n",
      "Epoch 38/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7173 - accuracy: 0.4030 - val_loss: 5.0039 - val_accuracy: 0.0995\n",
      "Epoch 39/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7131 - accuracy: 0.4036 - val_loss: 88.3001 - val_accuracy: 0.1000\n",
      "Epoch 40/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6987 - accuracy: 0.4042 - val_loss: 3.6345 - val_accuracy: 0.1000\n",
      "Epoch 41/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.7062 - accuracy: 0.4035 - val_loss: 10.8412 - val_accuracy: 0.1000\n",
      "Epoch 42/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6953 - accuracy: 0.4123 - val_loss: 3.1192 - val_accuracy: 0.1000\n",
      "Epoch 43/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.6857 - accuracy: 0.4099 - val_loss: 3.7031 - val_accuracy: 0.1000\n",
      "Epoch 44/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.6857 - accuracy: 0.4087 - val_loss: 5.3940 - val_accuracy: 0.1000\n",
      "Epoch 45/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6757 - accuracy: 0.4096 - val_loss: 3.6044 - val_accuracy: 0.1000\n",
      "Epoch 46/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6763 - accuracy: 0.4155 - val_loss: 4.4581 - val_accuracy: 0.1188\n",
      "Epoch 47/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6650 - accuracy: 0.4160 - val_loss: 6.6001 - val_accuracy: 0.1000\n",
      "Epoch 48/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6597 - accuracy: 0.4196 - val_loss: 3.4613 - val_accuracy: 0.1168\n",
      "Epoch 49/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6506 - accuracy: 0.4176 - val_loss: 5.2139 - val_accuracy: 0.1030\n",
      "Epoch 50/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6501 - accuracy: 0.4170 - val_loss: 3.6455 - val_accuracy: 0.1153\n",
      "Epoch 51/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6506 - accuracy: 0.4190 - val_loss: 3.1580 - val_accuracy: 0.1003\n",
      "Epoch 52/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6408 - accuracy: 0.4221 - val_loss: 4.3057 - val_accuracy: 0.1191\n",
      "Epoch 53/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6382 - accuracy: 0.4234 - val_loss: 3.8636 - val_accuracy: 0.1002\n",
      "Epoch 54/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6322 - accuracy: 0.4216 - val_loss: 6.3260 - val_accuracy: 0.1000\n",
      "Epoch 55/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6206 - accuracy: 0.4237 - val_loss: 4.1751 - val_accuracy: 0.1000\n",
      "Epoch 56/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6181 - accuracy: 0.4261 - val_loss: 2.2487 - val_accuracy: 0.2728\n",
      "Epoch 57/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6177 - accuracy: 0.4247 - val_loss: 5.0805 - val_accuracy: 0.1000\n",
      "Epoch 58/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6107 - accuracy: 0.4288 - val_loss: 9.5471 - val_accuracy: 0.1000\n",
      "Epoch 59/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.6044 - accuracy: 0.4319 - val_loss: 8.2236 - val_accuracy: 0.1000\n",
      "Epoch 60/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5908 - accuracy: 0.4361 - val_loss: 3.8581 - val_accuracy: 0.1020\n",
      "Epoch 61/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5881 - accuracy: 0.4357 - val_loss: 6.0315 - val_accuracy: 0.1000\n",
      "Epoch 62/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5909 - accuracy: 0.4309 - val_loss: 5.4277 - val_accuracy: 0.1000\n",
      "Epoch 63/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5782 - accuracy: 0.4367 - val_loss: 1.6614 - val_accuracy: 0.3963\n",
      "Epoch 64/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5803 - accuracy: 0.4368 - val_loss: 5.0144 - val_accuracy: 0.1000\n",
      "Epoch 65/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5685 - accuracy: 0.4383 - val_loss: 8.7912 - val_accuracy: 0.1000\n",
      "Epoch 66/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.5735 - accuracy: 0.4407 - val_loss: 7.0033 - val_accuracy: 0.1000\n",
      "Epoch 67/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5599 - accuracy: 0.4446 - val_loss: 3.0127 - val_accuracy: 0.1635\n",
      "Epoch 68/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5472 - accuracy: 0.4466 - val_loss: 3.9237 - val_accuracy: 0.1000\n",
      "Epoch 69/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5471 - accuracy: 0.4489 - val_loss: 35.4052 - val_accuracy: 0.1110\n",
      "Epoch 70/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5477 - accuracy: 0.4438 - val_loss: 6.1231 - val_accuracy: 0.1000\n",
      "Epoch 71/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5428 - accuracy: 0.4477 - val_loss: 2.3705 - val_accuracy: 0.1966\n",
      "Epoch 72/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5349 - accuracy: 0.4540 - val_loss: 1.8793 - val_accuracy: 0.3390\n",
      "Epoch 73/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5350 - accuracy: 0.4496 - val_loss: 3.8150 - val_accuracy: 0.1000\n",
      "Epoch 74/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5265 - accuracy: 0.4508 - val_loss: 3.6688 - val_accuracy: 0.1000\n",
      "Epoch 75/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5250 - accuracy: 0.4536 - val_loss: 3.3486 - val_accuracy: 0.1520\n",
      "Epoch 76/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5224 - accuracy: 0.4549 - val_loss: 3.2418 - val_accuracy: 0.1194\n",
      "Epoch 77/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5039 - accuracy: 0.4603 - val_loss: 3.2474 - val_accuracy: 0.1656\n",
      "Epoch 78/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5085 - accuracy: 0.4588 - val_loss: 4.0709 - val_accuracy: 0.1000\n",
      "Epoch 79/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5040 - accuracy: 0.4575 - val_loss: 7.9294 - val_accuracy: 0.1000\n",
      "Epoch 80/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4935 - accuracy: 0.4628 - val_loss: 3.7053 - val_accuracy: 0.1000\n",
      "Epoch 81/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4884 - accuracy: 0.4642 - val_loss: 2.7015 - val_accuracy: 0.1965\n",
      "Epoch 82/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4903 - accuracy: 0.4637 - val_loss: 7.4415 - val_accuracy: 0.1000\n",
      "Epoch 83/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4831 - accuracy: 0.4657 - val_loss: 3.2791 - val_accuracy: 0.1183\n",
      "Epoch 84/108\n",
      "156/156 [==============================] - 9s 56ms/step - loss: 1.4818 - accuracy: 0.4694 - val_loss: 2.4711 - val_accuracy: 0.2146\n",
      "Epoch 85/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4710 - accuracy: 0.4709 - val_loss: 4.2100 - val_accuracy: 0.1017\n",
      "Epoch 86/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4720 - accuracy: 0.4701 - val_loss: 3.7625 - val_accuracy: 0.0998\n",
      "Epoch 87/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.4632 - accuracy: 0.4704 - val_loss: 2.0694 - val_accuracy: 0.2843\n",
      "Epoch 88/108\n",
      "156/156 [==============================] - 9s 56ms/step - loss: 1.4664 - accuracy: 0.4737 - val_loss: 4.9824 - val_accuracy: 0.1001\n",
      "Epoch 89/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4619 - accuracy: 0.4707 - val_loss: 4.8717 - val_accuracy: 0.1000\n",
      "Epoch 90/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4498 - accuracy: 0.4752 - val_loss: 1.5892 - val_accuracy: 0.4306\n",
      "Epoch 91/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4571 - accuracy: 0.4730 - val_loss: 3.2096 - val_accuracy: 0.1375\n",
      "Epoch 92/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4495 - accuracy: 0.4756 - val_loss: 1.5438 - val_accuracy: 0.4363\n",
      "Epoch 93/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4414 - accuracy: 0.4800 - val_loss: 1.4508 - val_accuracy: 0.4688\n",
      "Epoch 94/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4398 - accuracy: 0.4831 - val_loss: 1.6323 - val_accuracy: 0.4064\n",
      "Epoch 95/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.4385 - accuracy: 0.4812 - val_loss: 1.6783 - val_accuracy: 0.3963\n",
      "Epoch 96/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4358 - accuracy: 0.4836 - val_loss: 1.7149 - val_accuracy: 0.3858\n",
      "Epoch 97/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4344 - accuracy: 0.4819 - val_loss: 1.4979 - val_accuracy: 0.4624\n",
      "Epoch 98/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4279 - accuracy: 0.4853 - val_loss: 1.4276 - val_accuracy: 0.4837\n",
      "Epoch 99/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.4280 - accuracy: 0.4844 - val_loss: 1.8093 - val_accuracy: 0.3544\n",
      "Epoch 100/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4286 - accuracy: 0.4840 - val_loss: 1.4132 - val_accuracy: 0.4875\n",
      "Epoch 101/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4282 - accuracy: 0.4822 - val_loss: 1.4986 - val_accuracy: 0.4552\n",
      "Epoch 102/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4317 - accuracy: 0.4839 - val_loss: 1.4408 - val_accuracy: 0.4818\n",
      "Epoch 103/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.4171 - accuracy: 0.4879 - val_loss: 1.4434 - val_accuracy: 0.4794\n",
      "Epoch 104/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4290 - accuracy: 0.4860 - val_loss: 1.4049 - val_accuracy: 0.4906\n",
      "Epoch 105/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4223 - accuracy: 0.4850 - val_loss: 1.4177 - val_accuracy: 0.4893\n",
      "Epoch 106/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4273 - accuracy: 0.4862 - val_loss: 1.4201 - val_accuracy: 0.4887\n",
      "Epoch 107/108\n",
      "156/156 [==============================] - 9s 56ms/step - loss: 1.4216 - accuracy: 0.4885 - val_loss: 1.4225 - val_accuracy: 0.4880\n",
      "Epoch 108/108\n",
      "156/156 [==============================] - 9s 56ms/step - loss: 1.4199 - accuracy: 0.4867 - val_loss: 1.4210 - val_accuracy: 0.4892\n",
      "10000/10000 [==============================] - 2s 188us/step\n",
      "Test loss: 1.420961264038086\n",
      "Test accuracy: 0.48919999599456787\n",
      "x_train shape: (40000, 32, 32, 3)\n",
      "y_train shape: (40000, 10)\n",
      "40000 train samples\n",
      "10000 test samples\n",
      "10000 validation samples\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "ResNet20v1\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/108\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 3.4321 - accuracy: 0.1104 - val_loss: 2.9641 - val_accuracy: 0.0860\n",
      "Epoch 2/108\n",
      "  1/156 [..............................] - ETA: 11s - loss: 2.2610 - accuracy: 0.2109"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheverazo/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 10s 63ms/step - loss: 2.3185 - accuracy: 0.1749 - val_loss: 2.3967 - val_accuracy: 0.1483\n",
      "Epoch 3/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 2.1648 - accuracy: 0.1933 - val_loss: 2.7328 - val_accuracy: 0.1864\n",
      "Epoch 4/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 2.0645 - accuracy: 0.2258 - val_loss: 3.3157 - val_accuracy: 0.1107\n",
      "Epoch 5/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.9989 - accuracy: 0.2635 - val_loss: 3.2816 - val_accuracy: 0.1868\n",
      "Epoch 6/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.9634 - accuracy: 0.2833 - val_loss: 2.4561 - val_accuracy: 0.1737\n",
      "Epoch 7/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.9463 - accuracy: 0.2906 - val_loss: 2.4268 - val_accuracy: 0.2382\n",
      "Epoch 8/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.9271 - accuracy: 0.2979 - val_loss: 2.8397 - val_accuracy: 0.2079\n",
      "Epoch 9/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.9250 - accuracy: 0.2995 - val_loss: 4.6262 - val_accuracy: 0.1824\n",
      "Epoch 10/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9159 - accuracy: 0.2998 - val_loss: 2.6117 - val_accuracy: 0.2385\n",
      "Epoch 11/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9079 - accuracy: 0.3052 - val_loss: 4.9480 - val_accuracy: 0.1831\n",
      "Epoch 12/108\n",
      "156/156 [==============================] - 10s 66ms/step - loss: 1.8952 - accuracy: 0.3097 - val_loss: 3.6401 - val_accuracy: 0.1453\n",
      "Epoch 13/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8992 - accuracy: 0.3061 - val_loss: 3.2948 - val_accuracy: 0.2051\n",
      "Epoch 14/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8813 - accuracy: 0.3131 - val_loss: 2.0229 - val_accuracy: 0.2717\n",
      "Epoch 15/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8852 - accuracy: 0.3152 - val_loss: 5.7669 - val_accuracy: 0.1544\n",
      "Epoch 16/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8817 - accuracy: 0.3166 - val_loss: 5.0944 - val_accuracy: 0.1362\n",
      "Epoch 17/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8784 - accuracy: 0.3184 - val_loss: 2.0232 - val_accuracy: 0.2972\n",
      "Epoch 18/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8803 - accuracy: 0.3148 - val_loss: 2.3560 - val_accuracy: 0.2677\n",
      "Epoch 19/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8660 - accuracy: 0.3233 - val_loss: 5.5225 - val_accuracy: 0.1604\n",
      "Epoch 20/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8650 - accuracy: 0.3233 - val_loss: 2.1955 - val_accuracy: 0.2978\n",
      "Epoch 21/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.8642 - accuracy: 0.3246 - val_loss: 3.0031 - val_accuracy: 0.1282\n",
      "Epoch 22/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8591 - accuracy: 0.3265 - val_loss: 2.0561 - val_accuracy: 0.2593\n",
      "Epoch 23/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8616 - accuracy: 0.3267 - val_loss: 2.4180 - val_accuracy: 0.1955\n",
      "Epoch 24/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8491 - accuracy: 0.3310 - val_loss: 2.0599 - val_accuracy: 0.2745\n",
      "Epoch 25/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8411 - accuracy: 0.3340 - val_loss: 1.9008 - val_accuracy: 0.3310\n",
      "Epoch 26/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8427 - accuracy: 0.3368 - val_loss: 1.8417 - val_accuracy: 0.3415\n",
      "Epoch 27/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8324 - accuracy: 0.3397 - val_loss: 1.8729 - val_accuracy: 0.3028\n",
      "Epoch 28/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8375 - accuracy: 0.3412 - val_loss: 2.1782 - val_accuracy: 0.3081\n",
      "Epoch 29/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8255 - accuracy: 0.3421 - val_loss: 2.3908 - val_accuracy: 0.2697\n",
      "Epoch 30/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8210 - accuracy: 0.3463 - val_loss: 2.5279 - val_accuracy: 0.1930\n",
      "Epoch 31/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8213 - accuracy: 0.3461 - val_loss: 1.8170 - val_accuracy: 0.3495\n",
      "Epoch 32/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8161 - accuracy: 0.3455 - val_loss: 3.3129 - val_accuracy: 0.2162\n",
      "Epoch 33/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8151 - accuracy: 0.3483 - val_loss: 2.0652 - val_accuracy: 0.3244\n",
      "Epoch 34/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8046 - accuracy: 0.3559 - val_loss: 1.9773 - val_accuracy: 0.2948\n",
      "Epoch 35/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.7993 - accuracy: 0.3501 - val_loss: 4.0919 - val_accuracy: 0.2012\n",
      "Epoch 36/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7950 - accuracy: 0.3514 - val_loss: 2.6231 - val_accuracy: 0.1770\n",
      "Epoch 37/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7841 - accuracy: 0.3586 - val_loss: 3.6734 - val_accuracy: 0.1305\n",
      "Epoch 38/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7780 - accuracy: 0.3629 - val_loss: 2.6121 - val_accuracy: 0.1993\n",
      "Epoch 39/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7779 - accuracy: 0.3600 - val_loss: 1.9522 - val_accuracy: 0.3393\n",
      "Epoch 40/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7673 - accuracy: 0.3631 - val_loss: 2.0169 - val_accuracy: 0.3170\n",
      "Epoch 41/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7671 - accuracy: 0.3640 - val_loss: 2.7080 - val_accuracy: 0.2267\n",
      "Epoch 42/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7588 - accuracy: 0.3666 - val_loss: 1.9423 - val_accuracy: 0.3341\n",
      "Epoch 43/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7503 - accuracy: 0.3676 - val_loss: 2.1941 - val_accuracy: 0.2452\n",
      "Epoch 44/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7515 - accuracy: 0.3710 - val_loss: 1.7386 - val_accuracy: 0.3754\n",
      "Epoch 45/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7439 - accuracy: 0.3720 - val_loss: 3.2779 - val_accuracy: 0.1987\n",
      "Epoch 46/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7400 - accuracy: 0.3709 - val_loss: 1.8703 - val_accuracy: 0.3095\n",
      "Epoch 47/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7270 - accuracy: 0.3771 - val_loss: 2.4618 - val_accuracy: 0.2783\n",
      "Epoch 48/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7172 - accuracy: 0.3781 - val_loss: 4.6936 - val_accuracy: 0.1586\n",
      "Epoch 49/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7190 - accuracy: 0.3823 - val_loss: 4.3279 - val_accuracy: 0.1660\n",
      "Epoch 50/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7130 - accuracy: 0.3792 - val_loss: 1.7824 - val_accuracy: 0.3567\n",
      "Epoch 51/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.7056 - accuracy: 0.3848 - val_loss: 1.7791 - val_accuracy: 0.3452\n",
      "Epoch 52/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7032 - accuracy: 0.3884 - val_loss: 1.9953 - val_accuracy: 0.2817\n",
      "Epoch 53/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.6975 - accuracy: 0.3859 - val_loss: 2.1874 - val_accuracy: 0.3182\n",
      "Epoch 54/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6857 - accuracy: 0.3940 - val_loss: 1.9056 - val_accuracy: 0.3310\n",
      "Epoch 55/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6830 - accuracy: 0.3947 - val_loss: 2.6601 - val_accuracy: 0.2316\n",
      "Epoch 56/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6676 - accuracy: 0.3963 - val_loss: 2.0991 - val_accuracy: 0.3140\n",
      "Epoch 57/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6725 - accuracy: 0.3964 - val_loss: 1.8234 - val_accuracy: 0.3130\n",
      "Epoch 58/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6580 - accuracy: 0.4034 - val_loss: 2.4365 - val_accuracy: 0.2696\n",
      "Epoch 59/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6546 - accuracy: 0.4019 - val_loss: 1.7679 - val_accuracy: 0.3727\n",
      "Epoch 60/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6545 - accuracy: 0.3976 - val_loss: 1.9009 - val_accuracy: 0.3359\n",
      "Epoch 61/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6391 - accuracy: 0.4040 - val_loss: 6.9349 - val_accuracy: 0.1382\n",
      "Epoch 62/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.6406 - accuracy: 0.4048 - val_loss: 2.1611 - val_accuracy: 0.3068\n",
      "Epoch 63/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6441 - accuracy: 0.4060 - val_loss: 2.7393 - val_accuracy: 0.2184\n",
      "Epoch 64/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6223 - accuracy: 0.4092 - val_loss: 2.0057 - val_accuracy: 0.3075\n",
      "Epoch 65/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6246 - accuracy: 0.4097 - val_loss: 2.9695 - val_accuracy: 0.2094\n",
      "Epoch 66/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6121 - accuracy: 0.4173 - val_loss: 1.8093 - val_accuracy: 0.3354\n",
      "Epoch 67/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6038 - accuracy: 0.4166 - val_loss: 4.4754 - val_accuracy: 0.1747\n",
      "Epoch 68/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6061 - accuracy: 0.4162 - val_loss: 1.7321 - val_accuracy: 0.3570\n",
      "Epoch 69/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5980 - accuracy: 0.4139 - val_loss: 2.6392 - val_accuracy: 0.3004\n",
      "Epoch 70/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5909 - accuracy: 0.4189 - val_loss: 1.6884 - val_accuracy: 0.3844\n",
      "Epoch 71/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5919 - accuracy: 0.4200 - val_loss: 1.7522 - val_accuracy: 0.3513\n",
      "Epoch 72/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5781 - accuracy: 0.4253 - val_loss: 1.7913 - val_accuracy: 0.3424\n",
      "Epoch 73/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5759 - accuracy: 0.4251 - val_loss: 2.3305 - val_accuracy: 0.2367\n",
      "Epoch 74/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5687 - accuracy: 0.4261 - val_loss: 1.6539 - val_accuracy: 0.3881\n",
      "Epoch 75/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5635 - accuracy: 0.4292 - val_loss: 2.1972 - val_accuracy: 0.3285\n",
      "Epoch 76/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5636 - accuracy: 0.4277 - val_loss: 2.3344 - val_accuracy: 0.2252\n",
      "Epoch 77/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5550 - accuracy: 0.4300 - val_loss: 1.7342 - val_accuracy: 0.3835\n",
      "Epoch 78/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5407 - accuracy: 0.4331 - val_loss: 1.8169 - val_accuracy: 0.3451\n",
      "Epoch 79/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5390 - accuracy: 0.4370 - val_loss: 1.6897 - val_accuracy: 0.4025\n",
      "Epoch 80/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.5374 - accuracy: 0.4382 - val_loss: 2.2961 - val_accuracy: 0.2812\n",
      "Epoch 81/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5408 - accuracy: 0.4409 - val_loss: 1.8758 - val_accuracy: 0.3107\n",
      "Epoch 82/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5247 - accuracy: 0.4448 - val_loss: 2.0343 - val_accuracy: 0.2718\n",
      "Epoch 83/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5211 - accuracy: 0.4459 - val_loss: 2.3859 - val_accuracy: 0.2901\n",
      "Epoch 84/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5152 - accuracy: 0.4486 - val_loss: 1.6116 - val_accuracy: 0.4034\n",
      "Epoch 85/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.5037 - accuracy: 0.4518 - val_loss: 1.8503 - val_accuracy: 0.3232\n",
      "Epoch 86/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.5064 - accuracy: 0.4499 - val_loss: 1.7022 - val_accuracy: 0.3651\n",
      "Epoch 87/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5109 - accuracy: 0.4488 - val_loss: 3.0534 - val_accuracy: 0.2365\n",
      "Epoch 88/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4932 - accuracy: 0.4529 - val_loss: 2.0099 - val_accuracy: 0.2910\n",
      "Epoch 89/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.4829 - accuracy: 0.4600 - val_loss: 1.4790 - val_accuracy: 0.4535\n",
      "Epoch 90/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4902 - accuracy: 0.4553 - val_loss: 1.6944 - val_accuracy: 0.3986\n",
      "Epoch 91/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4939 - accuracy: 0.4527 - val_loss: 2.3942 - val_accuracy: 0.2984\n",
      "Epoch 92/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4814 - accuracy: 0.4598 - val_loss: 1.5745 - val_accuracy: 0.4248\n",
      "Epoch 93/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4755 - accuracy: 0.4626 - val_loss: 1.6786 - val_accuracy: 0.3927\n",
      "Epoch 94/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4791 - accuracy: 0.4604 - val_loss: 1.4699 - val_accuracy: 0.4549\n",
      "Epoch 95/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4700 - accuracy: 0.4615 - val_loss: 1.4564 - val_accuracy: 0.4704\n",
      "Epoch 96/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4686 - accuracy: 0.4612 - val_loss: 1.8487 - val_accuracy: 0.3451\n",
      "Epoch 97/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.4620 - accuracy: 0.4676 - val_loss: 2.0244 - val_accuracy: 0.3318\n",
      "Epoch 98/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4651 - accuracy: 0.4643 - val_loss: 1.7569 - val_accuracy: 0.3587\n",
      "Epoch 99/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4639 - accuracy: 0.4671 - val_loss: 1.6617 - val_accuracy: 0.3825\n",
      "Epoch 100/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4691 - accuracy: 0.4646 - val_loss: 1.9155 - val_accuracy: 0.3338\n",
      "Epoch 101/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4591 - accuracy: 0.4686 - val_loss: 1.7542 - val_accuracy: 0.3606\n",
      "Epoch 102/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4579 - accuracy: 0.4655 - val_loss: 1.4841 - val_accuracy: 0.4521\n",
      "Epoch 103/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4553 - accuracy: 0.4694 - val_loss: 1.4511 - val_accuracy: 0.4697\n",
      "Epoch 104/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4580 - accuracy: 0.4700 - val_loss: 1.4559 - val_accuracy: 0.4633\n",
      "Epoch 105/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4575 - accuracy: 0.4681 - val_loss: 1.4405 - val_accuracy: 0.4747\n",
      "Epoch 106/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4541 - accuracy: 0.4700 - val_loss: 1.4619 - val_accuracy: 0.4606\n",
      "Epoch 107/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.4552 - accuracy: 0.4666 - val_loss: 1.4530 - val_accuracy: 0.4659\n",
      "Epoch 108/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4491 - accuracy: 0.4731 - val_loss: 1.4506 - val_accuracy: 0.4681\n",
      "10000/10000 [==============================] - 2s 247us/step\n",
      "Test loss: 1.4505935321807861\n",
      "Test accuracy: 0.46810001134872437\n",
      "x_train shape: (40000, 32, 32, 3)\n",
      "y_train shape: (40000, 10)\n",
      "40000 train samples\n",
      "10000 test samples\n",
      "10000 validation samples\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "ResNet20v1\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/108\n",
      "156/156 [==============================] - 13s 82ms/step - loss: 3.8322 - accuracy: 0.1267 - val_loss: 2.5978 - val_accuracy: 0.1311\n",
      "Epoch 2/108\n",
      "  1/156 [..............................] - ETA: 10s - loss: 2.9661 - accuracy: 0.1484"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheverazo/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 9s 61ms/step - loss: 2.3843 - accuracy: 0.1754 - val_loss: 2.7346 - val_accuracy: 0.1000\n",
      "Epoch 3/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 2.1257 - accuracy: 0.2126 - val_loss: 2.5633 - val_accuracy: 0.1342\n",
      "Epoch 4/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 2.0922 - accuracy: 0.2372 - val_loss: 3.2498 - val_accuracy: 0.1341\n",
      "Epoch 5/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 2.0247 - accuracy: 0.2658 - val_loss: 3.7948 - val_accuracy: 0.1398\n",
      "Epoch 6/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9571 - accuracy: 0.2925 - val_loss: 2.3603 - val_accuracy: 0.2464\n",
      "Epoch 7/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.9237 - accuracy: 0.3171 - val_loss: 7.8580 - val_accuracy: 0.1000\n",
      "Epoch 8/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.9118 - accuracy: 0.3238 - val_loss: 2.3615 - val_accuracy: 0.2291\n",
      "Epoch 9/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8835 - accuracy: 0.3328 - val_loss: 2.3127 - val_accuracy: 0.2593\n",
      "Epoch 10/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8676 - accuracy: 0.3370 - val_loss: 3.2474 - val_accuracy: 0.1569\n",
      "Epoch 11/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.8642 - accuracy: 0.3410 - val_loss: 11.6815 - val_accuracy: 0.1023\n",
      "Epoch 12/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8689 - accuracy: 0.3407 - val_loss: 2.0719 - val_accuracy: 0.2628\n",
      "Epoch 13/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8544 - accuracy: 0.3444 - val_loss: 9.5212 - val_accuracy: 0.1088\n",
      "Epoch 14/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8470 - accuracy: 0.3507 - val_loss: 3.1850 - val_accuracy: 0.2309\n",
      "Epoch 15/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8397 - accuracy: 0.3527 - val_loss: 2.6187 - val_accuracy: 0.2472\n",
      "Epoch 16/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.8321 - accuracy: 0.3540 - val_loss: 2.0769 - val_accuracy: 0.2919\n",
      "Epoch 17/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8325 - accuracy: 0.3596 - val_loss: 3.3372 - val_accuracy: 0.1506\n",
      "Epoch 18/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.8352 - accuracy: 0.3535 - val_loss: 3.0114 - val_accuracy: 0.1310\n",
      "Epoch 19/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8273 - accuracy: 0.3612 - val_loss: 2.4630 - val_accuracy: 0.2038\n",
      "Epoch 20/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8312 - accuracy: 0.3584 - val_loss: 1.8978 - val_accuracy: 0.3418\n",
      "Epoch 21/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8159 - accuracy: 0.3643 - val_loss: 3.7103 - val_accuracy: 0.1822\n",
      "Epoch 22/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8153 - accuracy: 0.3657 - val_loss: 4.5986 - val_accuracy: 0.1667\n",
      "Epoch 23/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8174 - accuracy: 0.3580 - val_loss: 2.3300 - val_accuracy: 0.2213\n",
      "Epoch 24/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8057 - accuracy: 0.3656 - val_loss: 1.9697 - val_accuracy: 0.3342\n",
      "Epoch 25/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8052 - accuracy: 0.3688 - val_loss: 1.9848 - val_accuracy: 0.3306\n",
      "Epoch 26/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8069 - accuracy: 0.3657 - val_loss: 2.0380 - val_accuracy: 0.3062\n",
      "Epoch 27/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7856 - accuracy: 0.3713 - val_loss: 1.9379 - val_accuracy: 0.3340\n",
      "Epoch 28/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7964 - accuracy: 0.3688 - val_loss: 4.2034 - val_accuracy: 0.1305\n",
      "Epoch 29/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7994 - accuracy: 0.3705 - val_loss: 2.9089 - val_accuracy: 0.1694\n",
      "Epoch 30/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7719 - accuracy: 0.3744 - val_loss: 1.9274 - val_accuracy: 0.3070\n",
      "Epoch 31/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.7740 - accuracy: 0.3735 - val_loss: 2.2958 - val_accuracy: 0.2563\n",
      "Epoch 32/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7732 - accuracy: 0.3772 - val_loss: 4.0036 - val_accuracy: 0.1862\n",
      "Epoch 33/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.7610 - accuracy: 0.3802 - val_loss: 2.8455 - val_accuracy: 0.1883\n",
      "Epoch 34/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.7582 - accuracy: 0.3795 - val_loss: 2.3645 - val_accuracy: 0.2706\n",
      "Epoch 35/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7612 - accuracy: 0.3805 - val_loss: 5.9339 - val_accuracy: 0.1037\n",
      "Epoch 36/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7459 - accuracy: 0.3806 - val_loss: 3.9959 - val_accuracy: 0.1449\n",
      "Epoch 37/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7393 - accuracy: 0.3818 - val_loss: 1.8340 - val_accuracy: 0.3303\n",
      "Epoch 38/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7386 - accuracy: 0.3851 - val_loss: 1.7922 - val_accuracy: 0.3759\n",
      "Epoch 39/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7290 - accuracy: 0.3856 - val_loss: 4.6160 - val_accuracy: 0.1496\n",
      "Epoch 40/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7325 - accuracy: 0.3848 - val_loss: 2.6814 - val_accuracy: 0.2534\n",
      "Epoch 41/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7233 - accuracy: 0.3893 - val_loss: 2.7415 - val_accuracy: 0.2061\n",
      "Epoch 42/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7189 - accuracy: 0.3895 - val_loss: 3.0858 - val_accuracy: 0.2147\n",
      "Epoch 43/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7078 - accuracy: 0.3949 - val_loss: 2.3315 - val_accuracy: 0.2309\n",
      "Epoch 44/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7068 - accuracy: 0.3932 - val_loss: 2.4010 - val_accuracy: 0.2521\n",
      "Epoch 45/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.6975 - accuracy: 0.3903 - val_loss: 2.6595 - val_accuracy: 0.2463\n",
      "Epoch 46/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6956 - accuracy: 0.3966 - val_loss: 3.1499 - val_accuracy: 0.1856\n",
      "Epoch 47/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.6914 - accuracy: 0.3997 - val_loss: 3.5842 - val_accuracy: 0.2032\n",
      "Epoch 48/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6825 - accuracy: 0.3991 - val_loss: 1.6590 - val_accuracy: 0.4081\n",
      "Epoch 49/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6829 - accuracy: 0.4016 - val_loss: 3.5568 - val_accuracy: 0.1878\n",
      "Epoch 50/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6691 - accuracy: 0.4067 - val_loss: 2.1171 - val_accuracy: 0.3260\n",
      "Epoch 51/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6630 - accuracy: 0.4076 - val_loss: 2.4688 - val_accuracy: 0.2210\n",
      "Epoch 52/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6608 - accuracy: 0.4115 - val_loss: 2.8697 - val_accuracy: 0.2649\n",
      "Epoch 53/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.6453 - accuracy: 0.4157 - val_loss: 3.6574 - val_accuracy: 0.1604\n",
      "Epoch 54/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6420 - accuracy: 0.4125 - val_loss: 2.4137 - val_accuracy: 0.2389\n",
      "Epoch 55/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6381 - accuracy: 0.4162 - val_loss: 2.3152 - val_accuracy: 0.2533\n",
      "Epoch 56/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.6429 - accuracy: 0.4138 - val_loss: 2.5195 - val_accuracy: 0.2319\n",
      "Epoch 57/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6263 - accuracy: 0.4173 - val_loss: 2.9623 - val_accuracy: 0.1799\n",
      "Epoch 58/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6249 - accuracy: 0.4223 - val_loss: 3.1484 - val_accuracy: 0.2095\n",
      "Epoch 59/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6207 - accuracy: 0.4216 - val_loss: 2.1367 - val_accuracy: 0.2921\n",
      "Epoch 60/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6084 - accuracy: 0.4238 - val_loss: 2.2715 - val_accuracy: 0.2601\n",
      "Epoch 61/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.6077 - accuracy: 0.4247 - val_loss: 3.1682 - val_accuracy: 0.2736\n",
      "Epoch 62/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6016 - accuracy: 0.4253 - val_loss: 2.4433 - val_accuracy: 0.2394\n",
      "Epoch 63/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5939 - accuracy: 0.4321 - val_loss: 1.8249 - val_accuracy: 0.3541\n",
      "Epoch 64/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5827 - accuracy: 0.4330 - val_loss: 2.9049 - val_accuracy: 0.1937\n",
      "Epoch 65/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5796 - accuracy: 0.4353 - val_loss: 4.2235 - val_accuracy: 0.1846\n",
      "Epoch 66/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5748 - accuracy: 0.4352 - val_loss: 3.7672 - val_accuracy: 0.1281\n",
      "Epoch 67/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5715 - accuracy: 0.4385 - val_loss: 2.5317 - val_accuracy: 0.3016\n",
      "Epoch 68/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5685 - accuracy: 0.4382 - val_loss: 3.7190 - val_accuracy: 0.2198\n",
      "Epoch 69/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5541 - accuracy: 0.4411 - val_loss: 3.3901 - val_accuracy: 0.2014\n",
      "Epoch 70/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5486 - accuracy: 0.4479 - val_loss: 1.7732 - val_accuracy: 0.3842\n",
      "Epoch 71/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5516 - accuracy: 0.4422 - val_loss: 1.9582 - val_accuracy: 0.3288\n",
      "Epoch 72/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.5453 - accuracy: 0.4437 - val_loss: 1.8106 - val_accuracy: 0.3660\n",
      "Epoch 73/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5328 - accuracy: 0.4493 - val_loss: 1.7709 - val_accuracy: 0.3732\n",
      "Epoch 74/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5366 - accuracy: 0.4483 - val_loss: 2.1183 - val_accuracy: 0.3204\n",
      "Epoch 75/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5245 - accuracy: 0.4516 - val_loss: 1.7006 - val_accuracy: 0.3976\n",
      "Epoch 76/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5189 - accuracy: 0.4560 - val_loss: 2.0056 - val_accuracy: 0.3310\n",
      "Epoch 77/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5118 - accuracy: 0.4583 - val_loss: 1.8125 - val_accuracy: 0.3959\n",
      "Epoch 78/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5074 - accuracy: 0.4592 - val_loss: 3.2747 - val_accuracy: 0.1921\n",
      "Epoch 79/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.5016 - accuracy: 0.4601 - val_loss: 1.9044 - val_accuracy: 0.3444\n",
      "Epoch 80/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.4940 - accuracy: 0.4607 - val_loss: 2.4325 - val_accuracy: 0.2592\n",
      "Epoch 81/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.4981 - accuracy: 0.4601 - val_loss: 1.7243 - val_accuracy: 0.4096\n",
      "Epoch 82/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4905 - accuracy: 0.4626 - val_loss: 2.3878 - val_accuracy: 0.2706\n",
      "Epoch 83/108\n",
      "156/156 [==============================] - 10s 63ms/step - loss: 1.4786 - accuracy: 0.4700 - val_loss: 2.0727 - val_accuracy: 0.3327\n",
      "Epoch 84/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.4763 - accuracy: 0.4678 - val_loss: 2.5111 - val_accuracy: 0.2649\n",
      "Epoch 85/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4688 - accuracy: 0.4761 - val_loss: 2.3836 - val_accuracy: 0.2967\n",
      "Epoch 86/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4656 - accuracy: 0.4742 - val_loss: 1.7347 - val_accuracy: 0.3936\n",
      "Epoch 87/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4542 - accuracy: 0.4762 - val_loss: 2.3453 - val_accuracy: 0.3085\n",
      "Epoch 88/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4542 - accuracy: 0.4777 - val_loss: 1.5018 - val_accuracy: 0.4606\n",
      "Epoch 89/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4520 - accuracy: 0.4783 - val_loss: 1.9125 - val_accuracy: 0.3731\n",
      "Epoch 90/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4476 - accuracy: 0.4808 - val_loss: 1.5659 - val_accuracy: 0.4401\n",
      "Epoch 91/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.4366 - accuracy: 0.4852 - val_loss: 1.7324 - val_accuracy: 0.4004\n",
      "Epoch 92/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.4434 - accuracy: 0.4828 - val_loss: 1.4439 - val_accuracy: 0.4806\n",
      "Epoch 93/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4390 - accuracy: 0.4848 - val_loss: 1.4922 - val_accuracy: 0.4688\n",
      "Epoch 94/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4347 - accuracy: 0.4820 - val_loss: 1.5200 - val_accuracy: 0.4543\n",
      "Epoch 95/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4244 - accuracy: 0.4903 - val_loss: 1.6276 - val_accuracy: 0.4281\n",
      "Epoch 96/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.4256 - accuracy: 0.4875 - val_loss: 1.8228 - val_accuracy: 0.3857\n",
      "Epoch 97/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4238 - accuracy: 0.4908 - val_loss: 1.4503 - val_accuracy: 0.4754\n",
      "Epoch 98/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4207 - accuracy: 0.4906 - val_loss: 1.4496 - val_accuracy: 0.4755\n",
      "Epoch 99/108\n",
      "156/156 [==============================] - 10s 63ms/step - loss: 1.4238 - accuracy: 0.4909 - val_loss: 1.4382 - val_accuracy: 0.4769\n",
      "Epoch 100/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4116 - accuracy: 0.4928 - val_loss: 1.3863 - val_accuracy: 0.4945\n",
      "Epoch 101/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.4135 - accuracy: 0.4930 - val_loss: 1.4395 - val_accuracy: 0.4746\n",
      "Epoch 102/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4179 - accuracy: 0.4908 - val_loss: 1.4226 - val_accuracy: 0.4876\n",
      "Epoch 103/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4112 - accuracy: 0.4919 - val_loss: 1.4170 - val_accuracy: 0.4874\n",
      "Epoch 104/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4199 - accuracy: 0.4899 - val_loss: 1.4262 - val_accuracy: 0.4877\n",
      "Epoch 105/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.4058 - accuracy: 0.4971 - val_loss: 1.4157 - val_accuracy: 0.4914\n",
      "Epoch 106/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4118 - accuracy: 0.4923 - val_loss: 1.4195 - val_accuracy: 0.4897\n",
      "Epoch 107/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.4055 - accuracy: 0.4955 - val_loss: 1.4186 - val_accuracy: 0.4903\n",
      "Epoch 108/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.4078 - accuracy: 0.4968 - val_loss: 1.4106 - val_accuracy: 0.4918\n",
      "10000/10000 [==============================] - 2s 231us/step\n",
      "Test loss: 1.4105834495544434\n",
      "Test accuracy: 0.4918000102043152\n",
      "x_train shape: (40000, 32, 32, 3)\n",
      "y_train shape: (40000, 10)\n",
      "40000 train samples\n",
      "10000 test samples\n",
      "10000 validation samples\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "ResNet20v1\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/108\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 3.4066 - accuracy: 0.1051 - val_loss: 2.8756 - val_accuracy: 0.1751\n",
      "Epoch 2/108\n",
      "  1/156 [..............................] - ETA: 11s - loss: 2.5732 - accuracy: 0.1094"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheverazo/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 9s 60ms/step - loss: 2.2861 - accuracy: 0.1745 - val_loss: 4.3592 - val_accuracy: 0.1104\n",
      "Epoch 3/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 2.0954 - accuracy: 0.2212 - val_loss: 2.4079 - val_accuracy: 0.1439\n",
      "Epoch 4/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 2.0277 - accuracy: 0.2520 - val_loss: 4.1344 - val_accuracy: 0.1531\n",
      "Epoch 5/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.9947 - accuracy: 0.2699 - val_loss: 2.3662 - val_accuracy: 0.1911\n",
      "Epoch 6/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.9571 - accuracy: 0.2891 - val_loss: 1.9194 - val_accuracy: 0.2852\n",
      "Epoch 7/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9380 - accuracy: 0.2937 - val_loss: 3.1822 - val_accuracy: 0.1723\n",
      "Epoch 8/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9336 - accuracy: 0.3025 - val_loss: 1.9369 - val_accuracy: 0.3141\n",
      "Epoch 9/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.9313 - accuracy: 0.3014 - val_loss: 9.4101 - val_accuracy: 0.1288\n",
      "Epoch 10/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9205 - accuracy: 0.3059 - val_loss: 2.0277 - val_accuracy: 0.3030\n",
      "Epoch 11/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.9175 - accuracy: 0.3104 - val_loss: 3.8868 - val_accuracy: 0.1905\n",
      "Epoch 12/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9098 - accuracy: 0.3104 - val_loss: 2.2400 - val_accuracy: 0.2494\n",
      "Epoch 13/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9034 - accuracy: 0.3159 - val_loss: 2.1304 - val_accuracy: 0.2800\n",
      "Epoch 14/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9005 - accuracy: 0.3168 - val_loss: 2.0410 - val_accuracy: 0.2767\n",
      "Epoch 15/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8965 - accuracy: 0.3206 - val_loss: 2.0585 - val_accuracy: 0.2509\n",
      "Epoch 16/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8961 - accuracy: 0.3173 - val_loss: 2.2438 - val_accuracy: 0.2570\n",
      "Epoch 17/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8915 - accuracy: 0.3216 - val_loss: 2.0198 - val_accuracy: 0.2588\n",
      "Epoch 18/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8879 - accuracy: 0.3249 - val_loss: 5.0899 - val_accuracy: 0.1310\n",
      "Epoch 19/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8834 - accuracy: 0.3252 - val_loss: 3.0226 - val_accuracy: 0.2270\n",
      "Epoch 20/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8762 - accuracy: 0.3267 - val_loss: 3.0305 - val_accuracy: 0.1386\n",
      "Epoch 21/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8736 - accuracy: 0.3242 - val_loss: 2.1904 - val_accuracy: 0.2735\n",
      "Epoch 22/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8750 - accuracy: 0.3239 - val_loss: 2.1033 - val_accuracy: 0.2965\n",
      "Epoch 23/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8785 - accuracy: 0.3250 - val_loss: 2.2119 - val_accuracy: 0.2432\n",
      "Epoch 24/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8619 - accuracy: 0.3263 - val_loss: 2.2480 - val_accuracy: 0.2628\n",
      "Epoch 25/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8664 - accuracy: 0.3269 - val_loss: 2.0941 - val_accuracy: 0.2422\n",
      "Epoch 26/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8568 - accuracy: 0.3333 - val_loss: 2.1223 - val_accuracy: 0.2271\n",
      "Epoch 27/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.8538 - accuracy: 0.3322 - val_loss: 2.2268 - val_accuracy: 0.2966\n",
      "Epoch 28/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8493 - accuracy: 0.3332 - val_loss: 2.2520 - val_accuracy: 0.2687\n",
      "Epoch 29/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8477 - accuracy: 0.3297 - val_loss: 2.0666 - val_accuracy: 0.2879\n",
      "Epoch 30/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.8443 - accuracy: 0.3341 - val_loss: 3.4710 - val_accuracy: 0.1326\n",
      "Epoch 31/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8425 - accuracy: 0.3372 - val_loss: 2.3327 - val_accuracy: 0.2784\n",
      "Epoch 32/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8330 - accuracy: 0.3372 - val_loss: 2.2086 - val_accuracy: 0.2327\n",
      "Epoch 33/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8351 - accuracy: 0.3344 - val_loss: 1.8526 - val_accuracy: 0.3356\n",
      "Epoch 34/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8279 - accuracy: 0.3427 - val_loss: 3.1664 - val_accuracy: 0.2028\n",
      "Epoch 35/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8257 - accuracy: 0.3390 - val_loss: 1.9750 - val_accuracy: 0.2963\n",
      "Epoch 36/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.8212 - accuracy: 0.3397 - val_loss: 2.6749 - val_accuracy: 0.2348\n",
      "Epoch 37/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8149 - accuracy: 0.3428 - val_loss: 3.1729 - val_accuracy: 0.1586\n",
      "Epoch 38/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8182 - accuracy: 0.3439 - val_loss: 2.2931 - val_accuracy: 0.2376\n",
      "Epoch 39/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8116 - accuracy: 0.3426 - val_loss: 1.8732 - val_accuracy: 0.3244\n",
      "Epoch 40/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.8010 - accuracy: 0.3431 - val_loss: 2.0675 - val_accuracy: 0.2739\n",
      "Epoch 41/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7989 - accuracy: 0.3447 - val_loss: 3.1462 - val_accuracy: 0.1834\n",
      "Epoch 42/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7942 - accuracy: 0.3483 - val_loss: 4.5906 - val_accuracy: 0.1251\n",
      "Epoch 43/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7886 - accuracy: 0.3514 - val_loss: 1.9893 - val_accuracy: 0.2683\n",
      "Epoch 44/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7864 - accuracy: 0.3536 - val_loss: 1.8304 - val_accuracy: 0.3379\n",
      "Epoch 45/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7777 - accuracy: 0.3547 - val_loss: 1.9814 - val_accuracy: 0.2980\n",
      "Epoch 46/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7777 - accuracy: 0.3517 - val_loss: 1.8255 - val_accuracy: 0.3341\n",
      "Epoch 47/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7616 - accuracy: 0.3627 - val_loss: 2.0306 - val_accuracy: 0.2430\n",
      "Epoch 48/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7628 - accuracy: 0.3600 - val_loss: 2.0199 - val_accuracy: 0.2880\n",
      "Epoch 49/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7560 - accuracy: 0.3632 - val_loss: 1.8415 - val_accuracy: 0.3541\n",
      "Epoch 50/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.7554 - accuracy: 0.3622 - val_loss: 1.8639 - val_accuracy: 0.3293\n",
      "Epoch 51/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7484 - accuracy: 0.3685 - val_loss: 1.8439 - val_accuracy: 0.3202\n",
      "Epoch 52/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.7422 - accuracy: 0.3662 - val_loss: 1.9318 - val_accuracy: 0.2848\n",
      "Epoch 53/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7300 - accuracy: 0.3702 - val_loss: 1.8345 - val_accuracy: 0.3285\n",
      "Epoch 54/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7326 - accuracy: 0.3725 - val_loss: 1.9302 - val_accuracy: 0.3401\n",
      "Epoch 55/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.7252 - accuracy: 0.3739 - val_loss: 1.7649 - val_accuracy: 0.3631\n",
      "Epoch 56/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7157 - accuracy: 0.3743 - val_loss: 2.1475 - val_accuracy: 0.2541\n",
      "Epoch 57/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7233 - accuracy: 0.3729 - val_loss: 1.8643 - val_accuracy: 0.3283\n",
      "Epoch 58/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7158 - accuracy: 0.3759 - val_loss: 2.0015 - val_accuracy: 0.2577\n",
      "Epoch 59/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7062 - accuracy: 0.3785 - val_loss: 1.9682 - val_accuracy: 0.2820\n",
      "Epoch 60/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6967 - accuracy: 0.3815 - val_loss: 2.0885 - val_accuracy: 0.2705\n",
      "Epoch 61/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6936 - accuracy: 0.3821 - val_loss: 1.7399 - val_accuracy: 0.3510\n",
      "Epoch 62/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.6914 - accuracy: 0.3852 - val_loss: 1.8449 - val_accuracy: 0.3527\n",
      "Epoch 63/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6905 - accuracy: 0.3890 - val_loss: 1.9141 - val_accuracy: 0.3098\n",
      "Epoch 64/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6860 - accuracy: 0.3867 - val_loss: 1.8512 - val_accuracy: 0.3341\n",
      "Epoch 65/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.6798 - accuracy: 0.3903 - val_loss: 2.7290 - val_accuracy: 0.2230\n",
      "Epoch 66/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.6754 - accuracy: 0.3901 - val_loss: 2.0787 - val_accuracy: 0.2796\n",
      "Epoch 67/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6694 - accuracy: 0.3944 - val_loss: 2.3726 - val_accuracy: 0.2595\n",
      "Epoch 68/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6621 - accuracy: 0.3950 - val_loss: 1.7569 - val_accuracy: 0.3583\n",
      "Epoch 69/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6637 - accuracy: 0.3967 - val_loss: 1.9252 - val_accuracy: 0.3100\n",
      "Epoch 70/108\n",
      "156/156 [==============================] - 10s 64ms/step - loss: 1.6503 - accuracy: 0.3993 - val_loss: 1.6845 - val_accuracy: 0.3695\n",
      "Epoch 71/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6524 - accuracy: 0.4000 - val_loss: 2.4919 - val_accuracy: 0.2275\n",
      "Epoch 72/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6482 - accuracy: 0.3975 - val_loss: 1.6488 - val_accuracy: 0.3848\n",
      "Epoch 73/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6433 - accuracy: 0.3994 - val_loss: 1.7552 - val_accuracy: 0.3513\n",
      "Epoch 74/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6333 - accuracy: 0.4065 - val_loss: 2.4584 - val_accuracy: 0.2520\n",
      "Epoch 75/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6293 - accuracy: 0.4069 - val_loss: 2.0058 - val_accuracy: 0.2986\n",
      "Epoch 76/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6263 - accuracy: 0.4082 - val_loss: 1.7324 - val_accuracy: 0.3555\n",
      "Epoch 77/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6181 - accuracy: 0.4096 - val_loss: 1.7380 - val_accuracy: 0.3653\n",
      "Epoch 78/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6201 - accuracy: 0.4088 - val_loss: 1.6758 - val_accuracy: 0.3862\n",
      "Epoch 79/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6134 - accuracy: 0.4120 - val_loss: 1.9071 - val_accuracy: 0.3301\n",
      "Epoch 80/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6125 - accuracy: 0.4124 - val_loss: 1.6884 - val_accuracy: 0.3828\n",
      "Epoch 81/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6087 - accuracy: 0.4105 - val_loss: 1.6752 - val_accuracy: 0.3794\n",
      "Epoch 82/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5981 - accuracy: 0.4150 - val_loss: 1.8823 - val_accuracy: 0.3023\n",
      "Epoch 83/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5912 - accuracy: 0.4180 - val_loss: 1.8150 - val_accuracy: 0.3417\n",
      "Epoch 84/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5952 - accuracy: 0.4203 - val_loss: 1.5757 - val_accuracy: 0.4102\n",
      "Epoch 85/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5937 - accuracy: 0.4186 - val_loss: 1.6224 - val_accuracy: 0.4007\n",
      "Epoch 86/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5832 - accuracy: 0.4192 - val_loss: 1.6173 - val_accuracy: 0.3999\n",
      "Epoch 87/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.5814 - accuracy: 0.4200 - val_loss: 1.6650 - val_accuracy: 0.3715\n",
      "Epoch 88/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5794 - accuracy: 0.4250 - val_loss: 1.6520 - val_accuracy: 0.3859\n",
      "Epoch 89/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.5648 - accuracy: 0.4291 - val_loss: 1.5661 - val_accuracy: 0.4110\n",
      "Epoch 90/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5696 - accuracy: 0.4261 - val_loss: 1.6001 - val_accuracy: 0.4022\n",
      "Epoch 91/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5622 - accuracy: 0.4293 - val_loss: 1.6374 - val_accuracy: 0.3960\n",
      "Epoch 92/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.5658 - accuracy: 0.4318 - val_loss: 1.5445 - val_accuracy: 0.4231\n",
      "Epoch 93/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5622 - accuracy: 0.4283 - val_loss: 1.6386 - val_accuracy: 0.3976\n",
      "Epoch 94/108\n",
      "156/156 [==============================] - 9s 56ms/step - loss: 1.5611 - accuracy: 0.4299 - val_loss: 1.6047 - val_accuracy: 0.4004\n",
      "Epoch 95/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5512 - accuracy: 0.4352 - val_loss: 1.5773 - val_accuracy: 0.4206\n",
      "Epoch 96/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5530 - accuracy: 0.4308 - val_loss: 1.5364 - val_accuracy: 0.4247\n",
      "Epoch 97/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5490 - accuracy: 0.4346 - val_loss: 1.5844 - val_accuracy: 0.4187\n",
      "Epoch 98/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5424 - accuracy: 0.4343 - val_loss: 1.5432 - val_accuracy: 0.4246\n",
      "Epoch 99/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.5506 - accuracy: 0.4346 - val_loss: 1.5965 - val_accuracy: 0.4190\n",
      "Epoch 100/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5443 - accuracy: 0.4349 - val_loss: 1.5195 - val_accuracy: 0.4336\n",
      "Epoch 101/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5430 - accuracy: 0.4368 - val_loss: 1.5267 - val_accuracy: 0.4336\n",
      "Epoch 102/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5444 - accuracy: 0.4332 - val_loss: 1.5581 - val_accuracy: 0.4176\n",
      "Epoch 103/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5381 - accuracy: 0.4375 - val_loss: 1.5458 - val_accuracy: 0.4283\n",
      "Epoch 104/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5452 - accuracy: 0.4343 - val_loss: 1.5551 - val_accuracy: 0.4308\n",
      "Epoch 105/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5362 - accuracy: 0.4380 - val_loss: 1.5334 - val_accuracy: 0.4318\n",
      "Epoch 106/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5404 - accuracy: 0.4386 - val_loss: 1.5377 - val_accuracy: 0.4344\n",
      "Epoch 107/108\n",
      "156/156 [==============================] - 9s 56ms/step - loss: 1.5415 - accuracy: 0.4391 - val_loss: 1.5381 - val_accuracy: 0.4336\n",
      "Epoch 108/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.5396 - accuracy: 0.4390 - val_loss: 1.5374 - val_accuracy: 0.4331\n",
      "10000/10000 [==============================] - 2s 204us/step\n",
      "Test loss: 1.537421519088745\n",
      "Test accuracy: 0.43309998512268066\n",
      "x_train shape: (40000, 32, 32, 3)\n",
      "y_train shape: (40000, 10)\n",
      "40000 train samples\n",
      "10000 test samples\n",
      "10000 validation samples\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "ResNet20v1\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/108\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 2.1137 - accuracy: 0.2871 - val_loss: 1.8941 - val_accuracy: 0.3472\n",
      "Epoch 2/108\n",
      "  1/156 [..............................] - ETA: 10s - loss: 1.7805 - accuracy: 0.3789"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheverazo/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 9s 61ms/step - loss: 1.6231 - accuracy: 0.4478 - val_loss: 1.8167 - val_accuracy: 0.4042\n",
      "Epoch 3/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.4010 - accuracy: 0.5409 - val_loss: 1.3872 - val_accuracy: 0.5474\n",
      "Epoch 4/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.2658 - accuracy: 0.5984 - val_loss: 1.3265 - val_accuracy: 0.5870\n",
      "Epoch 5/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1635 - accuracy: 0.6413 - val_loss: 1.5777 - val_accuracy: 0.5263\n",
      "Epoch 6/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.0874 - accuracy: 0.6753 - val_loss: 1.8051 - val_accuracy: 0.5369\n",
      "Epoch 7/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.0265 - accuracy: 0.7013 - val_loss: 1.1437 - val_accuracy: 0.6683\n",
      "Epoch 8/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9917 - accuracy: 0.7190 - val_loss: 1.1257 - val_accuracy: 0.6912\n",
      "Epoch 9/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9622 - accuracy: 0.7336 - val_loss: 1.3165 - val_accuracy: 0.6199\n",
      "Epoch 10/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9351 - accuracy: 0.7413 - val_loss: 1.3855 - val_accuracy: 0.6003\n",
      "Epoch 11/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.9145 - accuracy: 0.7526 - val_loss: 1.4694 - val_accuracy: 0.6017\n",
      "Epoch 12/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9039 - accuracy: 0.7553 - val_loss: 1.4845 - val_accuracy: 0.5680\n",
      "Epoch 13/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8909 - accuracy: 0.7612 - val_loss: 1.1599 - val_accuracy: 0.6753\n",
      "Epoch 14/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8781 - accuracy: 0.7660 - val_loss: 1.3085 - val_accuracy: 0.6595\n",
      "Epoch 15/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8646 - accuracy: 0.7709 - val_loss: 1.3788 - val_accuracy: 0.6543\n",
      "Epoch 16/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8457 - accuracy: 0.7763 - val_loss: 1.6768 - val_accuracy: 0.6043\n",
      "Epoch 17/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8450 - accuracy: 0.7768 - val_loss: 1.3421 - val_accuracy: 0.6532\n",
      "Epoch 18/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8385 - accuracy: 0.7789 - val_loss: 1.0391 - val_accuracy: 0.7151\n",
      "Epoch 19/108\n",
      "156/156 [==============================] - 10s 63ms/step - loss: 0.8198 - accuracy: 0.7882 - val_loss: 1.2716 - val_accuracy: 0.6443\n",
      "Epoch 20/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8091 - accuracy: 0.7897 - val_loss: 0.9741 - val_accuracy: 0.7372\n",
      "Epoch 21/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8032 - accuracy: 0.7901 - val_loss: 3.2484 - val_accuracy: 0.4239\n",
      "Epoch 22/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8061 - accuracy: 0.7930 - val_loss: 1.1711 - val_accuracy: 0.6974\n",
      "Epoch 23/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7849 - accuracy: 0.7981 - val_loss: 1.5757 - val_accuracy: 0.6259\n",
      "Epoch 24/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.7840 - accuracy: 0.7981 - val_loss: 1.1326 - val_accuracy: 0.6892\n",
      "Epoch 25/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7829 - accuracy: 0.7979 - val_loss: 1.0137 - val_accuracy: 0.7457\n",
      "Epoch 26/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7663 - accuracy: 0.8033 - val_loss: 0.9839 - val_accuracy: 0.7413\n",
      "Epoch 27/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7623 - accuracy: 0.8040 - val_loss: 1.6450 - val_accuracy: 0.6091\n",
      "Epoch 28/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7586 - accuracy: 0.8066 - val_loss: 1.3318 - val_accuracy: 0.6477\n",
      "Epoch 29/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7520 - accuracy: 0.8067 - val_loss: 1.0764 - val_accuracy: 0.7216\n",
      "Epoch 30/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7513 - accuracy: 0.8077 - val_loss: 1.0405 - val_accuracy: 0.7156\n",
      "Epoch 31/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7391 - accuracy: 0.8143 - val_loss: 1.3092 - val_accuracy: 0.6774\n",
      "Epoch 32/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7314 - accuracy: 0.8162 - val_loss: 1.3516 - val_accuracy: 0.6462\n",
      "Epoch 33/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.7212 - accuracy: 0.8176 - val_loss: 1.0258 - val_accuracy: 0.7373\n",
      "Epoch 34/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7238 - accuracy: 0.8165 - val_loss: 1.6333 - val_accuracy: 0.6328\n",
      "Epoch 35/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7104 - accuracy: 0.8213 - val_loss: 1.6485 - val_accuracy: 0.6058\n",
      "Epoch 36/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7175 - accuracy: 0.8214 - val_loss: 1.4508 - val_accuracy: 0.6475\n",
      "Epoch 37/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7094 - accuracy: 0.8223 - val_loss: 0.8948 - val_accuracy: 0.7700\n",
      "Epoch 38/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6904 - accuracy: 0.8257 - val_loss: 1.5037 - val_accuracy: 0.6294\n",
      "Epoch 39/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6917 - accuracy: 0.8247 - val_loss: 1.1037 - val_accuracy: 0.7170\n",
      "Epoch 40/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.6814 - accuracy: 0.8288 - val_loss: 1.2251 - val_accuracy: 0.7106\n",
      "Epoch 41/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6681 - accuracy: 0.8311 - val_loss: 1.0791 - val_accuracy: 0.7130\n",
      "Epoch 42/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6645 - accuracy: 0.8318 - val_loss: 1.0795 - val_accuracy: 0.7301\n",
      "Epoch 43/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.6617 - accuracy: 0.8353 - val_loss: 1.0002 - val_accuracy: 0.7347\n",
      "Epoch 44/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6532 - accuracy: 0.8362 - val_loss: 0.8214 - val_accuracy: 0.7821\n",
      "Epoch 45/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6453 - accuracy: 0.8378 - val_loss: 0.9423 - val_accuracy: 0.7482\n",
      "Epoch 46/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.6407 - accuracy: 0.8379 - val_loss: 0.9978 - val_accuracy: 0.7286\n",
      "Epoch 47/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6380 - accuracy: 0.8418 - val_loss: 0.8972 - val_accuracy: 0.7597\n",
      "Epoch 48/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6239 - accuracy: 0.8433 - val_loss: 0.9283 - val_accuracy: 0.7496\n",
      "Epoch 49/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6146 - accuracy: 0.8463 - val_loss: 1.0330 - val_accuracy: 0.7345\n",
      "Epoch 50/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6111 - accuracy: 0.8473 - val_loss: 1.4643 - val_accuracy: 0.6488\n",
      "Epoch 51/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5985 - accuracy: 0.8494 - val_loss: 0.9461 - val_accuracy: 0.7570\n",
      "Epoch 52/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5988 - accuracy: 0.8502 - val_loss: 1.5486 - val_accuracy: 0.6450\n",
      "Epoch 53/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5846 - accuracy: 0.8540 - val_loss: 1.0901 - val_accuracy: 0.7366\n",
      "Epoch 54/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.5874 - accuracy: 0.8534 - val_loss: 0.7251 - val_accuracy: 0.8140\n",
      "Epoch 55/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5731 - accuracy: 0.8561 - val_loss: 0.8392 - val_accuracy: 0.7726\n",
      "Epoch 56/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.5698 - accuracy: 0.8559 - val_loss: 0.7639 - val_accuracy: 0.8045\n",
      "Epoch 57/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5593 - accuracy: 0.8592 - val_loss: 1.3798 - val_accuracy: 0.6768\n",
      "Epoch 58/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.5547 - accuracy: 0.8622 - val_loss: 1.1075 - val_accuracy: 0.7088\n",
      "Epoch 59/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.5403 - accuracy: 0.8673 - val_loss: 0.7766 - val_accuracy: 0.7991\n",
      "Epoch 60/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.5371 - accuracy: 0.8652 - val_loss: 0.8334 - val_accuracy: 0.7897\n",
      "Epoch 61/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5288 - accuracy: 0.8676 - val_loss: 1.0107 - val_accuracy: 0.7510\n",
      "Epoch 62/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.5304 - accuracy: 0.8671 - val_loss: 0.8466 - val_accuracy: 0.7636\n",
      "Epoch 63/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5087 - accuracy: 0.8735 - val_loss: 0.7561 - val_accuracy: 0.7951\n",
      "Epoch 64/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5117 - accuracy: 0.8719 - val_loss: 0.6696 - val_accuracy: 0.8204\n",
      "Epoch 65/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.4982 - accuracy: 0.8769 - val_loss: 0.7907 - val_accuracy: 0.7885\n",
      "Epoch 66/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4907 - accuracy: 0.8797 - val_loss: 1.2564 - val_accuracy: 0.7057\n",
      "Epoch 67/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.4768 - accuracy: 0.8814 - val_loss: 0.6803 - val_accuracy: 0.8167\n",
      "Epoch 68/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4838 - accuracy: 0.8790 - val_loss: 0.6931 - val_accuracy: 0.8206\n",
      "Epoch 69/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4693 - accuracy: 0.8819 - val_loss: 0.8206 - val_accuracy: 0.7974\n",
      "Epoch 70/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.4690 - accuracy: 0.8859 - val_loss: 0.7586 - val_accuracy: 0.8030\n",
      "Epoch 71/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.4546 - accuracy: 0.8878 - val_loss: 0.8066 - val_accuracy: 0.8018\n",
      "Epoch 72/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.4430 - accuracy: 0.8893 - val_loss: 0.6737 - val_accuracy: 0.8230\n",
      "Epoch 73/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.4428 - accuracy: 0.8901 - val_loss: 0.7487 - val_accuracy: 0.8010\n",
      "Epoch 74/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4279 - accuracy: 0.8944 - val_loss: 0.6825 - val_accuracy: 0.8256\n",
      "Epoch 75/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.4175 - accuracy: 0.8956 - val_loss: 0.6301 - val_accuracy: 0.8347\n",
      "Epoch 76/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.4159 - accuracy: 0.8971 - val_loss: 0.6146 - val_accuracy: 0.8371\n",
      "Epoch 77/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4099 - accuracy: 0.8974 - val_loss: 0.6524 - val_accuracy: 0.8321\n",
      "Epoch 78/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.3957 - accuracy: 0.9036 - val_loss: 0.7548 - val_accuracy: 0.8048\n",
      "Epoch 79/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3941 - accuracy: 0.9024 - val_loss: 0.5906 - val_accuracy: 0.8452\n",
      "Epoch 80/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.3900 - accuracy: 0.9039 - val_loss: 0.5733 - val_accuracy: 0.8501\n",
      "Epoch 81/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.3738 - accuracy: 0.9096 - val_loss: 0.6026 - val_accuracy: 0.8363\n",
      "Epoch 82/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.3718 - accuracy: 0.9103 - val_loss: 0.5774 - val_accuracy: 0.8477\n",
      "Epoch 83/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3637 - accuracy: 0.9122 - val_loss: 0.5264 - val_accuracy: 0.8613\n",
      "Epoch 84/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3578 - accuracy: 0.9138 - val_loss: 0.6211 - val_accuracy: 0.8379\n",
      "Epoch 85/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3504 - accuracy: 0.9166 - val_loss: 0.6355 - val_accuracy: 0.8367\n",
      "Epoch 86/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3420 - accuracy: 0.9170 - val_loss: 0.6267 - val_accuracy: 0.8434\n",
      "Epoch 87/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.3366 - accuracy: 0.9205 - val_loss: 0.5558 - val_accuracy: 0.8586\n",
      "Epoch 88/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.3332 - accuracy: 0.9205 - val_loss: 0.6021 - val_accuracy: 0.8513\n",
      "Epoch 89/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.3277 - accuracy: 0.9221 - val_loss: 0.5316 - val_accuracy: 0.8628\n",
      "Epoch 90/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.3187 - accuracy: 0.9248 - val_loss: 0.5091 - val_accuracy: 0.8713\n",
      "Epoch 91/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3127 - accuracy: 0.9265 - val_loss: 0.5264 - val_accuracy: 0.8681\n",
      "Epoch 92/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.3117 - accuracy: 0.9261 - val_loss: 0.5298 - val_accuracy: 0.8647\n",
      "Epoch 93/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.3047 - accuracy: 0.9272 - val_loss: 0.5217 - val_accuracy: 0.8690\n",
      "Epoch 94/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2954 - accuracy: 0.9325 - val_loss: 0.5336 - val_accuracy: 0.8664\n",
      "Epoch 95/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2977 - accuracy: 0.9314 - val_loss: 0.5156 - val_accuracy: 0.8722\n",
      "Epoch 96/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.2893 - accuracy: 0.9335 - val_loss: 0.5084 - val_accuracy: 0.8767\n",
      "Epoch 97/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2916 - accuracy: 0.9322 - val_loss: 0.4975 - val_accuracy: 0.8734\n",
      "Epoch 98/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.2881 - accuracy: 0.9341 - val_loss: 0.4887 - val_accuracy: 0.8780\n",
      "Epoch 99/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.2783 - accuracy: 0.9379 - val_loss: 0.5118 - val_accuracy: 0.8775\n",
      "Epoch 100/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.2871 - accuracy: 0.9329 - val_loss: 0.4929 - val_accuracy: 0.8787\n",
      "Epoch 101/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2763 - accuracy: 0.9376 - val_loss: 0.4964 - val_accuracy: 0.8785\n",
      "Epoch 102/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2763 - accuracy: 0.9362 - val_loss: 0.4957 - val_accuracy: 0.8794\n",
      "Epoch 103/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2759 - accuracy: 0.9370 - val_loss: 0.4930 - val_accuracy: 0.8798\n",
      "Epoch 104/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2752 - accuracy: 0.9382 - val_loss: 0.4859 - val_accuracy: 0.8795\n",
      "Epoch 105/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2722 - accuracy: 0.9384 - val_loss: 0.4893 - val_accuracy: 0.8803\n",
      "Epoch 106/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2730 - accuracy: 0.9382 - val_loss: 0.4886 - val_accuracy: 0.8797\n",
      "Epoch 107/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2708 - accuracy: 0.9403 - val_loss: 0.4881 - val_accuracy: 0.8805\n",
      "Epoch 108/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2737 - accuracy: 0.9398 - val_loss: 0.4887 - val_accuracy: 0.8802\n",
      "10000/10000 [==============================] - 2s 207us/step\n",
      "Test loss: 0.4886612678527832\n",
      "Test accuracy: 0.8802000284194946\n",
      "x_train shape: (40000, 32, 32, 3)\n",
      "y_train shape: (40000, 10)\n",
      "40000 train samples\n",
      "10000 test samples\n",
      "10000 validation samples\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "ResNet20v1\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/108\n",
      "156/156 [==============================] - 13s 82ms/step - loss: 2.1759 - accuracy: 0.2699 - val_loss: 2.9310 - val_accuracy: 0.2537\n",
      "Epoch 2/108\n",
      "  1/156 [..............................] - ETA: 11s - loss: 1.8197 - accuracy: 0.3359"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheverazo/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 9s 60ms/step - loss: 1.6850 - accuracy: 0.4114 - val_loss: 1.7054 - val_accuracy: 0.4187\n",
      "Epoch 3/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.4688 - accuracy: 0.5052 - val_loss: 1.6226 - val_accuracy: 0.4905\n",
      "Epoch 4/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.3272 - accuracy: 0.5683 - val_loss: 1.7732 - val_accuracy: 0.4503\n",
      "Epoch 5/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.2263 - accuracy: 0.6136 - val_loss: 1.4925 - val_accuracy: 0.5333\n",
      "Epoch 6/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1386 - accuracy: 0.6500 - val_loss: 1.7926 - val_accuracy: 0.5275\n",
      "Epoch 7/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0763 - accuracy: 0.6761 - val_loss: 1.3679 - val_accuracy: 0.5990\n",
      "Epoch 8/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0329 - accuracy: 0.6972 - val_loss: 1.5865 - val_accuracy: 0.5421\n",
      "Epoch 9/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9937 - accuracy: 0.7118 - val_loss: 1.2954 - val_accuracy: 0.6234\n",
      "Epoch 10/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9754 - accuracy: 0.7219 - val_loss: 2.0025 - val_accuracy: 0.5041\n",
      "Epoch 11/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.9429 - accuracy: 0.7334 - val_loss: 1.2876 - val_accuracy: 0.6303\n",
      "Epoch 12/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9270 - accuracy: 0.7413 - val_loss: 1.4630 - val_accuracy: 0.6151\n",
      "Epoch 13/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9021 - accuracy: 0.7477 - val_loss: 1.1152 - val_accuracy: 0.6797\n",
      "Epoch 14/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8955 - accuracy: 0.7533 - val_loss: 1.3387 - val_accuracy: 0.6411\n",
      "Epoch 15/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8819 - accuracy: 0.7568 - val_loss: 1.1531 - val_accuracy: 0.6884\n",
      "Epoch 16/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8693 - accuracy: 0.7626 - val_loss: 1.1684 - val_accuracy: 0.6794\n",
      "Epoch 17/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8671 - accuracy: 0.7662 - val_loss: 1.1537 - val_accuracy: 0.6754\n",
      "Epoch 18/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8511 - accuracy: 0.7705 - val_loss: 2.0033 - val_accuracy: 0.4884\n",
      "Epoch 19/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8381 - accuracy: 0.7752 - val_loss: 1.6730 - val_accuracy: 0.5989\n",
      "Epoch 20/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8390 - accuracy: 0.7757 - val_loss: 1.0548 - val_accuracy: 0.7003\n",
      "Epoch 21/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8156 - accuracy: 0.7855 - val_loss: 3.1530 - val_accuracy: 0.4272\n",
      "Epoch 22/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8158 - accuracy: 0.7858 - val_loss: 1.0998 - val_accuracy: 0.6848\n",
      "Epoch 23/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8122 - accuracy: 0.7852 - val_loss: 1.3390 - val_accuracy: 0.6478\n",
      "Epoch 24/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8082 - accuracy: 0.7875 - val_loss: 1.4209 - val_accuracy: 0.6406\n",
      "Epoch 25/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7950 - accuracy: 0.7915 - val_loss: 0.9081 - val_accuracy: 0.7580\n",
      "Epoch 26/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7929 - accuracy: 0.7919 - val_loss: 1.2408 - val_accuracy: 0.6826\n",
      "Epoch 27/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7779 - accuracy: 0.7969 - val_loss: 1.1344 - val_accuracy: 0.6920\n",
      "Epoch 28/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7720 - accuracy: 0.7993 - val_loss: 1.0548 - val_accuracy: 0.7107\n",
      "Epoch 29/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7729 - accuracy: 0.7991 - val_loss: 1.2955 - val_accuracy: 0.6843\n",
      "Epoch 30/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7549 - accuracy: 0.8027 - val_loss: 0.9707 - val_accuracy: 0.7404\n",
      "Epoch 31/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7578 - accuracy: 0.8043 - val_loss: 1.1125 - val_accuracy: 0.7013\n",
      "Epoch 32/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7538 - accuracy: 0.8048 - val_loss: 1.1262 - val_accuracy: 0.7002\n",
      "Epoch 33/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7421 - accuracy: 0.8081 - val_loss: 1.3087 - val_accuracy: 0.6753\n",
      "Epoch 34/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7311 - accuracy: 0.8127 - val_loss: 1.0514 - val_accuracy: 0.7230\n",
      "Epoch 35/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7289 - accuracy: 0.8124 - val_loss: 0.9983 - val_accuracy: 0.7263\n",
      "Epoch 36/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.7185 - accuracy: 0.8154 - val_loss: 0.9307 - val_accuracy: 0.7479\n",
      "Epoch 37/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7143 - accuracy: 0.8172 - val_loss: 1.0008 - val_accuracy: 0.7188\n",
      "Epoch 38/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7082 - accuracy: 0.8185 - val_loss: 0.9707 - val_accuracy: 0.7423\n",
      "Epoch 39/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7054 - accuracy: 0.8182 - val_loss: 0.8874 - val_accuracy: 0.7544\n",
      "Epoch 40/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6911 - accuracy: 0.8239 - val_loss: 1.5782 - val_accuracy: 0.6198\n",
      "Epoch 41/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6860 - accuracy: 0.8241 - val_loss: 1.0473 - val_accuracy: 0.7175\n",
      "Epoch 42/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6794 - accuracy: 0.8274 - val_loss: 0.9836 - val_accuracy: 0.7462\n",
      "Epoch 43/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6777 - accuracy: 0.8271 - val_loss: 0.9547 - val_accuracy: 0.7524\n",
      "Epoch 44/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6689 - accuracy: 0.8300 - val_loss: 0.9023 - val_accuracy: 0.7546\n",
      "Epoch 45/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6622 - accuracy: 0.8321 - val_loss: 0.8888 - val_accuracy: 0.7624\n",
      "Epoch 46/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6547 - accuracy: 0.8328 - val_loss: 1.0287 - val_accuracy: 0.7232\n",
      "Epoch 47/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6400 - accuracy: 0.8390 - val_loss: 0.9372 - val_accuracy: 0.7576\n",
      "Epoch 48/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6446 - accuracy: 0.8346 - val_loss: 0.7723 - val_accuracy: 0.7918\n",
      "Epoch 49/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6365 - accuracy: 0.8384 - val_loss: 1.0182 - val_accuracy: 0.7386\n",
      "Epoch 50/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6252 - accuracy: 0.8425 - val_loss: 1.0988 - val_accuracy: 0.7218\n",
      "Epoch 51/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6163 - accuracy: 0.8433 - val_loss: 1.0914 - val_accuracy: 0.7157\n",
      "Epoch 52/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.6138 - accuracy: 0.8421 - val_loss: 0.9743 - val_accuracy: 0.7395\n",
      "Epoch 53/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6036 - accuracy: 0.8470 - val_loss: 1.0417 - val_accuracy: 0.7310\n",
      "Epoch 54/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5991 - accuracy: 0.8488 - val_loss: 0.8520 - val_accuracy: 0.7782\n",
      "Epoch 55/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5956 - accuracy: 0.8478 - val_loss: 0.9818 - val_accuracy: 0.7403\n",
      "Epoch 56/108\n",
      "156/156 [==============================] - 10s 64ms/step - loss: 0.5878 - accuracy: 0.8508 - val_loss: 0.7998 - val_accuracy: 0.7866\n",
      "Epoch 57/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.5788 - accuracy: 0.8522 - val_loss: 1.2344 - val_accuracy: 0.6748\n",
      "Epoch 58/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5671 - accuracy: 0.8560 - val_loss: 0.8629 - val_accuracy: 0.7719\n",
      "Epoch 59/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.5620 - accuracy: 0.8598 - val_loss: 0.8948 - val_accuracy: 0.7698\n",
      "Epoch 60/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.5526 - accuracy: 0.8599 - val_loss: 0.9260 - val_accuracy: 0.7659\n",
      "Epoch 61/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5507 - accuracy: 0.8619 - val_loss: 0.7591 - val_accuracy: 0.7980\n",
      "Epoch 62/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.5439 - accuracy: 0.8608 - val_loss: 0.7674 - val_accuracy: 0.7924\n",
      "Epoch 63/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5291 - accuracy: 0.8657 - val_loss: 0.7450 - val_accuracy: 0.8042\n",
      "Epoch 64/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.5256 - accuracy: 0.8663 - val_loss: 0.9798 - val_accuracy: 0.7556\n",
      "Epoch 65/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.5171 - accuracy: 0.8694 - val_loss: 0.7160 - val_accuracy: 0.8179\n",
      "Epoch 66/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5137 - accuracy: 0.8718 - val_loss: 0.7032 - val_accuracy: 0.8125\n",
      "Epoch 67/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.5046 - accuracy: 0.8741 - val_loss: 0.9249 - val_accuracy: 0.7651\n",
      "Epoch 68/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4977 - accuracy: 0.8746 - val_loss: 0.6817 - val_accuracy: 0.8143\n",
      "Epoch 69/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.4903 - accuracy: 0.8746 - val_loss: 0.7351 - val_accuracy: 0.8081\n",
      "Epoch 70/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.4809 - accuracy: 0.8799 - val_loss: 0.6772 - val_accuracy: 0.8192\n",
      "Epoch 71/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.4659 - accuracy: 0.8845 - val_loss: 0.6634 - val_accuracy: 0.8220\n",
      "Epoch 72/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.4702 - accuracy: 0.8826 - val_loss: 0.7649 - val_accuracy: 0.8055\n",
      "Epoch 73/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.4536 - accuracy: 0.8876 - val_loss: 0.7631 - val_accuracy: 0.8009\n",
      "Epoch 74/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4513 - accuracy: 0.8872 - val_loss: 0.6140 - val_accuracy: 0.8338\n",
      "Epoch 75/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.4463 - accuracy: 0.8883 - val_loss: 0.6681 - val_accuracy: 0.8323\n",
      "Epoch 76/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4279 - accuracy: 0.8949 - val_loss: 0.6424 - val_accuracy: 0.8304\n",
      "Epoch 77/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.4237 - accuracy: 0.8941 - val_loss: 0.7455 - val_accuracy: 0.8068\n",
      "Epoch 78/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4186 - accuracy: 0.8961 - val_loss: 0.7977 - val_accuracy: 0.7949\n",
      "Epoch 79/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4090 - accuracy: 0.8980 - val_loss: 0.5845 - val_accuracy: 0.8503\n",
      "Epoch 80/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4082 - accuracy: 0.8995 - val_loss: 0.6875 - val_accuracy: 0.8145\n",
      "Epoch 81/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.3946 - accuracy: 0.9040 - val_loss: 0.6103 - val_accuracy: 0.8425\n",
      "Epoch 82/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.3909 - accuracy: 0.9032 - val_loss: 0.6259 - val_accuracy: 0.8435\n",
      "Epoch 83/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3765 - accuracy: 0.9091 - val_loss: 0.6882 - val_accuracy: 0.8239\n",
      "Epoch 84/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.3738 - accuracy: 0.9077 - val_loss: 0.6640 - val_accuracy: 0.8328\n",
      "Epoch 85/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.3709 - accuracy: 0.9096 - val_loss: 0.6047 - val_accuracy: 0.8464\n",
      "Epoch 86/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.3601 - accuracy: 0.9129 - val_loss: 0.5396 - val_accuracy: 0.8638\n",
      "Epoch 87/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.3552 - accuracy: 0.9133 - val_loss: 0.5958 - val_accuracy: 0.8482\n",
      "Epoch 88/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3539 - accuracy: 0.9152 - val_loss: 0.5481 - val_accuracy: 0.8603\n",
      "Epoch 89/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.3472 - accuracy: 0.9156 - val_loss: 0.5542 - val_accuracy: 0.8574\n",
      "Epoch 90/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3411 - accuracy: 0.9190 - val_loss: 0.5670 - val_accuracy: 0.8548\n",
      "Epoch 91/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3292 - accuracy: 0.9227 - val_loss: 0.5510 - val_accuracy: 0.8597\n",
      "Epoch 92/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.3291 - accuracy: 0.9227 - val_loss: 0.5178 - val_accuracy: 0.8669\n",
      "Epoch 93/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.3236 - accuracy: 0.9228 - val_loss: 0.5420 - val_accuracy: 0.8627\n",
      "Epoch 94/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.3184 - accuracy: 0.9248 - val_loss: 0.5233 - val_accuracy: 0.8699\n",
      "Epoch 95/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3145 - accuracy: 0.9250 - val_loss: 0.5435 - val_accuracy: 0.8646\n",
      "Epoch 96/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.3147 - accuracy: 0.9258 - val_loss: 0.5306 - val_accuracy: 0.8657\n",
      "Epoch 97/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3058 - accuracy: 0.9290 - val_loss: 0.5289 - val_accuracy: 0.8693\n",
      "Epoch 98/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3047 - accuracy: 0.9296 - val_loss: 0.5343 - val_accuracy: 0.8678\n",
      "Epoch 99/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2980 - accuracy: 0.9303 - val_loss: 0.5393 - val_accuracy: 0.8688\n",
      "Epoch 100/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2985 - accuracy: 0.9304 - val_loss: 0.5292 - val_accuracy: 0.8694\n",
      "Epoch 101/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2948 - accuracy: 0.9312 - val_loss: 0.5320 - val_accuracy: 0.8689\n",
      "Epoch 102/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.2929 - accuracy: 0.9322 - val_loss: 0.5218 - val_accuracy: 0.8714\n",
      "Epoch 103/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2923 - accuracy: 0.9335 - val_loss: 0.5211 - val_accuracy: 0.8701\n",
      "Epoch 104/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2901 - accuracy: 0.9344 - val_loss: 0.5249 - val_accuracy: 0.8716\n",
      "Epoch 105/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2898 - accuracy: 0.9353 - val_loss: 0.5233 - val_accuracy: 0.8709\n",
      "Epoch 106/108\n",
      "156/156 [==============================] - 9s 56ms/step - loss: 0.2867 - accuracy: 0.9347 - val_loss: 0.5208 - val_accuracy: 0.8723\n",
      "Epoch 107/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2908 - accuracy: 0.9347 - val_loss: 0.5224 - val_accuracy: 0.8723\n",
      "Epoch 108/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2893 - accuracy: 0.9330 - val_loss: 0.5222 - val_accuracy: 0.8724\n",
      "10000/10000 [==============================] - 2s 173us/step\n",
      "Test loss: 0.5221968387603759\n",
      "Test accuracy: 0.8723999857902527\n",
      "x_train shape: (40000, 32, 32, 3)\n",
      "y_train shape: (40000, 10)\n",
      "40000 train samples\n",
      "10000 test samples\n",
      "10000 validation samples\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "ResNet20v1\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/108\n",
      "156/156 [==============================] - 13s 82ms/step - loss: 2.1913 - accuracy: 0.2785 - val_loss: 2.3210 - val_accuracy: 0.2380\n",
      "Epoch 2/108\n",
      "  1/156 [..............................] - ETA: 7s - loss: 1.8413 - accuracy: 0.3945"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheverazo/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 9s 60ms/step - loss: 1.6710 - accuracy: 0.4294 - val_loss: 1.9202 - val_accuracy: 0.3479\n",
      "Epoch 3/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.4203 - accuracy: 0.5369 - val_loss: 1.7090 - val_accuracy: 0.4477\n",
      "Epoch 4/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.2707 - accuracy: 0.6005 - val_loss: 1.6656 - val_accuracy: 0.4752\n",
      "Epoch 5/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1570 - accuracy: 0.6492 - val_loss: 1.7717 - val_accuracy: 0.5163\n",
      "Epoch 6/108\n",
      "156/156 [==============================] - 10s 63ms/step - loss: 1.0843 - accuracy: 0.6843 - val_loss: 1.3380 - val_accuracy: 0.5948\n",
      "Epoch 7/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.0335 - accuracy: 0.7038 - val_loss: 1.3716 - val_accuracy: 0.6297\n",
      "Epoch 8/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9910 - accuracy: 0.7238 - val_loss: 1.3377 - val_accuracy: 0.6338\n",
      "Epoch 9/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9668 - accuracy: 0.7329 - val_loss: 1.4316 - val_accuracy: 0.5727\n",
      "Epoch 10/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9370 - accuracy: 0.7412 - val_loss: 1.2566 - val_accuracy: 0.6626\n",
      "Epoch 11/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9180 - accuracy: 0.7511 - val_loss: 1.7994 - val_accuracy: 0.5523\n",
      "Epoch 12/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8984 - accuracy: 0.7577 - val_loss: 1.1778 - val_accuracy: 0.6678\n",
      "Epoch 13/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8916 - accuracy: 0.7607 - val_loss: 1.3253 - val_accuracy: 0.6589\n",
      "Epoch 14/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8777 - accuracy: 0.7659 - val_loss: 1.4515 - val_accuracy: 0.6150\n",
      "Epoch 15/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8636 - accuracy: 0.7738 - val_loss: 1.0736 - val_accuracy: 0.7157\n",
      "Epoch 16/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8593 - accuracy: 0.7727 - val_loss: 1.1245 - val_accuracy: 0.7065\n",
      "Epoch 17/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8402 - accuracy: 0.7830 - val_loss: 1.3309 - val_accuracy: 0.6327\n",
      "Epoch 18/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8390 - accuracy: 0.7837 - val_loss: 1.2400 - val_accuracy: 0.6612\n",
      "Epoch 19/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8208 - accuracy: 0.7893 - val_loss: 1.3386 - val_accuracy: 0.6508\n",
      "Epoch 20/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8213 - accuracy: 0.7883 - val_loss: 1.2915 - val_accuracy: 0.6633\n",
      "Epoch 21/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8114 - accuracy: 0.7933 - val_loss: 1.3587 - val_accuracy: 0.6557\n",
      "Epoch 22/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7965 - accuracy: 0.7965 - val_loss: 1.5422 - val_accuracy: 0.6387\n",
      "Epoch 23/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7935 - accuracy: 0.7967 - val_loss: 1.2456 - val_accuracy: 0.6807\n",
      "Epoch 24/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7841 - accuracy: 0.8006 - val_loss: 1.3089 - val_accuracy: 0.6888\n",
      "Epoch 25/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7791 - accuracy: 0.8018 - val_loss: 1.1593 - val_accuracy: 0.7004\n",
      "Epoch 26/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7678 - accuracy: 0.8057 - val_loss: 0.9496 - val_accuracy: 0.7504\n",
      "Epoch 27/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7610 - accuracy: 0.8093 - val_loss: 2.3990 - val_accuracy: 0.5343\n",
      "Epoch 28/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.7608 - accuracy: 0.8089 - val_loss: 1.9484 - val_accuracy: 0.5261\n",
      "Epoch 29/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7565 - accuracy: 0.8102 - val_loss: 1.0973 - val_accuracy: 0.7117\n",
      "Epoch 30/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7393 - accuracy: 0.8155 - val_loss: 0.9618 - val_accuracy: 0.7460\n",
      "Epoch 31/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7355 - accuracy: 0.8167 - val_loss: 1.6283 - val_accuracy: 0.6251\n",
      "Epoch 32/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7263 - accuracy: 0.8179 - val_loss: 1.2016 - val_accuracy: 0.6669\n",
      "Epoch 33/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7261 - accuracy: 0.8184 - val_loss: 1.1628 - val_accuracy: 0.7057\n",
      "Epoch 34/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.7179 - accuracy: 0.8224 - val_loss: 1.0749 - val_accuracy: 0.7241\n",
      "Epoch 35/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7086 - accuracy: 0.8261 - val_loss: 1.2621 - val_accuracy: 0.6713\n",
      "Epoch 36/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6979 - accuracy: 0.8285 - val_loss: 0.9515 - val_accuracy: 0.7546\n",
      "Epoch 37/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.7025 - accuracy: 0.8258 - val_loss: 1.0577 - val_accuracy: 0.7433\n",
      "Epoch 38/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.6921 - accuracy: 0.8280 - val_loss: 1.1177 - val_accuracy: 0.6937\n",
      "Epoch 39/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.6845 - accuracy: 0.8302 - val_loss: 0.8440 - val_accuracy: 0.7848\n",
      "Epoch 40/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6734 - accuracy: 0.8337 - val_loss: 1.7487 - val_accuracy: 0.5890\n",
      "Epoch 41/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.6674 - accuracy: 0.8366 - val_loss: 0.9875 - val_accuracy: 0.7563\n",
      "Epoch 42/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6592 - accuracy: 0.8375 - val_loss: 1.1093 - val_accuracy: 0.7133\n",
      "Epoch 43/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6624 - accuracy: 0.8366 - val_loss: 0.8589 - val_accuracy: 0.7823\n",
      "Epoch 44/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6464 - accuracy: 0.8415 - val_loss: 0.9273 - val_accuracy: 0.7480\n",
      "Epoch 45/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6430 - accuracy: 0.8419 - val_loss: 1.5157 - val_accuracy: 0.6540\n",
      "Epoch 46/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6389 - accuracy: 0.8437 - val_loss: 0.8281 - val_accuracy: 0.7961\n",
      "Epoch 47/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6305 - accuracy: 0.8462 - val_loss: 0.9100 - val_accuracy: 0.7513\n",
      "Epoch 48/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6249 - accuracy: 0.8457 - val_loss: 0.8797 - val_accuracy: 0.7637\n",
      "Epoch 49/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6156 - accuracy: 0.8497 - val_loss: 0.8112 - val_accuracy: 0.7939\n",
      "Epoch 50/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.6119 - accuracy: 0.8503 - val_loss: 0.9900 - val_accuracy: 0.7572\n",
      "Epoch 51/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.5987 - accuracy: 0.8532 - val_loss: 0.8689 - val_accuracy: 0.7693\n",
      "Epoch 52/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.6018 - accuracy: 0.8514 - val_loss: 0.7416 - val_accuracy: 0.8075\n",
      "Epoch 53/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.5873 - accuracy: 0.8574 - val_loss: 0.9414 - val_accuracy: 0.7425\n",
      "Epoch 54/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.5835 - accuracy: 0.8591 - val_loss: 1.2901 - val_accuracy: 0.6785\n",
      "Epoch 55/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5702 - accuracy: 0.8606 - val_loss: 0.9646 - val_accuracy: 0.7594\n",
      "Epoch 56/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.5640 - accuracy: 0.8624 - val_loss: 0.8596 - val_accuracy: 0.7764\n",
      "Epoch 57/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5498 - accuracy: 0.8680 - val_loss: 0.9295 - val_accuracy: 0.7572\n",
      "Epoch 58/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.5599 - accuracy: 0.8611 - val_loss: 0.9819 - val_accuracy: 0.7469\n",
      "Epoch 59/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.5399 - accuracy: 0.8691 - val_loss: 0.7931 - val_accuracy: 0.7908\n",
      "Epoch 60/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5381 - accuracy: 0.8676 - val_loss: 0.7809 - val_accuracy: 0.8004\n",
      "Epoch 61/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5354 - accuracy: 0.8690 - val_loss: 0.6985 - val_accuracy: 0.8236\n",
      "Epoch 62/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.5145 - accuracy: 0.8741 - val_loss: 0.8825 - val_accuracy: 0.7868\n",
      "Epoch 63/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.5131 - accuracy: 0.8746 - val_loss: 0.7963 - val_accuracy: 0.8034\n",
      "Epoch 64/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.4969 - accuracy: 0.8795 - val_loss: 0.8805 - val_accuracy: 0.7719\n",
      "Epoch 65/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4989 - accuracy: 0.8804 - val_loss: 0.7652 - val_accuracy: 0.8092\n",
      "Epoch 66/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4820 - accuracy: 0.8852 - val_loss: 0.8364 - val_accuracy: 0.7763\n",
      "Epoch 67/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.4810 - accuracy: 0.8823 - val_loss: 0.6315 - val_accuracy: 0.8385\n",
      "Epoch 68/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.4731 - accuracy: 0.8840 - val_loss: 0.7745 - val_accuracy: 0.7974\n",
      "Epoch 69/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4647 - accuracy: 0.8870 - val_loss: 0.6401 - val_accuracy: 0.8350\n",
      "Epoch 70/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4586 - accuracy: 0.8876 - val_loss: 0.7654 - val_accuracy: 0.8028\n",
      "Epoch 71/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4449 - accuracy: 0.8932 - val_loss: 0.9728 - val_accuracy: 0.7500\n",
      "Epoch 72/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4413 - accuracy: 0.8940 - val_loss: 0.6296 - val_accuracy: 0.8376\n",
      "Epoch 73/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.4275 - accuracy: 0.8970 - val_loss: 0.6081 - val_accuracy: 0.8461\n",
      "Epoch 74/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4270 - accuracy: 0.8974 - val_loss: 0.6065 - val_accuracy: 0.8457\n",
      "Epoch 75/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.4146 - accuracy: 0.9002 - val_loss: 0.6661 - val_accuracy: 0.8336\n",
      "Epoch 76/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.4103 - accuracy: 0.9014 - val_loss: 0.7409 - val_accuracy: 0.8124\n",
      "Epoch 77/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.4005 - accuracy: 0.9045 - val_loss: 0.7891 - val_accuracy: 0.8046\n",
      "Epoch 78/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3893 - accuracy: 0.9081 - val_loss: 0.6278 - val_accuracy: 0.8372\n",
      "Epoch 79/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3891 - accuracy: 0.9051 - val_loss: 0.6491 - val_accuracy: 0.8363\n",
      "Epoch 80/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.3754 - accuracy: 0.9115 - val_loss: 0.5756 - val_accuracy: 0.8548\n",
      "Epoch 81/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3704 - accuracy: 0.9118 - val_loss: 0.5806 - val_accuracy: 0.8520\n",
      "Epoch 82/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.3581 - accuracy: 0.9169 - val_loss: 0.5953 - val_accuracy: 0.8526\n",
      "Epoch 83/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.3561 - accuracy: 0.9160 - val_loss: 0.6401 - val_accuracy: 0.8458\n",
      "Epoch 84/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.3450 - accuracy: 0.9192 - val_loss: 0.6846 - val_accuracy: 0.8321\n",
      "Epoch 85/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.3373 - accuracy: 0.9214 - val_loss: 0.5148 - val_accuracy: 0.8709\n",
      "Epoch 86/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.3312 - accuracy: 0.9221 - val_loss: 0.5472 - val_accuracy: 0.8686\n",
      "Epoch 87/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3296 - accuracy: 0.9234 - val_loss: 0.5159 - val_accuracy: 0.8705\n",
      "Epoch 88/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3167 - accuracy: 0.9278 - val_loss: 0.5623 - val_accuracy: 0.8571\n",
      "Epoch 89/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3146 - accuracy: 0.9275 - val_loss: 0.5125 - val_accuracy: 0.8720\n",
      "Epoch 90/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.3063 - accuracy: 0.9303 - val_loss: 0.5818 - val_accuracy: 0.8546\n",
      "Epoch 91/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.3005 - accuracy: 0.9319 - val_loss: 0.5055 - val_accuracy: 0.8781\n",
      "Epoch 92/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2981 - accuracy: 0.9324 - val_loss: 0.5342 - val_accuracy: 0.8669\n",
      "Epoch 93/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.2927 - accuracy: 0.9353 - val_loss: 0.5453 - val_accuracy: 0.8667\n",
      "Epoch 94/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2843 - accuracy: 0.9376 - val_loss: 0.5662 - val_accuracy: 0.8647\n",
      "Epoch 95/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2843 - accuracy: 0.9362 - val_loss: 0.5324 - val_accuracy: 0.8742\n",
      "Epoch 96/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.2801 - accuracy: 0.9393 - val_loss: 0.5103 - val_accuracy: 0.8778\n",
      "Epoch 97/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2752 - accuracy: 0.9401 - val_loss: 0.5036 - val_accuracy: 0.8827\n",
      "Epoch 98/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.2741 - accuracy: 0.9404 - val_loss: 0.5095 - val_accuracy: 0.8804\n",
      "Epoch 99/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2730 - accuracy: 0.9411 - val_loss: 0.5012 - val_accuracy: 0.8822\n",
      "Epoch 100/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2596 - accuracy: 0.9458 - val_loss: 0.4953 - val_accuracy: 0.8818\n",
      "Epoch 101/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2694 - accuracy: 0.9406 - val_loss: 0.4980 - val_accuracy: 0.8814\n",
      "Epoch 102/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2611 - accuracy: 0.9446 - val_loss: 0.4981 - val_accuracy: 0.8842\n",
      "Epoch 103/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2638 - accuracy: 0.9436 - val_loss: 0.4908 - val_accuracy: 0.8844\n",
      "Epoch 104/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.2632 - accuracy: 0.9438 - val_loss: 0.4947 - val_accuracy: 0.8844\n",
      "Epoch 105/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2592 - accuracy: 0.9438 - val_loss: 0.4908 - val_accuracy: 0.8848\n",
      "Epoch 106/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.2638 - accuracy: 0.9436 - val_loss: 0.4920 - val_accuracy: 0.8838\n",
      "Epoch 107/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2551 - accuracy: 0.9454 - val_loss: 0.4914 - val_accuracy: 0.8840\n",
      "Epoch 108/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2571 - accuracy: 0.9448 - val_loss: 0.4913 - val_accuracy: 0.8845\n",
      "10000/10000 [==============================] - 2s 247us/step\n",
      "Test loss: 0.4912939685821533\n",
      "Test accuracy: 0.8845000267028809\n",
      "x_train shape: (40000, 32, 32, 3)\n",
      "y_train shape: (40000, 10)\n",
      "40000 train samples\n",
      "10000 test samples\n",
      "10000 validation samples\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "ResNet20v1\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/108\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 2.1230 - accuracy: 0.2918 - val_loss: 2.0429 - val_accuracy: 0.2949\n",
      "Epoch 2/108\n",
      "  1/156 [..............................] - ETA: 11s - loss: 1.9170 - accuracy: 0.3281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheverazo/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 9s 61ms/step - loss: 1.6011 - accuracy: 0.4617 - val_loss: 1.7686 - val_accuracy: 0.4021\n",
      "Epoch 3/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.3857 - accuracy: 0.5542 - val_loss: 1.6377 - val_accuracy: 0.4617\n",
      "Epoch 4/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.2489 - accuracy: 0.6109 - val_loss: 2.4965 - val_accuracy: 0.3259\n",
      "Epoch 5/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.1503 - accuracy: 0.6541 - val_loss: 1.4270 - val_accuracy: 0.5483\n",
      "Epoch 6/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.0722 - accuracy: 0.6876 - val_loss: 1.6761 - val_accuracy: 0.5790\n",
      "Epoch 7/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.0173 - accuracy: 0.7119 - val_loss: 1.1378 - val_accuracy: 0.6691\n",
      "Epoch 8/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9799 - accuracy: 0.7272 - val_loss: 1.3799 - val_accuracy: 0.6015\n",
      "Epoch 9/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9546 - accuracy: 0.7360 - val_loss: 1.1190 - val_accuracy: 0.6831\n",
      "Epoch 10/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9251 - accuracy: 0.7437 - val_loss: 1.2887 - val_accuracy: 0.6419\n",
      "Epoch 11/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8995 - accuracy: 0.7562 - val_loss: 1.1884 - val_accuracy: 0.6721\n",
      "Epoch 12/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8874 - accuracy: 0.7605 - val_loss: 1.7864 - val_accuracy: 0.5980\n",
      "Epoch 13/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8710 - accuracy: 0.7685 - val_loss: 2.4303 - val_accuracy: 0.4846\n",
      "Epoch 14/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8645 - accuracy: 0.7742 - val_loss: 1.7434 - val_accuracy: 0.5784\n",
      "Epoch 15/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8490 - accuracy: 0.7782 - val_loss: 1.2429 - val_accuracy: 0.6470\n",
      "Epoch 16/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8381 - accuracy: 0.7778 - val_loss: 1.4228 - val_accuracy: 0.6410\n",
      "Epoch 17/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8273 - accuracy: 0.7847 - val_loss: 1.9914 - val_accuracy: 0.5154\n",
      "Epoch 18/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8248 - accuracy: 0.7860 - val_loss: 1.5215 - val_accuracy: 0.6221\n",
      "Epoch 19/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8098 - accuracy: 0.7917 - val_loss: 1.2045 - val_accuracy: 0.6915\n",
      "Epoch 20/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8064 - accuracy: 0.7923 - val_loss: 1.0872 - val_accuracy: 0.6966\n",
      "Epoch 21/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7948 - accuracy: 0.7966 - val_loss: 1.5796 - val_accuracy: 0.5940\n",
      "Epoch 22/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7977 - accuracy: 0.7947 - val_loss: 1.1671 - val_accuracy: 0.6737\n",
      "Epoch 23/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7803 - accuracy: 0.8010 - val_loss: 1.1836 - val_accuracy: 0.6949\n",
      "Epoch 24/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7759 - accuracy: 0.8016 - val_loss: 1.2319 - val_accuracy: 0.6624\n",
      "Epoch 25/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7726 - accuracy: 0.8028 - val_loss: 1.0792 - val_accuracy: 0.7147\n",
      "Epoch 26/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7586 - accuracy: 0.8077 - val_loss: 1.5212 - val_accuracy: 0.6016\n",
      "Epoch 27/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7568 - accuracy: 0.8076 - val_loss: 1.0291 - val_accuracy: 0.7241\n",
      "Epoch 28/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7522 - accuracy: 0.8081 - val_loss: 1.6807 - val_accuracy: 0.6450\n",
      "Epoch 29/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.7451 - accuracy: 0.8094 - val_loss: 1.2618 - val_accuracy: 0.6569\n",
      "Epoch 30/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7352 - accuracy: 0.8125 - val_loss: 1.0215 - val_accuracy: 0.7453\n",
      "Epoch 31/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7285 - accuracy: 0.8137 - val_loss: 0.8318 - val_accuracy: 0.7857\n",
      "Epoch 32/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7305 - accuracy: 0.8157 - val_loss: 1.0611 - val_accuracy: 0.7255\n",
      "Epoch 33/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7241 - accuracy: 0.8142 - val_loss: 1.0014 - val_accuracy: 0.7482\n",
      "Epoch 34/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7136 - accuracy: 0.8218 - val_loss: 0.9584 - val_accuracy: 0.7348\n",
      "Epoch 35/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7072 - accuracy: 0.8216 - val_loss: 1.1698 - val_accuracy: 0.7072\n",
      "Epoch 36/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7020 - accuracy: 0.8235 - val_loss: 1.4071 - val_accuracy: 0.6332\n",
      "Epoch 37/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.6937 - accuracy: 0.8231 - val_loss: 1.1048 - val_accuracy: 0.7068\n",
      "Epoch 38/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.6847 - accuracy: 0.8258 - val_loss: 1.1307 - val_accuracy: 0.7249\n",
      "Epoch 39/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6832 - accuracy: 0.8296 - val_loss: 1.3390 - val_accuracy: 0.6420\n",
      "Epoch 40/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6703 - accuracy: 0.8324 - val_loss: 0.7963 - val_accuracy: 0.7870\n",
      "Epoch 41/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6715 - accuracy: 0.8335 - val_loss: 1.0666 - val_accuracy: 0.7154\n",
      "Epoch 42/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6619 - accuracy: 0.8352 - val_loss: 1.3761 - val_accuracy: 0.6539\n",
      "Epoch 43/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.6544 - accuracy: 0.8372 - val_loss: 1.3558 - val_accuracy: 0.6693\n",
      "Epoch 44/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6480 - accuracy: 0.8375 - val_loss: 1.1676 - val_accuracy: 0.6997\n",
      "Epoch 45/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6412 - accuracy: 0.8418 - val_loss: 1.1407 - val_accuracy: 0.7256\n",
      "Epoch 46/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6359 - accuracy: 0.8421 - val_loss: 0.8768 - val_accuracy: 0.7736\n",
      "Epoch 47/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6291 - accuracy: 0.8421 - val_loss: 0.9221 - val_accuracy: 0.7568\n",
      "Epoch 48/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6217 - accuracy: 0.8440 - val_loss: 0.9660 - val_accuracy: 0.7584\n",
      "Epoch 49/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6100 - accuracy: 0.8511 - val_loss: 1.3998 - val_accuracy: 0.6587\n",
      "Epoch 50/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.6107 - accuracy: 0.8469 - val_loss: 1.0946 - val_accuracy: 0.7262\n",
      "Epoch 51/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6008 - accuracy: 0.8492 - val_loss: 0.9986 - val_accuracy: 0.7379\n",
      "Epoch 52/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5939 - accuracy: 0.8507 - val_loss: 0.8851 - val_accuracy: 0.7561\n",
      "Epoch 53/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.5845 - accuracy: 0.8544 - val_loss: 0.9331 - val_accuracy: 0.7708\n",
      "Epoch 54/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5795 - accuracy: 0.8557 - val_loss: 0.8948 - val_accuracy: 0.7692\n",
      "Epoch 55/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5685 - accuracy: 0.8596 - val_loss: 0.7374 - val_accuracy: 0.8076\n",
      "Epoch 56/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5658 - accuracy: 0.8589 - val_loss: 0.9086 - val_accuracy: 0.7618\n",
      "Epoch 57/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.5602 - accuracy: 0.8617 - val_loss: 0.8956 - val_accuracy: 0.7742\n",
      "Epoch 58/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5515 - accuracy: 0.8641 - val_loss: 0.7037 - val_accuracy: 0.8139\n",
      "Epoch 59/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.5396 - accuracy: 0.8670 - val_loss: 0.7935 - val_accuracy: 0.7867\n",
      "Epoch 60/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.5357 - accuracy: 0.8670 - val_loss: 0.8126 - val_accuracy: 0.7877\n",
      "Epoch 61/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.5241 - accuracy: 0.8703 - val_loss: 1.0618 - val_accuracy: 0.7281\n",
      "Epoch 62/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5203 - accuracy: 0.8688 - val_loss: 0.8318 - val_accuracy: 0.7815\n",
      "Epoch 63/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.5089 - accuracy: 0.8745 - val_loss: 0.7643 - val_accuracy: 0.7931\n",
      "Epoch 64/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5082 - accuracy: 0.8755 - val_loss: 0.7603 - val_accuracy: 0.8004\n",
      "Epoch 65/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4924 - accuracy: 0.8796 - val_loss: 0.9967 - val_accuracy: 0.7541\n",
      "Epoch 66/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4945 - accuracy: 0.8776 - val_loss: 0.6621 - val_accuracy: 0.8332\n",
      "Epoch 67/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4864 - accuracy: 0.8795 - val_loss: 0.7315 - val_accuracy: 0.8056\n",
      "Epoch 68/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.4730 - accuracy: 0.8858 - val_loss: 0.7471 - val_accuracy: 0.8140\n",
      "Epoch 69/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4671 - accuracy: 0.8840 - val_loss: 0.6639 - val_accuracy: 0.8281\n",
      "Epoch 70/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.4599 - accuracy: 0.8875 - val_loss: 0.6926 - val_accuracy: 0.8218\n",
      "Epoch 71/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4497 - accuracy: 0.8917 - val_loss: 0.6781 - val_accuracy: 0.8245\n",
      "Epoch 72/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4434 - accuracy: 0.8887 - val_loss: 0.6186 - val_accuracy: 0.8438\n",
      "Epoch 73/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4359 - accuracy: 0.8928 - val_loss: 0.6972 - val_accuracy: 0.8224\n",
      "Epoch 74/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4229 - accuracy: 0.8965 - val_loss: 0.7151 - val_accuracy: 0.8146\n",
      "Epoch 75/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4240 - accuracy: 0.8963 - val_loss: 0.7706 - val_accuracy: 0.7997\n",
      "Epoch 76/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4117 - accuracy: 0.9022 - val_loss: 0.5871 - val_accuracy: 0.8480\n",
      "Epoch 77/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4018 - accuracy: 0.9038 - val_loss: 0.6704 - val_accuracy: 0.8219\n",
      "Epoch 78/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.3977 - accuracy: 0.9040 - val_loss: 0.5586 - val_accuracy: 0.8576\n",
      "Epoch 79/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.3848 - accuracy: 0.9073 - val_loss: 0.6570 - val_accuracy: 0.8382\n",
      "Epoch 80/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3838 - accuracy: 0.9064 - val_loss: 0.6195 - val_accuracy: 0.8434\n",
      "Epoch 81/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.3718 - accuracy: 0.9110 - val_loss: 0.6083 - val_accuracy: 0.8448\n",
      "Epoch 82/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.3682 - accuracy: 0.9113 - val_loss: 0.5782 - val_accuracy: 0.8480\n",
      "Epoch 83/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3609 - accuracy: 0.9123 - val_loss: 0.6476 - val_accuracy: 0.8387\n",
      "Epoch 84/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.3573 - accuracy: 0.9134 - val_loss: 0.5964 - val_accuracy: 0.8491\n",
      "Epoch 85/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.3449 - accuracy: 0.9196 - val_loss: 0.5204 - val_accuracy: 0.8694\n",
      "Epoch 86/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3414 - accuracy: 0.9185 - val_loss: 0.5871 - val_accuracy: 0.8491\n",
      "Epoch 87/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3360 - accuracy: 0.9195 - val_loss: 0.5676 - val_accuracy: 0.8554\n",
      "Epoch 88/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.3272 - accuracy: 0.9230 - val_loss: 0.5475 - val_accuracy: 0.8582\n",
      "Epoch 89/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.3184 - accuracy: 0.9252 - val_loss: 0.5347 - val_accuracy: 0.8646\n",
      "Epoch 90/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.3194 - accuracy: 0.9243 - val_loss: 0.5173 - val_accuracy: 0.8685\n",
      "Epoch 91/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3099 - accuracy: 0.9280 - val_loss: 0.5064 - val_accuracy: 0.8708\n",
      "Epoch 92/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.3049 - accuracy: 0.9284 - val_loss: 0.5507 - val_accuracy: 0.8640\n",
      "Epoch 93/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3011 - accuracy: 0.9307 - val_loss: 0.5265 - val_accuracy: 0.8698\n",
      "Epoch 94/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2992 - accuracy: 0.9315 - val_loss: 0.5031 - val_accuracy: 0.8745\n",
      "Epoch 95/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2930 - accuracy: 0.9331 - val_loss: 0.5135 - val_accuracy: 0.8717\n",
      "Epoch 96/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2924 - accuracy: 0.9323 - val_loss: 0.5153 - val_accuracy: 0.8711\n",
      "Epoch 97/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.2849 - accuracy: 0.9359 - val_loss: 0.4889 - val_accuracy: 0.8752\n",
      "Epoch 98/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2820 - accuracy: 0.9376 - val_loss: 0.4911 - val_accuracy: 0.8769\n",
      "Epoch 99/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2768 - accuracy: 0.9385 - val_loss: 0.5068 - val_accuracy: 0.8741\n",
      "Epoch 100/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2742 - accuracy: 0.9376 - val_loss: 0.4879 - val_accuracy: 0.8783\n",
      "Epoch 101/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2753 - accuracy: 0.9387 - val_loss: 0.5016 - val_accuracy: 0.8770\n",
      "Epoch 102/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2736 - accuracy: 0.9381 - val_loss: 0.4924 - val_accuracy: 0.8767\n",
      "Epoch 103/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2685 - accuracy: 0.9402 - val_loss: 0.4921 - val_accuracy: 0.8777\n",
      "Epoch 104/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2712 - accuracy: 0.9392 - val_loss: 0.4921 - val_accuracy: 0.8783\n",
      "Epoch 105/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2684 - accuracy: 0.9406 - val_loss: 0.4899 - val_accuracy: 0.8781\n",
      "Epoch 106/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2682 - accuracy: 0.9400 - val_loss: 0.4889 - val_accuracy: 0.8784\n",
      "Epoch 107/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2655 - accuracy: 0.9419 - val_loss: 0.4881 - val_accuracy: 0.8787\n",
      "Epoch 108/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2671 - accuracy: 0.9413 - val_loss: 0.4890 - val_accuracy: 0.8782\n",
      "10000/10000 [==============================] - 2s 242us/step\n",
      "Test loss: 0.4890229293823242\n",
      "Test accuracy: 0.8781999945640564\n",
      "x_train shape: (40000, 32, 32, 3)\n",
      "y_train shape: (40000, 10)\n",
      "40000 train samples\n",
      "10000 test samples\n",
      "10000 validation samples\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "ResNet20v1\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/108\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 2.1965 - accuracy: 0.2798 - val_loss: 12.6461 - val_accuracy: 0.1893\n",
      "Epoch 2/108\n",
      "  1/156 [..............................] - ETA: 12s - loss: 1.9489 - accuracy: 0.3125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheverazo/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 9s 60ms/step - loss: 1.6654 - accuracy: 0.4395 - val_loss: 1.6308 - val_accuracy: 0.4698\n",
      "Epoch 3/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.4206 - accuracy: 0.5416 - val_loss: 1.5600 - val_accuracy: 0.5160\n",
      "Epoch 4/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.2756 - accuracy: 0.6054 - val_loss: 2.2326 - val_accuracy: 0.4148\n",
      "Epoch 5/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.1675 - accuracy: 0.6492 - val_loss: 1.4168 - val_accuracy: 0.5820\n",
      "Epoch 6/108\n",
      "156/156 [==============================] - 10s 64ms/step - loss: 1.0888 - accuracy: 0.6816 - val_loss: 1.9879 - val_accuracy: 0.4978\n",
      "Epoch 7/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0337 - accuracy: 0.7069 - val_loss: 1.3189 - val_accuracy: 0.6021\n",
      "Epoch 8/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9917 - accuracy: 0.7232 - val_loss: 2.4201 - val_accuracy: 0.4639\n",
      "Epoch 9/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9646 - accuracy: 0.7326 - val_loss: 1.1780 - val_accuracy: 0.6855\n",
      "Epoch 10/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9348 - accuracy: 0.7436 - val_loss: 1.4245 - val_accuracy: 0.6430\n",
      "Epoch 11/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9243 - accuracy: 0.7523 - val_loss: 1.2183 - val_accuracy: 0.6635\n",
      "Epoch 12/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8978 - accuracy: 0.7603 - val_loss: 1.4422 - val_accuracy: 0.6182\n",
      "Epoch 13/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8797 - accuracy: 0.7683 - val_loss: 1.8095 - val_accuracy: 0.5706\n",
      "Epoch 14/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8763 - accuracy: 0.7678 - val_loss: 1.2840 - val_accuracy: 0.6547\n",
      "Epoch 15/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.8600 - accuracy: 0.7737 - val_loss: 2.4269 - val_accuracy: 0.4630\n",
      "Epoch 16/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8475 - accuracy: 0.7778 - val_loss: 1.1014 - val_accuracy: 0.7114\n",
      "Epoch 17/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8401 - accuracy: 0.7812 - val_loss: 1.1008 - val_accuracy: 0.7079\n",
      "Epoch 18/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8317 - accuracy: 0.7840 - val_loss: 1.4850 - val_accuracy: 0.6223\n",
      "Epoch 19/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8243 - accuracy: 0.7873 - val_loss: 1.6026 - val_accuracy: 0.6021\n",
      "Epoch 20/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8111 - accuracy: 0.7918 - val_loss: 2.2764 - val_accuracy: 0.5406\n",
      "Epoch 21/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8048 - accuracy: 0.7931 - val_loss: 1.3568 - val_accuracy: 0.6585\n",
      "Epoch 22/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7938 - accuracy: 0.7966 - val_loss: 1.1363 - val_accuracy: 0.7006\n",
      "Epoch 23/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7879 - accuracy: 0.7989 - val_loss: 1.1261 - val_accuracy: 0.7135\n",
      "Epoch 24/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7846 - accuracy: 0.7994 - val_loss: 1.1695 - val_accuracy: 0.7103\n",
      "Epoch 25/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.7793 - accuracy: 0.8002 - val_loss: 1.8130 - val_accuracy: 0.5759\n",
      "Epoch 26/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7733 - accuracy: 0.8042 - val_loss: 1.1521 - val_accuracy: 0.6931\n",
      "Epoch 27/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7622 - accuracy: 0.8067 - val_loss: 1.0916 - val_accuracy: 0.6972\n",
      "Epoch 28/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.7510 - accuracy: 0.8102 - val_loss: 1.0189 - val_accuracy: 0.7276\n",
      "Epoch 29/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.7501 - accuracy: 0.8101 - val_loss: 0.9891 - val_accuracy: 0.7330\n",
      "Epoch 30/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7475 - accuracy: 0.8096 - val_loss: 1.2665 - val_accuracy: 0.6678\n",
      "Epoch 31/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7380 - accuracy: 0.8141 - val_loss: 1.1045 - val_accuracy: 0.7045\n",
      "Epoch 32/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7269 - accuracy: 0.8159 - val_loss: 1.1225 - val_accuracy: 0.7131\n",
      "Epoch 33/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.7201 - accuracy: 0.8217 - val_loss: 1.2600 - val_accuracy: 0.6747\n",
      "Epoch 34/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7202 - accuracy: 0.8184 - val_loss: 1.1653 - val_accuracy: 0.6895\n",
      "Epoch 35/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7210 - accuracy: 0.8175 - val_loss: 1.0681 - val_accuracy: 0.7272\n",
      "Epoch 36/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7123 - accuracy: 0.8207 - val_loss: 1.5553 - val_accuracy: 0.6486\n",
      "Epoch 37/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7009 - accuracy: 0.8229 - val_loss: 0.9577 - val_accuracy: 0.7436\n",
      "Epoch 38/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6867 - accuracy: 0.8281 - val_loss: 0.9245 - val_accuracy: 0.7567\n",
      "Epoch 39/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6883 - accuracy: 0.8268 - val_loss: 1.0557 - val_accuracy: 0.7185\n",
      "Epoch 40/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6798 - accuracy: 0.8304 - val_loss: 1.1045 - val_accuracy: 0.7073\n",
      "Epoch 41/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6725 - accuracy: 0.8304 - val_loss: 1.2656 - val_accuracy: 0.7038\n",
      "Epoch 42/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6678 - accuracy: 0.8340 - val_loss: 1.1163 - val_accuracy: 0.7024\n",
      "Epoch 43/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6608 - accuracy: 0.8370 - val_loss: 1.1581 - val_accuracy: 0.7079\n",
      "Epoch 44/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.6490 - accuracy: 0.8364 - val_loss: 0.7740 - val_accuracy: 0.8007\n",
      "Epoch 45/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6519 - accuracy: 0.8377 - val_loss: 0.8084 - val_accuracy: 0.7898\n",
      "Epoch 46/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6373 - accuracy: 0.8418 - val_loss: 0.9070 - val_accuracy: 0.7673\n",
      "Epoch 47/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.6356 - accuracy: 0.8423 - val_loss: 1.2612 - val_accuracy: 0.6858\n",
      "Epoch 48/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.6301 - accuracy: 0.8439 - val_loss: 0.7849 - val_accuracy: 0.7936\n",
      "Epoch 49/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.6203 - accuracy: 0.8447 - val_loss: 1.1279 - val_accuracy: 0.7014\n",
      "Epoch 50/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.6171 - accuracy: 0.8459 - val_loss: 0.9656 - val_accuracy: 0.7570\n",
      "Epoch 51/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.6024 - accuracy: 0.8500 - val_loss: 1.3617 - val_accuracy: 0.6308\n",
      "Epoch 52/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5957 - accuracy: 0.8520 - val_loss: 0.9929 - val_accuracy: 0.7520\n",
      "Epoch 53/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5911 - accuracy: 0.8544 - val_loss: 0.8376 - val_accuracy: 0.7889\n",
      "Epoch 54/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.5894 - accuracy: 0.8512 - val_loss: 0.8677 - val_accuracy: 0.7790\n",
      "Epoch 55/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.5740 - accuracy: 0.8581 - val_loss: 0.8707 - val_accuracy: 0.7785\n",
      "Epoch 56/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.5666 - accuracy: 0.8598 - val_loss: 0.7924 - val_accuracy: 0.7940\n",
      "Epoch 57/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.5627 - accuracy: 0.8600 - val_loss: 0.7824 - val_accuracy: 0.7920\n",
      "Epoch 58/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.5522 - accuracy: 0.8627 - val_loss: 0.8816 - val_accuracy: 0.7778\n",
      "Epoch 59/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.5471 - accuracy: 0.8650 - val_loss: 0.8598 - val_accuracy: 0.7811\n",
      "Epoch 60/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.5401 - accuracy: 0.8667 - val_loss: 0.7711 - val_accuracy: 0.8006\n",
      "Epoch 61/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.5350 - accuracy: 0.8674 - val_loss: 1.3501 - val_accuracy: 0.7187\n",
      "Epoch 62/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.5317 - accuracy: 0.8674 - val_loss: 0.9287 - val_accuracy: 0.7584\n",
      "Epoch 63/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.5128 - accuracy: 0.8740 - val_loss: 1.0794 - val_accuracy: 0.7327\n",
      "Epoch 64/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.5100 - accuracy: 0.8746 - val_loss: 0.7694 - val_accuracy: 0.7981\n",
      "Epoch 65/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4989 - accuracy: 0.8781 - val_loss: 0.8965 - val_accuracy: 0.7778\n",
      "Epoch 66/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.4896 - accuracy: 0.8789 - val_loss: 0.8882 - val_accuracy: 0.7811\n",
      "Epoch 67/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4821 - accuracy: 0.8810 - val_loss: 0.7255 - val_accuracy: 0.8131\n",
      "Epoch 68/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.4757 - accuracy: 0.8822 - val_loss: 0.8671 - val_accuracy: 0.7857\n",
      "Epoch 69/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.4674 - accuracy: 0.8858 - val_loss: 0.7130 - val_accuracy: 0.8174\n",
      "Epoch 70/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.4588 - accuracy: 0.8880 - val_loss: 0.8437 - val_accuracy: 0.7785\n",
      "Epoch 71/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.4559 - accuracy: 0.8890 - val_loss: 0.6892 - val_accuracy: 0.8153\n",
      "Epoch 72/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4447 - accuracy: 0.8925 - val_loss: 0.7519 - val_accuracy: 0.8107\n",
      "Epoch 73/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.4388 - accuracy: 0.8927 - val_loss: 0.9166 - val_accuracy: 0.7832\n",
      "Epoch 74/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.4278 - accuracy: 0.8964 - val_loss: 0.6961 - val_accuracy: 0.8204\n",
      "Epoch 75/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.4185 - accuracy: 0.8982 - val_loss: 0.6195 - val_accuracy: 0.8374\n",
      "Epoch 76/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.4141 - accuracy: 0.8979 - val_loss: 0.6840 - val_accuracy: 0.8287\n",
      "Epoch 77/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.4051 - accuracy: 0.9020 - val_loss: 0.7212 - val_accuracy: 0.8204\n",
      "Epoch 78/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.4024 - accuracy: 0.9036 - val_loss: 0.6230 - val_accuracy: 0.8394\n",
      "Epoch 79/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.3872 - accuracy: 0.9075 - val_loss: 0.6530 - val_accuracy: 0.8320\n",
      "Epoch 80/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3843 - accuracy: 0.9066 - val_loss: 0.5709 - val_accuracy: 0.8536\n",
      "Epoch 81/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.3705 - accuracy: 0.9115 - val_loss: 0.6570 - val_accuracy: 0.8347\n",
      "Epoch 82/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.3717 - accuracy: 0.9092 - val_loss: 0.6028 - val_accuracy: 0.8506\n",
      "Epoch 83/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.3594 - accuracy: 0.9137 - val_loss: 0.5781 - val_accuracy: 0.8490\n",
      "Epoch 84/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.3521 - accuracy: 0.9166 - val_loss: 0.6329 - val_accuracy: 0.8404\n",
      "Epoch 85/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3514 - accuracy: 0.9147 - val_loss: 0.6026 - val_accuracy: 0.8487\n",
      "Epoch 86/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.3420 - accuracy: 0.9186 - val_loss: 0.5291 - val_accuracy: 0.8620\n",
      "Epoch 87/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.3360 - accuracy: 0.9208 - val_loss: 0.5514 - val_accuracy: 0.8604\n",
      "Epoch 88/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3243 - accuracy: 0.9246 - val_loss: 0.5591 - val_accuracy: 0.8561\n",
      "Epoch 89/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.3248 - accuracy: 0.9242 - val_loss: 0.5447 - val_accuracy: 0.8641\n",
      "Epoch 90/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.3207 - accuracy: 0.9249 - val_loss: 0.5363 - val_accuracy: 0.8628\n",
      "Epoch 91/108\n",
      "156/156 [==============================] - 10s 63ms/step - loss: 0.3086 - accuracy: 0.9280 - val_loss: 0.5347 - val_accuracy: 0.8675\n",
      "Epoch 92/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.3074 - accuracy: 0.9295 - val_loss: 0.5817 - val_accuracy: 0.8571\n",
      "Epoch 93/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.2992 - accuracy: 0.9313 - val_loss: 0.5758 - val_accuracy: 0.8619\n",
      "Epoch 94/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.2970 - accuracy: 0.9319 - val_loss: 0.5419 - val_accuracy: 0.8652\n",
      "Epoch 95/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2886 - accuracy: 0.9356 - val_loss: 0.5227 - val_accuracy: 0.8728\n",
      "Epoch 96/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2910 - accuracy: 0.9340 - val_loss: 0.5389 - val_accuracy: 0.8660\n",
      "Epoch 97/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2850 - accuracy: 0.9353 - val_loss: 0.5119 - val_accuracy: 0.8710\n",
      "Epoch 98/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2812 - accuracy: 0.9366 - val_loss: 0.5149 - val_accuracy: 0.8722\n",
      "Epoch 99/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2787 - accuracy: 0.9370 - val_loss: 0.5062 - val_accuracy: 0.8725\n",
      "Epoch 100/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2777 - accuracy: 0.9387 - val_loss: 0.5088 - val_accuracy: 0.8772\n",
      "Epoch 101/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2734 - accuracy: 0.9391 - val_loss: 0.5064 - val_accuracy: 0.8773\n",
      "Epoch 102/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2706 - accuracy: 0.9425 - val_loss: 0.5132 - val_accuracy: 0.8731\n",
      "Epoch 103/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2674 - accuracy: 0.9421 - val_loss: 0.5062 - val_accuracy: 0.8753\n",
      "Epoch 104/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2703 - accuracy: 0.9389 - val_loss: 0.5074 - val_accuracy: 0.8743\n",
      "Epoch 105/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2659 - accuracy: 0.9430 - val_loss: 0.5037 - val_accuracy: 0.8756\n",
      "Epoch 106/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2712 - accuracy: 0.9401 - val_loss: 0.5051 - val_accuracy: 0.8760\n",
      "Epoch 107/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.2664 - accuracy: 0.9409 - val_loss: 0.5058 - val_accuracy: 0.8757\n",
      "Epoch 108/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.2675 - accuracy: 0.9430 - val_loss: 0.5070 - val_accuracy: 0.8750\n",
      "10000/10000 [==============================] - 2s 206us/step\n",
      "Test loss: 0.506987271976471\n",
      "Test accuracy: 0.875\n",
      "x_train shape: (40000, 32, 32, 3)\n",
      "y_train shape: (40000, 10)\n",
      "40000 train samples\n",
      "10000 test samples\n",
      "10000 validation samples\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "ResNet20v1\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/108\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 3.1748 - accuracy: 0.1225 - val_loss: 2.7005 - val_accuracy: 0.1039\n",
      "Epoch 2/108\n",
      "  1/156 [..............................] - ETA: 10s - loss: 2.6398 - accuracy: 0.1016"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheverazo/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 9s 61ms/step - loss: 2.3317 - accuracy: 0.1927 - val_loss: 2.6696 - val_accuracy: 0.1091\n",
      "Epoch 3/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 2.2034 - accuracy: 0.2392 - val_loss: 2.6003 - val_accuracy: 0.1621\n",
      "Epoch 4/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 2.1085 - accuracy: 0.2774 - val_loss: 2.8500 - val_accuracy: 0.1318\n",
      "Epoch 5/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 2.0774 - accuracy: 0.2875 - val_loss: 3.9726 - val_accuracy: 0.1026\n",
      "Epoch 6/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 2.0386 - accuracy: 0.3111 - val_loss: 3.0095 - val_accuracy: 0.1211\n",
      "Epoch 7/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 2.0140 - accuracy: 0.3258 - val_loss: 2.2216 - val_accuracy: 0.2519\n",
      "Epoch 8/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9963 - accuracy: 0.3396 - val_loss: 2.8288 - val_accuracy: 0.1551\n",
      "Epoch 9/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9948 - accuracy: 0.3479 - val_loss: 2.2199 - val_accuracy: 0.2264\n",
      "Epoch 10/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9925 - accuracy: 0.3480 - val_loss: 2.0650 - val_accuracy: 0.2794\n",
      "Epoch 11/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9884 - accuracy: 0.3481 - val_loss: 1.9151 - val_accuracy: 0.3430\n",
      "Epoch 12/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9849 - accuracy: 0.3494 - val_loss: 3.1863 - val_accuracy: 0.1876\n",
      "Epoch 13/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9819 - accuracy: 0.3553 - val_loss: 2.7463 - val_accuracy: 0.1439\n",
      "Epoch 14/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9799 - accuracy: 0.3555 - val_loss: 3.1373 - val_accuracy: 0.1532\n",
      "Epoch 15/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9743 - accuracy: 0.3645 - val_loss: 2.1739 - val_accuracy: 0.2587\n",
      "Epoch 16/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9669 - accuracy: 0.3621 - val_loss: 1.8957 - val_accuracy: 0.3334\n",
      "Epoch 17/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.9619 - accuracy: 0.3630 - val_loss: 2.1618 - val_accuracy: 0.2582\n",
      "Epoch 18/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9640 - accuracy: 0.3644 - val_loss: 2.0561 - val_accuracy: 0.2844\n",
      "Epoch 19/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9599 - accuracy: 0.3665 - val_loss: 2.4804 - val_accuracy: 0.2312\n",
      "Epoch 20/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9636 - accuracy: 0.3637 - val_loss: 2.3382 - val_accuracy: 0.2168\n",
      "Epoch 21/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9530 - accuracy: 0.3675 - val_loss: 2.3918 - val_accuracy: 0.2619\n",
      "Epoch 22/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9503 - accuracy: 0.3722 - val_loss: 1.7958 - val_accuracy: 0.3683\n",
      "Epoch 23/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9434 - accuracy: 0.3732 - val_loss: 1.8401 - val_accuracy: 0.3491\n",
      "Epoch 24/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9414 - accuracy: 0.3795 - val_loss: 1.9034 - val_accuracy: 0.3481\n",
      "Epoch 25/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9374 - accuracy: 0.3804 - val_loss: 2.7806 - val_accuracy: 0.1687\n",
      "Epoch 26/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9285 - accuracy: 0.3870 - val_loss: 1.8258 - val_accuracy: 0.3540\n",
      "Epoch 27/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9251 - accuracy: 0.3833 - val_loss: 2.0272 - val_accuracy: 0.2905\n",
      "Epoch 28/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9235 - accuracy: 0.3866 - val_loss: 1.9706 - val_accuracy: 0.2710\n",
      "Epoch 29/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9167 - accuracy: 0.3891 - val_loss: 2.0382 - val_accuracy: 0.2893\n",
      "Epoch 30/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9163 - accuracy: 0.3857 - val_loss: 2.2560 - val_accuracy: 0.2449\n",
      "Epoch 31/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9114 - accuracy: 0.3910 - val_loss: 2.0749 - val_accuracy: 0.3179\n",
      "Epoch 32/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9059 - accuracy: 0.3890 - val_loss: 2.0092 - val_accuracy: 0.3092\n",
      "Epoch 33/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8976 - accuracy: 0.3971 - val_loss: 2.0568 - val_accuracy: 0.2981\n",
      "Epoch 34/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8979 - accuracy: 0.3916 - val_loss: 2.1008 - val_accuracy: 0.2866\n",
      "Epoch 35/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.8928 - accuracy: 0.3933 - val_loss: 2.9942 - val_accuracy: 0.2567\n",
      "Epoch 36/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.8863 - accuracy: 0.3968 - val_loss: 2.7533 - val_accuracy: 0.2068\n",
      "Epoch 37/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8814 - accuracy: 0.3994 - val_loss: 2.2993 - val_accuracy: 0.2203\n",
      "Epoch 38/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8778 - accuracy: 0.4039 - val_loss: 3.0313 - val_accuracy: 0.1251\n",
      "Epoch 39/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8766 - accuracy: 0.3995 - val_loss: 1.9851 - val_accuracy: 0.3319\n",
      "Epoch 40/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8674 - accuracy: 0.4107 - val_loss: 2.1954 - val_accuracy: 0.2556\n",
      "Epoch 41/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8666 - accuracy: 0.4045 - val_loss: 1.9363 - val_accuracy: 0.3495\n",
      "Epoch 42/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.8609 - accuracy: 0.4050 - val_loss: 2.1064 - val_accuracy: 0.2931\n",
      "Epoch 43/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8528 - accuracy: 0.4165 - val_loss: 2.4967 - val_accuracy: 0.2226\n",
      "Epoch 44/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8488 - accuracy: 0.4123 - val_loss: 1.8204 - val_accuracy: 0.3821\n",
      "Epoch 45/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.8462 - accuracy: 0.4109 - val_loss: 2.7825 - val_accuracy: 0.1860\n",
      "Epoch 46/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8383 - accuracy: 0.4140 - val_loss: 1.8855 - val_accuracy: 0.3371\n",
      "Epoch 47/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8390 - accuracy: 0.4111 - val_loss: 2.5689 - val_accuracy: 0.2322\n",
      "Epoch 48/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.8324 - accuracy: 0.4170 - val_loss: 1.7634 - val_accuracy: 0.3834\n",
      "Epoch 49/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8239 - accuracy: 0.4185 - val_loss: 1.9041 - val_accuracy: 0.3417\n",
      "Epoch 50/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8248 - accuracy: 0.4196 - val_loss: 2.0356 - val_accuracy: 0.3133\n",
      "Epoch 51/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.8153 - accuracy: 0.4221 - val_loss: 2.0717 - val_accuracy: 0.3287\n",
      "Epoch 52/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.8131 - accuracy: 0.4233 - val_loss: 2.4013 - val_accuracy: 0.2260\n",
      "Epoch 53/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.8045 - accuracy: 0.4272 - val_loss: 2.2194 - val_accuracy: 0.2538\n",
      "Epoch 54/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.8027 - accuracy: 0.4248 - val_loss: 2.4592 - val_accuracy: 0.2443\n",
      "Epoch 55/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7933 - accuracy: 0.4302 - val_loss: 1.9113 - val_accuracy: 0.3446\n",
      "Epoch 56/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7916 - accuracy: 0.4308 - val_loss: 1.9478 - val_accuracy: 0.3282\n",
      "Epoch 57/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7904 - accuracy: 0.4273 - val_loss: 2.0706 - val_accuracy: 0.2770\n",
      "Epoch 58/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7807 - accuracy: 0.4380 - val_loss: 2.3174 - val_accuracy: 0.2471\n",
      "Epoch 59/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7759 - accuracy: 0.4359 - val_loss: 1.7365 - val_accuracy: 0.3925\n",
      "Epoch 60/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7707 - accuracy: 0.4373 - val_loss: 1.7508 - val_accuracy: 0.3816\n",
      "Epoch 61/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7707 - accuracy: 0.4356 - val_loss: 2.0158 - val_accuracy: 0.3008\n",
      "Epoch 62/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7626 - accuracy: 0.4418 - val_loss: 1.7595 - val_accuracy: 0.3590\n",
      "Epoch 63/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7574 - accuracy: 0.4433 - val_loss: 2.1799 - val_accuracy: 0.2760\n",
      "Epoch 64/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7590 - accuracy: 0.4426 - val_loss: 1.9662 - val_accuracy: 0.3522\n",
      "Epoch 65/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.7488 - accuracy: 0.4449 - val_loss: 1.7182 - val_accuracy: 0.3807\n",
      "Epoch 66/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7501 - accuracy: 0.4394 - val_loss: 1.9443 - val_accuracy: 0.3346\n",
      "Epoch 67/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7428 - accuracy: 0.4460 - val_loss: 1.6000 - val_accuracy: 0.4287\n",
      "Epoch 68/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7377 - accuracy: 0.4500 - val_loss: 1.8257 - val_accuracy: 0.3811\n",
      "Epoch 69/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.7315 - accuracy: 0.4521 - val_loss: 1.8123 - val_accuracy: 0.3714\n",
      "Epoch 70/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7260 - accuracy: 0.4539 - val_loss: 1.6787 - val_accuracy: 0.4049\n",
      "Epoch 71/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7279 - accuracy: 0.4515 - val_loss: 1.5613 - val_accuracy: 0.4494\n",
      "Epoch 72/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.7165 - accuracy: 0.4592 - val_loss: 2.5170 - val_accuracy: 0.2235\n",
      "Epoch 73/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7188 - accuracy: 0.4542 - val_loss: 1.6491 - val_accuracy: 0.4142\n",
      "Epoch 74/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7111 - accuracy: 0.4609 - val_loss: 2.6899 - val_accuracy: 0.2221\n",
      "Epoch 75/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7090 - accuracy: 0.4597 - val_loss: 1.6203 - val_accuracy: 0.4293\n",
      "Epoch 76/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7008 - accuracy: 0.4651 - val_loss: 1.6219 - val_accuracy: 0.4230\n",
      "Epoch 77/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7040 - accuracy: 0.4598 - val_loss: 1.7510 - val_accuracy: 0.3769\n",
      "Epoch 78/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6966 - accuracy: 0.4667 - val_loss: 1.8203 - val_accuracy: 0.3515\n",
      "Epoch 79/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6934 - accuracy: 0.4640 - val_loss: 1.5970 - val_accuracy: 0.4372\n",
      "Epoch 80/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6883 - accuracy: 0.4685 - val_loss: 1.7075 - val_accuracy: 0.3837\n",
      "Epoch 81/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6863 - accuracy: 0.4671 - val_loss: 1.6865 - val_accuracy: 0.4243\n",
      "Epoch 82/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.6799 - accuracy: 0.4748 - val_loss: 1.5593 - val_accuracy: 0.4537\n",
      "Epoch 83/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6822 - accuracy: 0.4659 - val_loss: 1.6040 - val_accuracy: 0.4269\n",
      "Epoch 84/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6755 - accuracy: 0.4750 - val_loss: 1.5664 - val_accuracy: 0.4531\n",
      "Epoch 85/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6718 - accuracy: 0.4772 - val_loss: 1.6818 - val_accuracy: 0.3859\n",
      "Epoch 86/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.6682 - accuracy: 0.4766 - val_loss: 1.5781 - val_accuracy: 0.4379\n",
      "Epoch 87/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6644 - accuracy: 0.4792 - val_loss: 1.5859 - val_accuracy: 0.4428\n",
      "Epoch 88/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.6711 - accuracy: 0.4740 - val_loss: 1.6441 - val_accuracy: 0.4086\n",
      "Epoch 89/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6562 - accuracy: 0.4800 - val_loss: 1.4828 - val_accuracy: 0.4892\n",
      "Epoch 90/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6612 - accuracy: 0.4781 - val_loss: 1.4912 - val_accuracy: 0.4682\n",
      "Epoch 91/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6515 - accuracy: 0.4838 - val_loss: 1.6507 - val_accuracy: 0.4101\n",
      "Epoch 92/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6519 - accuracy: 0.4833 - val_loss: 1.7590 - val_accuracy: 0.3586\n",
      "Epoch 93/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6536 - accuracy: 0.4823 - val_loss: 1.5320 - val_accuracy: 0.4554\n",
      "Epoch 94/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6442 - accuracy: 0.4887 - val_loss: 1.5049 - val_accuracy: 0.4663\n",
      "Epoch 95/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6471 - accuracy: 0.4831 - val_loss: 1.5334 - val_accuracy: 0.4513\n",
      "Epoch 96/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6458 - accuracy: 0.4850 - val_loss: 1.4750 - val_accuracy: 0.4761\n",
      "Epoch 97/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6413 - accuracy: 0.4895 - val_loss: 1.4488 - val_accuracy: 0.4878\n",
      "Epoch 98/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6361 - accuracy: 0.4904 - val_loss: 1.4251 - val_accuracy: 0.5029\n",
      "Epoch 99/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6429 - accuracy: 0.4854 - val_loss: 1.4629 - val_accuracy: 0.4782\n",
      "Epoch 100/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6331 - accuracy: 0.4924 - val_loss: 1.4152 - val_accuracy: 0.5052\n",
      "Epoch 101/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6385 - accuracy: 0.4891 - val_loss: 1.4829 - val_accuracy: 0.4660\n",
      "Epoch 102/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.6342 - accuracy: 0.4912 - val_loss: 1.4378 - val_accuracy: 0.4883\n",
      "Epoch 103/108\n",
      "156/156 [==============================] - 9s 56ms/step - loss: 1.6321 - accuracy: 0.4892 - val_loss: 1.4624 - val_accuracy: 0.4773\n",
      "Epoch 104/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.6334 - accuracy: 0.4920 - val_loss: 1.4395 - val_accuracy: 0.4864\n",
      "Epoch 105/108\n",
      "156/156 [==============================] - 9s 56ms/step - loss: 1.6364 - accuracy: 0.4883 - val_loss: 1.4439 - val_accuracy: 0.4869\n",
      "Epoch 106/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6314 - accuracy: 0.4930 - val_loss: 1.4380 - val_accuracy: 0.4899\n",
      "Epoch 107/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6375 - accuracy: 0.4890 - val_loss: 1.4407 - val_accuracy: 0.4880\n",
      "Epoch 108/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6304 - accuracy: 0.4959 - val_loss: 1.4399 - val_accuracy: 0.4882\n",
      "10000/10000 [==============================] - 2s 158us/step\n",
      "Test loss: 1.4399333503723144\n",
      "Test accuracy: 0.48820000886917114\n",
      "x_train shape: (40000, 32, 32, 3)\n",
      "y_train shape: (40000, 10)\n",
      "40000 train samples\n",
      "10000 test samples\n",
      "10000 validation samples\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "ResNet20v1\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/108\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 3.1895 - accuracy: 0.1115 - val_loss: 13.7367 - val_accuracy: 0.1000\n",
      "Epoch 2/108\n",
      "  1/156 [..............................] - ETA: 11s - loss: 2.7927 - accuracy: 0.1250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheverazo/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 9s 60ms/step - loss: 2.3783 - accuracy: 0.1806 - val_loss: 2.9611 - val_accuracy: 0.0982\n",
      "Epoch 3/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 2.2577 - accuracy: 0.1910 - val_loss: 2.2231 - val_accuracy: 0.1732\n",
      "Epoch 4/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 2.1798 - accuracy: 0.2091 - val_loss: 2.2230 - val_accuracy: 0.1704\n",
      "Epoch 5/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 2.1618 - accuracy: 0.2225 - val_loss: 2.1632 - val_accuracy: 0.1963\n",
      "Epoch 6/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 2.1316 - accuracy: 0.2382 - val_loss: 6.7268 - val_accuracy: 0.1000\n",
      "Epoch 7/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 2.0922 - accuracy: 0.2591 - val_loss: 2.3380 - val_accuracy: 0.2128\n",
      "Epoch 8/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 2.0698 - accuracy: 0.2698 - val_loss: 2.2952 - val_accuracy: 0.1777\n",
      "Epoch 9/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 2.0641 - accuracy: 0.2746 - val_loss: 2.5913 - val_accuracy: 0.1469\n",
      "Epoch 10/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 2.0443 - accuracy: 0.2940 - val_loss: 2.6024 - val_accuracy: 0.1881\n",
      "Epoch 11/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 2.0268 - accuracy: 0.2991 - val_loss: 3.0834 - val_accuracy: 0.1447\n",
      "Epoch 12/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 2.0225 - accuracy: 0.3094 - val_loss: 27.0975 - val_accuracy: 0.1000\n",
      "Epoch 13/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 2.0142 - accuracy: 0.3159 - val_loss: 2.6027 - val_accuracy: 0.1523\n",
      "Epoch 14/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 2.0160 - accuracy: 0.3192 - val_loss: 2.2302 - val_accuracy: 0.2732\n",
      "Epoch 15/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 2.0023 - accuracy: 0.3269 - val_loss: 1.9968 - val_accuracy: 0.2834\n",
      "Epoch 16/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 2.0010 - accuracy: 0.3207 - val_loss: 2.1494 - val_accuracy: 0.2499\n",
      "Epoch 17/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 2.0046 - accuracy: 0.3255 - val_loss: 2.5374 - val_accuracy: 0.1733\n",
      "Epoch 18/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9976 - accuracy: 0.3234 - val_loss: 1.9070 - val_accuracy: 0.3047\n",
      "Epoch 19/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9882 - accuracy: 0.3287 - val_loss: 2.3062 - val_accuracy: 0.2117\n",
      "Epoch 20/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9881 - accuracy: 0.3257 - val_loss: 4.6572 - val_accuracy: 0.1312\n",
      "Epoch 21/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9848 - accuracy: 0.3290 - val_loss: 2.3224 - val_accuracy: 0.2115\n",
      "Epoch 22/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9810 - accuracy: 0.3302 - val_loss: 3.4135 - val_accuracy: 0.1650\n",
      "Epoch 23/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.9774 - accuracy: 0.3320 - val_loss: 1.9350 - val_accuracy: 0.2691\n",
      "Epoch 24/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9701 - accuracy: 0.3361 - val_loss: 2.0972 - val_accuracy: 0.2760\n",
      "Epoch 25/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9700 - accuracy: 0.3342 - val_loss: 5.1039 - val_accuracy: 0.2163\n",
      "Epoch 26/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9695 - accuracy: 0.3336 - val_loss: 1.8509 - val_accuracy: 0.3110\n",
      "Epoch 27/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9587 - accuracy: 0.3380 - val_loss: 2.3129 - val_accuracy: 0.2234\n",
      "Epoch 28/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9632 - accuracy: 0.3376 - val_loss: 2.9003 - val_accuracy: 0.1808\n",
      "Epoch 29/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9543 - accuracy: 0.3392 - val_loss: 2.0424 - val_accuracy: 0.2588\n",
      "Epoch 30/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9486 - accuracy: 0.3422 - val_loss: 2.7180 - val_accuracy: 0.2447\n",
      "Epoch 31/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9435 - accuracy: 0.3462 - val_loss: 2.3053 - val_accuracy: 0.3189\n",
      "Epoch 32/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9376 - accuracy: 0.3464 - val_loss: 2.2672 - val_accuracy: 0.2714\n",
      "Epoch 33/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9356 - accuracy: 0.3459 - val_loss: 2.5041 - val_accuracy: 0.1631\n",
      "Epoch 34/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.9318 - accuracy: 0.3474 - val_loss: 3.0070 - val_accuracy: 0.1502\n",
      "Epoch 35/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9250 - accuracy: 0.3505 - val_loss: 1.9860 - val_accuracy: 0.2529\n",
      "Epoch 36/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9217 - accuracy: 0.3503 - val_loss: 2.1714 - val_accuracy: 0.2250\n",
      "Epoch 37/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9243 - accuracy: 0.3506 - val_loss: 2.3869 - val_accuracy: 0.1900\n",
      "Epoch 38/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9147 - accuracy: 0.3555 - val_loss: 1.9302 - val_accuracy: 0.2793\n",
      "Epoch 39/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9123 - accuracy: 0.3533 - val_loss: 2.0429 - val_accuracy: 0.3148\n",
      "Epoch 40/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9005 - accuracy: 0.3597 - val_loss: 9.5180 - val_accuracy: 0.1451\n",
      "Epoch 41/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9043 - accuracy: 0.3582 - val_loss: 2.0174 - val_accuracy: 0.2438\n",
      "Epoch 42/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.8908 - accuracy: 0.3615 - val_loss: 1.9718 - val_accuracy: 0.2941\n",
      "Epoch 43/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8908 - accuracy: 0.3662 - val_loss: 2.2230 - val_accuracy: 0.2630\n",
      "Epoch 44/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8831 - accuracy: 0.3685 - val_loss: 2.0813 - val_accuracy: 0.2693\n",
      "Epoch 45/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.8860 - accuracy: 0.3635 - val_loss: 2.5079 - val_accuracy: 0.1965\n",
      "Epoch 46/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8727 - accuracy: 0.3687 - val_loss: 2.4319 - val_accuracy: 0.2011\n",
      "Epoch 47/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8668 - accuracy: 0.3717 - val_loss: 2.1136 - val_accuracy: 0.2701\n",
      "Epoch 48/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8606 - accuracy: 0.3740 - val_loss: 1.9109 - val_accuracy: 0.3227\n",
      "Epoch 49/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8603 - accuracy: 0.3767 - val_loss: 2.0393 - val_accuracy: 0.2730\n",
      "Epoch 50/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8542 - accuracy: 0.3787 - val_loss: 2.1920 - val_accuracy: 0.2370\n",
      "Epoch 51/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8485 - accuracy: 0.3831 - val_loss: 1.9812 - val_accuracy: 0.3118\n",
      "Epoch 52/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.8383 - accuracy: 0.3862 - val_loss: 2.0737 - val_accuracy: 0.2552\n",
      "Epoch 53/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8408 - accuracy: 0.3884 - val_loss: 2.2879 - val_accuracy: 0.2456\n",
      "Epoch 54/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8359 - accuracy: 0.3884 - val_loss: 1.9322 - val_accuracy: 0.3341\n",
      "Epoch 55/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8219 - accuracy: 0.3939 - val_loss: 1.8543 - val_accuracy: 0.3503\n",
      "Epoch 56/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.8225 - accuracy: 0.3939 - val_loss: 4.2100 - val_accuracy: 0.1125\n",
      "Epoch 57/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8130 - accuracy: 0.4019 - val_loss: 1.9460 - val_accuracy: 0.2873\n",
      "Epoch 58/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.8129 - accuracy: 0.3988 - val_loss: 2.1185 - val_accuracy: 0.2823\n",
      "Epoch 59/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.8052 - accuracy: 0.4001 - val_loss: 2.4349 - val_accuracy: 0.2098\n",
      "Epoch 60/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7977 - accuracy: 0.4055 - val_loss: 2.2312 - val_accuracy: 0.2493\n",
      "Epoch 61/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8006 - accuracy: 0.4025 - val_loss: 1.9548 - val_accuracy: 0.2945\n",
      "Epoch 62/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.7921 - accuracy: 0.4060 - val_loss: 5.9506 - val_accuracy: 0.1687\n",
      "Epoch 63/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7865 - accuracy: 0.4116 - val_loss: 2.1251 - val_accuracy: 0.2684\n",
      "Epoch 64/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7815 - accuracy: 0.4106 - val_loss: 3.1914 - val_accuracy: 0.1255\n",
      "Epoch 65/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7802 - accuracy: 0.4101 - val_loss: 1.7637 - val_accuracy: 0.3703\n",
      "Epoch 66/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.7729 - accuracy: 0.4135 - val_loss: 2.4213 - val_accuracy: 0.2482\n",
      "Epoch 67/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7718 - accuracy: 0.4132 - val_loss: 1.7881 - val_accuracy: 0.3558\n",
      "Epoch 68/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7618 - accuracy: 0.4161 - val_loss: 2.3706 - val_accuracy: 0.2686\n",
      "Epoch 69/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7605 - accuracy: 0.4182 - val_loss: 1.9901 - val_accuracy: 0.2727\n",
      "Epoch 70/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.7543 - accuracy: 0.4204 - val_loss: 1.8174 - val_accuracy: 0.3394\n",
      "Epoch 71/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7516 - accuracy: 0.4218 - val_loss: 1.8731 - val_accuracy: 0.3434\n",
      "Epoch 72/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7518 - accuracy: 0.4245 - val_loss: 1.7041 - val_accuracy: 0.3658\n",
      "Epoch 73/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7422 - accuracy: 0.4236 - val_loss: 1.8106 - val_accuracy: 0.3561\n",
      "Epoch 74/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7418 - accuracy: 0.4266 - val_loss: 1.6984 - val_accuracy: 0.3797\n",
      "Epoch 75/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7347 - accuracy: 0.4256 - val_loss: 1.6917 - val_accuracy: 0.3826\n",
      "Epoch 76/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7345 - accuracy: 0.4303 - val_loss: 1.7191 - val_accuracy: 0.3632\n",
      "Epoch 77/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7271 - accuracy: 0.4323 - val_loss: 1.6905 - val_accuracy: 0.3873\n",
      "Epoch 78/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7252 - accuracy: 0.4325 - val_loss: 1.6638 - val_accuracy: 0.4034\n",
      "Epoch 79/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7230 - accuracy: 0.4327 - val_loss: 1.6226 - val_accuracy: 0.4016\n",
      "Epoch 80/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7182 - accuracy: 0.4369 - val_loss: 1.8089 - val_accuracy: 0.3612\n",
      "Epoch 81/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7132 - accuracy: 0.4383 - val_loss: 1.8408 - val_accuracy: 0.3510\n",
      "Epoch 82/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7074 - accuracy: 0.4405 - val_loss: 1.6183 - val_accuracy: 0.4239\n",
      "Epoch 83/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7119 - accuracy: 0.4386 - val_loss: 2.3778 - val_accuracy: 0.2252\n",
      "Epoch 84/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7077 - accuracy: 0.4398 - val_loss: 1.6133 - val_accuracy: 0.4136\n",
      "Epoch 85/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.6995 - accuracy: 0.4456 - val_loss: 1.6507 - val_accuracy: 0.4003\n",
      "Epoch 86/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.6965 - accuracy: 0.4466 - val_loss: 1.6168 - val_accuracy: 0.4200\n",
      "Epoch 87/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6928 - accuracy: 0.4443 - val_loss: 1.8468 - val_accuracy: 0.3494\n",
      "Epoch 88/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6875 - accuracy: 0.4505 - val_loss: 1.5910 - val_accuracy: 0.4120\n",
      "Epoch 89/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6859 - accuracy: 0.4505 - val_loss: 1.5180 - val_accuracy: 0.4520\n",
      "Epoch 90/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6835 - accuracy: 0.4517 - val_loss: 1.6464 - val_accuracy: 0.4079\n",
      "Epoch 91/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6845 - accuracy: 0.4496 - val_loss: 1.5246 - val_accuracy: 0.4544\n",
      "Epoch 92/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.6755 - accuracy: 0.4587 - val_loss: 1.5962 - val_accuracy: 0.4254\n",
      "Epoch 93/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6796 - accuracy: 0.4543 - val_loss: 1.5190 - val_accuracy: 0.4451\n",
      "Epoch 94/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6742 - accuracy: 0.4554 - val_loss: 1.6117 - val_accuracy: 0.4180\n",
      "Epoch 95/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6723 - accuracy: 0.4580 - val_loss: 1.5306 - val_accuracy: 0.4475\n",
      "Epoch 96/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6668 - accuracy: 0.4595 - val_loss: 1.5589 - val_accuracy: 0.4317\n",
      "Epoch 97/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6667 - accuracy: 0.4587 - val_loss: 1.5931 - val_accuracy: 0.4163\n",
      "Epoch 98/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6705 - accuracy: 0.4602 - val_loss: 1.5136 - val_accuracy: 0.4537\n",
      "Epoch 99/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6675 - accuracy: 0.4599 - val_loss: 1.5122 - val_accuracy: 0.4477\n",
      "Epoch 100/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6591 - accuracy: 0.4640 - val_loss: 1.5116 - val_accuracy: 0.4411\n",
      "Epoch 101/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.6606 - accuracy: 0.4637 - val_loss: 1.5096 - val_accuracy: 0.4554\n",
      "Epoch 102/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6646 - accuracy: 0.4633 - val_loss: 1.5193 - val_accuracy: 0.4516\n",
      "Epoch 103/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6626 - accuracy: 0.4604 - val_loss: 1.5113 - val_accuracy: 0.4484\n",
      "Epoch 104/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6567 - accuracy: 0.4647 - val_loss: 1.4926 - val_accuracy: 0.4567\n",
      "Epoch 105/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6594 - accuracy: 0.4624 - val_loss: 1.4914 - val_accuracy: 0.4567\n",
      "Epoch 106/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6541 - accuracy: 0.4682 - val_loss: 1.4953 - val_accuracy: 0.4526\n",
      "Epoch 107/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6626 - accuracy: 0.4604 - val_loss: 1.4953 - val_accuracy: 0.4513\n",
      "Epoch 108/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6579 - accuracy: 0.4638 - val_loss: 1.4951 - val_accuracy: 0.4516\n",
      "10000/10000 [==============================] - 2s 199us/step\n",
      "Test loss: 1.4950633722305298\n",
      "Test accuracy: 0.45159998536109924\n",
      "x_train shape: (40000, 32, 32, 3)\n",
      "y_train shape: (40000, 10)\n",
      "40000 train samples\n",
      "10000 test samples\n",
      "10000 validation samples\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "ResNet20v1\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/108\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 3.6686 - accuracy: 0.1062 - val_loss: 2.5828 - val_accuracy: 0.1000\n",
      "Epoch 2/108\n",
      "  1/156 [..............................] - ETA: 10s - loss: 2.4993 - accuracy: 0.0898"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheverazo/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 10s 62ms/step - loss: 2.4612 - accuracy: 0.1318 - val_loss: 3.8782 - val_accuracy: 0.1000\n",
      "Epoch 3/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 2.3049 - accuracy: 0.1855 - val_loss: 25.7020 - val_accuracy: 0.0599\n",
      "Epoch 4/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 2.1968 - accuracy: 0.2077 - val_loss: 2.5839 - val_accuracy: 0.1266\n",
      "Epoch 5/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 2.1385 - accuracy: 0.2370 - val_loss: 2.6231 - val_accuracy: 0.1204\n",
      "Epoch 6/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 2.1060 - accuracy: 0.2609 - val_loss: 2.8197 - val_accuracy: 0.2326\n",
      "Epoch 7/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 2.0764 - accuracy: 0.2733 - val_loss: 3.8527 - val_accuracy: 0.1449\n",
      "Epoch 8/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 2.0618 - accuracy: 0.2830 - val_loss: 3.0043 - val_accuracy: 0.1622\n",
      "Epoch 9/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 2.0375 - accuracy: 0.2940 - val_loss: 2.3069 - val_accuracy: 0.2203\n",
      "Epoch 10/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 2.0316 - accuracy: 0.3073 - val_loss: 5.2781 - val_accuracy: 0.1622\n",
      "Epoch 11/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 2.0199 - accuracy: 0.3121 - val_loss: 2.0290 - val_accuracy: 0.2686\n",
      "Epoch 12/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 2.0132 - accuracy: 0.3228 - val_loss: 2.4905 - val_accuracy: 0.1443\n",
      "Epoch 13/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 2.0074 - accuracy: 0.3219 - val_loss: 1.8477 - val_accuracy: 0.3183\n",
      "Epoch 14/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 2.0015 - accuracy: 0.3201 - val_loss: 2.0091 - val_accuracy: 0.2757\n",
      "Epoch 15/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9925 - accuracy: 0.3313 - val_loss: 2.3781 - val_accuracy: 0.1976\n",
      "Epoch 16/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9936 - accuracy: 0.3292 - val_loss: 2.4813 - val_accuracy: 0.1901\n",
      "Epoch 17/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9856 - accuracy: 0.3351 - val_loss: 3.0023 - val_accuracy: 0.2143\n",
      "Epoch 18/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9846 - accuracy: 0.3358 - val_loss: 1.9496 - val_accuracy: 0.3093\n",
      "Epoch 19/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9786 - accuracy: 0.3397 - val_loss: 3.6281 - val_accuracy: 0.1316\n",
      "Epoch 20/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9765 - accuracy: 0.3439 - val_loss: 2.6585 - val_accuracy: 0.1596\n",
      "Epoch 21/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9691 - accuracy: 0.3400 - val_loss: 2.5753 - val_accuracy: 0.2524\n",
      "Epoch 22/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9616 - accuracy: 0.3486 - val_loss: 2.0656 - val_accuracy: 0.2669\n",
      "Epoch 23/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9625 - accuracy: 0.3482 - val_loss: 2.3341 - val_accuracy: 0.2112\n",
      "Epoch 24/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.9543 - accuracy: 0.3494 - val_loss: 2.8009 - val_accuracy: 0.1731\n",
      "Epoch 25/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9514 - accuracy: 0.3501 - val_loss: 2.2199 - val_accuracy: 0.2366\n",
      "Epoch 26/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9517 - accuracy: 0.3525 - val_loss: 3.9323 - val_accuracy: 0.1287\n",
      "Epoch 27/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9397 - accuracy: 0.3542 - val_loss: 2.0155 - val_accuracy: 0.2862\n",
      "Epoch 28/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9365 - accuracy: 0.3579 - val_loss: 2.6162 - val_accuracy: 0.2369\n",
      "Epoch 29/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.9328 - accuracy: 0.3621 - val_loss: 3.2963 - val_accuracy: 0.2053\n",
      "Epoch 30/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9213 - accuracy: 0.3657 - val_loss: 2.1088 - val_accuracy: 0.2777\n",
      "Epoch 31/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9254 - accuracy: 0.3643 - val_loss: 2.6433 - val_accuracy: 0.2252\n",
      "Epoch 32/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.9170 - accuracy: 0.3677 - val_loss: 2.2116 - val_accuracy: 0.2439\n",
      "Epoch 33/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.9148 - accuracy: 0.3724 - val_loss: 2.1889 - val_accuracy: 0.2429\n",
      "Epoch 34/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9024 - accuracy: 0.3737 - val_loss: 2.4321 - val_accuracy: 0.2630\n",
      "Epoch 35/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8977 - accuracy: 0.3728 - val_loss: 2.2253 - val_accuracy: 0.2613\n",
      "Epoch 36/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9027 - accuracy: 0.3753 - val_loss: 2.4775 - val_accuracy: 0.1926\n",
      "Epoch 37/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8861 - accuracy: 0.3834 - val_loss: 2.2741 - val_accuracy: 0.2383\n",
      "Epoch 38/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8848 - accuracy: 0.3818 - val_loss: 1.9937 - val_accuracy: 0.2911\n",
      "Epoch 39/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8800 - accuracy: 0.3830 - val_loss: 4.2064 - val_accuracy: 0.1700\n",
      "Epoch 40/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8738 - accuracy: 0.3859 - val_loss: 2.1037 - val_accuracy: 0.2335\n",
      "Epoch 41/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8735 - accuracy: 0.3868 - val_loss: 2.1009 - val_accuracy: 0.2571\n",
      "Epoch 42/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8660 - accuracy: 0.3893 - val_loss: 2.7694 - val_accuracy: 0.2235\n",
      "Epoch 43/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8689 - accuracy: 0.3889 - val_loss: 2.3480 - val_accuracy: 0.2748\n",
      "Epoch 44/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8633 - accuracy: 0.3920 - val_loss: 3.1359 - val_accuracy: 0.1539\n",
      "Epoch 45/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8581 - accuracy: 0.3927 - val_loss: 1.7998 - val_accuracy: 0.3570\n",
      "Epoch 46/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8461 - accuracy: 0.3961 - val_loss: 1.9764 - val_accuracy: 0.2974\n",
      "Epoch 47/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8405 - accuracy: 0.3959 - val_loss: 3.2936 - val_accuracy: 0.1842\n",
      "Epoch 48/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8431 - accuracy: 0.3972 - val_loss: 1.9824 - val_accuracy: 0.3020\n",
      "Epoch 49/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8394 - accuracy: 0.4003 - val_loss: 1.9649 - val_accuracy: 0.3041\n",
      "Epoch 50/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8355 - accuracy: 0.4021 - val_loss: 2.5884 - val_accuracy: 0.1854\n",
      "Epoch 51/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8286 - accuracy: 0.4025 - val_loss: 2.0409 - val_accuracy: 0.3270\n",
      "Epoch 52/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8264 - accuracy: 0.4043 - val_loss: 2.1402 - val_accuracy: 0.2910\n",
      "Epoch 53/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8114 - accuracy: 0.4144 - val_loss: 2.4257 - val_accuracy: 0.1893\n",
      "Epoch 54/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8136 - accuracy: 0.4110 - val_loss: 2.5203 - val_accuracy: 0.1399\n",
      "Epoch 55/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8060 - accuracy: 0.4143 - val_loss: 1.8587 - val_accuracy: 0.3362\n",
      "Epoch 56/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7957 - accuracy: 0.4203 - val_loss: 1.7721 - val_accuracy: 0.3733\n",
      "Epoch 57/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7946 - accuracy: 0.4181 - val_loss: 1.7964 - val_accuracy: 0.3543\n",
      "Epoch 58/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.7931 - accuracy: 0.4174 - val_loss: 1.7615 - val_accuracy: 0.3474\n",
      "Epoch 59/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.7797 - accuracy: 0.4246 - val_loss: 1.6966 - val_accuracy: 0.3828\n",
      "Epoch 60/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7791 - accuracy: 0.4282 - val_loss: 1.7865 - val_accuracy: 0.3746\n",
      "Epoch 61/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7785 - accuracy: 0.4252 - val_loss: 2.1307 - val_accuracy: 0.2598\n",
      "Epoch 62/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7722 - accuracy: 0.4267 - val_loss: 1.9529 - val_accuracy: 0.3228\n",
      "Epoch 63/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.7673 - accuracy: 0.4318 - val_loss: 1.7571 - val_accuracy: 0.3610\n",
      "Epoch 64/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7660 - accuracy: 0.4325 - val_loss: 1.6346 - val_accuracy: 0.4188\n",
      "Epoch 65/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7549 - accuracy: 0.4325 - val_loss: 1.6426 - val_accuracy: 0.4056\n",
      "Epoch 66/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.7524 - accuracy: 0.4363 - val_loss: 1.6569 - val_accuracy: 0.4076\n",
      "Epoch 67/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.7445 - accuracy: 0.4379 - val_loss: 1.9608 - val_accuracy: 0.3028\n",
      "Epoch 68/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7448 - accuracy: 0.4381 - val_loss: 1.6898 - val_accuracy: 0.3933\n",
      "Epoch 69/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7441 - accuracy: 0.4373 - val_loss: 1.7731 - val_accuracy: 0.3706\n",
      "Epoch 70/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7380 - accuracy: 0.4415 - val_loss: 1.6738 - val_accuracy: 0.4053\n",
      "Epoch 71/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7311 - accuracy: 0.4440 - val_loss: 1.7497 - val_accuracy: 0.3713\n",
      "Epoch 72/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7310 - accuracy: 0.4439 - val_loss: 1.6907 - val_accuracy: 0.3952\n",
      "Epoch 73/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7234 - accuracy: 0.4465 - val_loss: 1.9092 - val_accuracy: 0.3206\n",
      "Epoch 74/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.7239 - accuracy: 0.4475 - val_loss: 2.2837 - val_accuracy: 0.2235\n",
      "Epoch 75/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7199 - accuracy: 0.4466 - val_loss: 1.8325 - val_accuracy: 0.3531\n",
      "Epoch 76/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7083 - accuracy: 0.4506 - val_loss: 1.6169 - val_accuracy: 0.3997\n",
      "Epoch 77/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7122 - accuracy: 0.4508 - val_loss: 1.7952 - val_accuracy: 0.3533\n",
      "Epoch 78/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7050 - accuracy: 0.4561 - val_loss: 1.8016 - val_accuracy: 0.3699\n",
      "Epoch 79/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6988 - accuracy: 0.4601 - val_loss: 1.5768 - val_accuracy: 0.4241\n",
      "Epoch 80/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6989 - accuracy: 0.4590 - val_loss: 1.7328 - val_accuracy: 0.3743\n",
      "Epoch 81/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6987 - accuracy: 0.4586 - val_loss: 1.6510 - val_accuracy: 0.3894\n",
      "Epoch 82/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6894 - accuracy: 0.4623 - val_loss: 1.5928 - val_accuracy: 0.4200\n",
      "Epoch 83/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6888 - accuracy: 0.4589 - val_loss: 1.5115 - val_accuracy: 0.4562\n",
      "Epoch 84/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6826 - accuracy: 0.4624 - val_loss: 1.5236 - val_accuracy: 0.4583\n",
      "Epoch 85/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.6847 - accuracy: 0.4624 - val_loss: 1.5990 - val_accuracy: 0.4271\n",
      "Epoch 86/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6733 - accuracy: 0.4690 - val_loss: 1.6560 - val_accuracy: 0.4116\n",
      "Epoch 87/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.6736 - accuracy: 0.4696 - val_loss: 1.5450 - val_accuracy: 0.4491\n",
      "Epoch 88/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6690 - accuracy: 0.4708 - val_loss: 1.5461 - val_accuracy: 0.4418\n",
      "Epoch 89/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.6638 - accuracy: 0.4713 - val_loss: 1.5886 - val_accuracy: 0.4419\n",
      "Epoch 90/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6649 - accuracy: 0.4719 - val_loss: 1.5514 - val_accuracy: 0.4588\n",
      "Epoch 91/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.6566 - accuracy: 0.4766 - val_loss: 1.5509 - val_accuracy: 0.4413\n",
      "Epoch 92/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6591 - accuracy: 0.4753 - val_loss: 1.5181 - val_accuracy: 0.4609\n",
      "Epoch 93/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6539 - accuracy: 0.4781 - val_loss: 1.5815 - val_accuracy: 0.4467\n",
      "Epoch 94/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6529 - accuracy: 0.4782 - val_loss: 1.5120 - val_accuracy: 0.4497\n",
      "Epoch 95/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6498 - accuracy: 0.4794 - val_loss: 1.4527 - val_accuracy: 0.4896\n",
      "Epoch 96/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6477 - accuracy: 0.4802 - val_loss: 1.4885 - val_accuracy: 0.4599\n",
      "Epoch 97/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6464 - accuracy: 0.4827 - val_loss: 1.4914 - val_accuracy: 0.4653\n",
      "Epoch 98/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6412 - accuracy: 0.4840 - val_loss: 1.5160 - val_accuracy: 0.4542\n",
      "Epoch 99/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6461 - accuracy: 0.4822 - val_loss: 1.4593 - val_accuracy: 0.4846\n",
      "Epoch 100/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6366 - accuracy: 0.4889 - val_loss: 1.4650 - val_accuracy: 0.4817\n",
      "Epoch 101/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6375 - accuracy: 0.4828 - val_loss: 1.4504 - val_accuracy: 0.4842\n",
      "Epoch 102/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6409 - accuracy: 0.4853 - val_loss: 1.4542 - val_accuracy: 0.4817\n",
      "Epoch 103/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6355 - accuracy: 0.4860 - val_loss: 1.4391 - val_accuracy: 0.4902\n",
      "Epoch 104/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6351 - accuracy: 0.4868 - val_loss: 1.4297 - val_accuracy: 0.4977\n",
      "Epoch 105/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6343 - accuracy: 0.4873 - val_loss: 1.4394 - val_accuracy: 0.4881\n",
      "Epoch 106/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.6350 - accuracy: 0.4871 - val_loss: 1.4405 - val_accuracy: 0.4919\n",
      "Epoch 107/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6325 - accuracy: 0.4864 - val_loss: 1.4382 - val_accuracy: 0.4929\n",
      "Epoch 108/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6378 - accuracy: 0.4842 - val_loss: 1.4370 - val_accuracy: 0.4937\n",
      "10000/10000 [==============================] - 2s 170us/step\n",
      "Test loss: 1.43698367729187\n",
      "Test accuracy: 0.4936999976634979\n",
      "x_train shape: (40000, 32, 32, 3)\n",
      "y_train shape: (40000, 10)\n",
      "40000 train samples\n",
      "10000 test samples\n",
      "10000 validation samples\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "ResNet20v1\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/108\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 4.1234 - accuracy: 0.1075 - val_loss: 6.9623 - val_accuracy: 0.0994\n",
      "Epoch 2/108\n",
      "  1/156 [..............................] - ETA: 11s - loss: 2.5863 - accuracy: 0.0742"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheverazo/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 10s 61ms/step - loss: 2.4477 - accuracy: 0.1523 - val_loss: 51.4236 - val_accuracy: 0.1203\n",
      "Epoch 3/108\n",
      "156/156 [==============================] - 10s 63ms/step - loss: 2.3014 - accuracy: 0.1920 - val_loss: 8.5428 - val_accuracy: 0.1000\n",
      "Epoch 4/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 2.1841 - accuracy: 0.2369 - val_loss: 2.8826 - val_accuracy: 0.1123\n",
      "Epoch 5/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 2.1317 - accuracy: 0.2638 - val_loss: 2.3716 - val_accuracy: 0.1663\n",
      "Epoch 6/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 2.0962 - accuracy: 0.2774 - val_loss: 2.4581 - val_accuracy: 0.1709\n",
      "Epoch 7/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 2.0669 - accuracy: 0.2933 - val_loss: 27.2965 - val_accuracy: 0.1534\n",
      "Epoch 8/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 2.0575 - accuracy: 0.2952 - val_loss: 2.6591 - val_accuracy: 0.1746\n",
      "Epoch 9/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 2.0431 - accuracy: 0.3041 - val_loss: 2.2337 - val_accuracy: 0.2061\n",
      "Epoch 10/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 2.0391 - accuracy: 0.3022 - val_loss: 2.4433 - val_accuracy: 0.1692\n",
      "Epoch 11/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 2.0338 - accuracy: 0.3106 - val_loss: 3.4436 - val_accuracy: 0.1238\n",
      "Epoch 12/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 2.0253 - accuracy: 0.3149 - val_loss: 2.0952 - val_accuracy: 0.2662\n",
      "Epoch 13/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 2.0295 - accuracy: 0.3130 - val_loss: 3.4154 - val_accuracy: 0.1521\n",
      "Epoch 14/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 2.0201 - accuracy: 0.3200 - val_loss: 2.1072 - val_accuracy: 0.2267\n",
      "Epoch 15/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 2.0161 - accuracy: 0.3201 - val_loss: 2.0529 - val_accuracy: 0.2733\n",
      "Epoch 16/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 2.0138 - accuracy: 0.3241 - val_loss: 2.2006 - val_accuracy: 0.2628\n",
      "Epoch 17/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 2.0109 - accuracy: 0.3236 - val_loss: 3.2896 - val_accuracy: 0.1752\n",
      "Epoch 18/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 2.0072 - accuracy: 0.3297 - val_loss: 2.1255 - val_accuracy: 0.2563\n",
      "Epoch 19/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 2.0050 - accuracy: 0.3285 - val_loss: 2.1449 - val_accuracy: 0.2505\n",
      "Epoch 20/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 2.0001 - accuracy: 0.3334 - val_loss: 2.2088 - val_accuracy: 0.2566\n",
      "Epoch 21/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9989 - accuracy: 0.3319 - val_loss: 2.0893 - val_accuracy: 0.2651\n",
      "Epoch 22/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 2.0034 - accuracy: 0.3329 - val_loss: 2.3092 - val_accuracy: 0.2394\n",
      "Epoch 23/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9910 - accuracy: 0.3365 - val_loss: 2.0906 - val_accuracy: 0.2639\n",
      "Epoch 24/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9892 - accuracy: 0.3379 - val_loss: 3.3280 - val_accuracy: 0.1143\n",
      "Epoch 25/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9824 - accuracy: 0.3370 - val_loss: 2.2501 - val_accuracy: 0.2674\n",
      "Epoch 26/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.9821 - accuracy: 0.3414 - val_loss: 2.7590 - val_accuracy: 0.1867\n",
      "Epoch 27/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9679 - accuracy: 0.3428 - val_loss: 2.4151 - val_accuracy: 0.2462\n",
      "Epoch 28/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9664 - accuracy: 0.3443 - val_loss: 2.1563 - val_accuracy: 0.2462\n",
      "Epoch 29/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9661 - accuracy: 0.3470 - val_loss: 2.1785 - val_accuracy: 0.2806\n",
      "Epoch 30/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9585 - accuracy: 0.3490 - val_loss: 2.3058 - val_accuracy: 0.2556\n",
      "Epoch 31/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9573 - accuracy: 0.3474 - val_loss: 4.0596 - val_accuracy: 0.1610\n",
      "Epoch 32/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9556 - accuracy: 0.3509 - val_loss: 2.8129 - val_accuracy: 0.2161\n",
      "Epoch 33/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9498 - accuracy: 0.3530 - val_loss: 2.8797 - val_accuracy: 0.1987\n",
      "Epoch 34/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9466 - accuracy: 0.3591 - val_loss: 2.0496 - val_accuracy: 0.2571\n",
      "Epoch 35/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9379 - accuracy: 0.3597 - val_loss: 4.1414 - val_accuracy: 0.1568\n",
      "Epoch 36/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.9350 - accuracy: 0.3629 - val_loss: 2.1270 - val_accuracy: 0.2444\n",
      "Epoch 37/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9290 - accuracy: 0.3645 - val_loss: 1.9271 - val_accuracy: 0.2978\n",
      "Epoch 38/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9271 - accuracy: 0.3656 - val_loss: 2.1509 - val_accuracy: 0.2811\n",
      "Epoch 39/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.9162 - accuracy: 0.3689 - val_loss: 2.4969 - val_accuracy: 0.2032\n",
      "Epoch 40/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9122 - accuracy: 0.3697 - val_loss: 2.4657 - val_accuracy: 0.2264\n",
      "Epoch 41/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9111 - accuracy: 0.3714 - val_loss: 1.8410 - val_accuracy: 0.3587\n",
      "Epoch 42/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9056 - accuracy: 0.3744 - val_loss: 2.2949 - val_accuracy: 0.2394\n",
      "Epoch 43/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.8970 - accuracy: 0.3765 - val_loss: 1.9379 - val_accuracy: 0.2959\n",
      "Epoch 44/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8895 - accuracy: 0.3811 - val_loss: 1.9202 - val_accuracy: 0.3291\n",
      "Epoch 45/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8913 - accuracy: 0.3784 - val_loss: 2.3491 - val_accuracy: 0.2410\n",
      "Epoch 46/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8857 - accuracy: 0.3767 - val_loss: 2.0621 - val_accuracy: 0.2911\n",
      "Epoch 47/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8719 - accuracy: 0.3841 - val_loss: 1.9501 - val_accuracy: 0.3122\n",
      "Epoch 48/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8779 - accuracy: 0.3819 - val_loss: 2.3834 - val_accuracy: 0.2682\n",
      "Epoch 49/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.8709 - accuracy: 0.3838 - val_loss: 2.0390 - val_accuracy: 0.3020\n",
      "Epoch 50/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8667 - accuracy: 0.3870 - val_loss: 1.7959 - val_accuracy: 0.3680\n",
      "Epoch 51/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.8632 - accuracy: 0.3880 - val_loss: 2.4215 - val_accuracy: 0.2214\n",
      "Epoch 52/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8509 - accuracy: 0.3929 - val_loss: 1.9249 - val_accuracy: 0.3029\n",
      "Epoch 53/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8506 - accuracy: 0.3912 - val_loss: 2.0178 - val_accuracy: 0.2542\n",
      "Epoch 54/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8521 - accuracy: 0.3911 - val_loss: 1.8360 - val_accuracy: 0.3463\n",
      "Epoch 55/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8366 - accuracy: 0.3960 - val_loss: 2.8745 - val_accuracy: 0.1973\n",
      "Epoch 56/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8408 - accuracy: 0.3938 - val_loss: 2.6217 - val_accuracy: 0.1721\n",
      "Epoch 57/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.8263 - accuracy: 0.4023 - val_loss: 2.3435 - val_accuracy: 0.2645\n",
      "Epoch 58/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.8274 - accuracy: 0.3976 - val_loss: 3.8281 - val_accuracy: 0.1500\n",
      "Epoch 59/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8215 - accuracy: 0.4023 - val_loss: 2.0014 - val_accuracy: 0.2920\n",
      "Epoch 60/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.8177 - accuracy: 0.4049 - val_loss: 1.8186 - val_accuracy: 0.3689\n",
      "Epoch 61/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8117 - accuracy: 0.4058 - val_loss: 1.8250 - val_accuracy: 0.3287\n",
      "Epoch 62/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8067 - accuracy: 0.4078 - val_loss: 1.6486 - val_accuracy: 0.4105\n",
      "Epoch 63/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.8030 - accuracy: 0.4062 - val_loss: 2.1078 - val_accuracy: 0.2777\n",
      "Epoch 64/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7979 - accuracy: 0.4098 - val_loss: 3.1572 - val_accuracy: 0.1808\n",
      "Epoch 65/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7944 - accuracy: 0.4107 - val_loss: 1.9329 - val_accuracy: 0.3257\n",
      "Epoch 66/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7816 - accuracy: 0.4152 - val_loss: 1.8645 - val_accuracy: 0.3602\n",
      "Epoch 67/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7878 - accuracy: 0.4129 - val_loss: 1.8812 - val_accuracy: 0.3514\n",
      "Epoch 68/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7806 - accuracy: 0.4162 - val_loss: 2.5993 - val_accuracy: 0.2056\n",
      "Epoch 69/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7751 - accuracy: 0.4198 - val_loss: 2.1793 - val_accuracy: 0.2752\n",
      "Epoch 70/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7769 - accuracy: 0.4165 - val_loss: 1.9668 - val_accuracy: 0.3308\n",
      "Epoch 71/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.7622 - accuracy: 0.4256 - val_loss: 1.9098 - val_accuracy: 0.3125\n",
      "Epoch 72/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7693 - accuracy: 0.4204 - val_loss: 2.0986 - val_accuracy: 0.2669\n",
      "Epoch 73/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7586 - accuracy: 0.4261 - val_loss: 1.5802 - val_accuracy: 0.4282\n",
      "Epoch 74/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7587 - accuracy: 0.4258 - val_loss: 2.6628 - val_accuracy: 0.2026\n",
      "Epoch 75/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7508 - accuracy: 0.4281 - val_loss: 1.6013 - val_accuracy: 0.4211\n",
      "Epoch 76/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7475 - accuracy: 0.4300 - val_loss: 1.7140 - val_accuracy: 0.3850\n",
      "Epoch 77/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7456 - accuracy: 0.4287 - val_loss: 1.9976 - val_accuracy: 0.2496\n",
      "Epoch 78/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7387 - accuracy: 0.4320 - val_loss: 1.7940 - val_accuracy: 0.3662\n",
      "Epoch 79/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7392 - accuracy: 0.4327 - val_loss: 1.9825 - val_accuracy: 0.3047\n",
      "Epoch 80/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7289 - accuracy: 0.4382 - val_loss: 1.6009 - val_accuracy: 0.4206\n",
      "Epoch 81/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7251 - accuracy: 0.4400 - val_loss: 1.7421 - val_accuracy: 0.3867\n",
      "Epoch 82/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7209 - accuracy: 0.4408 - val_loss: 1.8304 - val_accuracy: 0.3579\n",
      "Epoch 83/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7236 - accuracy: 0.4393 - val_loss: 1.9266 - val_accuracy: 0.3460\n",
      "Epoch 84/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7170 - accuracy: 0.4435 - val_loss: 1.9129 - val_accuracy: 0.3403\n",
      "Epoch 85/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7142 - accuracy: 0.4458 - val_loss: 1.6567 - val_accuracy: 0.3998\n",
      "Epoch 86/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7101 - accuracy: 0.4484 - val_loss: 1.6350 - val_accuracy: 0.4082\n",
      "Epoch 87/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7063 - accuracy: 0.4492 - val_loss: 1.6694 - val_accuracy: 0.3997\n",
      "Epoch 88/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7025 - accuracy: 0.4504 - val_loss: 1.5556 - val_accuracy: 0.4470\n",
      "Epoch 89/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7028 - accuracy: 0.4506 - val_loss: 1.5592 - val_accuracy: 0.4453\n",
      "Epoch 90/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.6999 - accuracy: 0.4513 - val_loss: 1.6647 - val_accuracy: 0.3755\n",
      "Epoch 91/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.6938 - accuracy: 0.4521 - val_loss: 1.7109 - val_accuracy: 0.3814\n",
      "Epoch 92/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6826 - accuracy: 0.4574 - val_loss: 1.5795 - val_accuracy: 0.4357\n",
      "Epoch 93/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6889 - accuracy: 0.4610 - val_loss: 1.5053 - val_accuracy: 0.4459\n",
      "Epoch 94/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6876 - accuracy: 0.4553 - val_loss: 1.5778 - val_accuracy: 0.4379\n",
      "Epoch 95/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6793 - accuracy: 0.4606 - val_loss: 1.5224 - val_accuracy: 0.4495\n",
      "Epoch 96/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6816 - accuracy: 0.4588 - val_loss: 1.5122 - val_accuracy: 0.4453\n",
      "Epoch 97/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6789 - accuracy: 0.4629 - val_loss: 1.5275 - val_accuracy: 0.4443\n",
      "Epoch 98/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.6766 - accuracy: 0.4603 - val_loss: 1.6279 - val_accuracy: 0.4067\n",
      "Epoch 99/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6783 - accuracy: 0.4591 - val_loss: 1.4886 - val_accuracy: 0.4602\n",
      "Epoch 100/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6711 - accuracy: 0.4637 - val_loss: 1.4696 - val_accuracy: 0.4646\n",
      "Epoch 101/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.6696 - accuracy: 0.4640 - val_loss: 1.4788 - val_accuracy: 0.4649\n",
      "Epoch 102/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6722 - accuracy: 0.4639 - val_loss: 1.5181 - val_accuracy: 0.4549\n",
      "Epoch 103/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.6722 - accuracy: 0.4657 - val_loss: 1.5014 - val_accuracy: 0.4546\n",
      "Epoch 104/108\n",
      "156/156 [==============================] - 9s 56ms/step - loss: 1.6647 - accuracy: 0.4677 - val_loss: 1.4751 - val_accuracy: 0.4700\n",
      "Epoch 105/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6645 - accuracy: 0.4663 - val_loss: 1.4809 - val_accuracy: 0.4657\n",
      "Epoch 106/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.6724 - accuracy: 0.4631 - val_loss: 1.4742 - val_accuracy: 0.4638\n",
      "Epoch 107/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6658 - accuracy: 0.4664 - val_loss: 1.4764 - val_accuracy: 0.4657\n",
      "Epoch 108/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6667 - accuracy: 0.4691 - val_loss: 1.4757 - val_accuracy: 0.4673\n",
      "10000/10000 [==============================] - 2s 197us/step\n",
      "Test loss: 1.4757359897613525\n",
      "Test accuracy: 0.4672999978065491\n",
      "x_train shape: (40000, 32, 32, 3)\n",
      "y_train shape: (40000, 10)\n",
      "40000 train samples\n",
      "10000 test samples\n",
      "10000 validation samples\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "ResNet20v1\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/108\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 3.7033 - accuracy: 0.1341 - val_loss: 2.4485 - val_accuracy: 0.1014\n",
      "Epoch 2/108\n",
      "  1/156 [..............................] - ETA: 7s - loss: 2.3028 - accuracy: 0.1875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheverazo/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 9s 60ms/step - loss: 2.2927 - accuracy: 0.1994 - val_loss: 3.5116 - val_accuracy: 0.1124\n",
      "Epoch 3/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 2.2265 - accuracy: 0.2168 - val_loss: 2.9758 - val_accuracy: 0.1208\n",
      "Epoch 4/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 2.1344 - accuracy: 0.2527 - val_loss: 2.3575 - val_accuracy: 0.2245\n",
      "Epoch 5/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 2.0788 - accuracy: 0.2841 - val_loss: 3.6212 - val_accuracy: 0.1367\n",
      "Epoch 6/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 2.0600 - accuracy: 0.2957 - val_loss: 3.5719 - val_accuracy: 0.1927\n",
      "Epoch 7/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 2.0452 - accuracy: 0.3040 - val_loss: 4.0635 - val_accuracy: 0.1001\n",
      "Epoch 8/108\n",
      "156/156 [==============================] - 10s 63ms/step - loss: 2.0391 - accuracy: 0.3102 - val_loss: 2.0226 - val_accuracy: 0.2685\n",
      "Epoch 9/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 2.0284 - accuracy: 0.3170 - val_loss: 4.0148 - val_accuracy: 0.1719\n",
      "Epoch 10/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 2.0181 - accuracy: 0.3207 - val_loss: 2.0236 - val_accuracy: 0.2839\n",
      "Epoch 11/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 2.0107 - accuracy: 0.3280 - val_loss: 2.1567 - val_accuracy: 0.2413\n",
      "Epoch 12/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9976 - accuracy: 0.3375 - val_loss: 5.3255 - val_accuracy: 0.1226\n",
      "Epoch 13/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9916 - accuracy: 0.3395 - val_loss: 1.9594 - val_accuracy: 0.2950\n",
      "Epoch 14/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9877 - accuracy: 0.3451 - val_loss: 1.8720 - val_accuracy: 0.3394\n",
      "Epoch 15/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9837 - accuracy: 0.3471 - val_loss: 2.2631 - val_accuracy: 0.2246\n",
      "Epoch 16/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9854 - accuracy: 0.3481 - val_loss: 7.2490 - val_accuracy: 0.1020\n",
      "Epoch 17/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9766 - accuracy: 0.3483 - val_loss: 2.2031 - val_accuracy: 0.2567\n",
      "Epoch 18/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9747 - accuracy: 0.3491 - val_loss: 2.0141 - val_accuracy: 0.2630\n",
      "Epoch 19/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9682 - accuracy: 0.3576 - val_loss: 2.1621 - val_accuracy: 0.2948\n",
      "Epoch 20/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9672 - accuracy: 0.3550 - val_loss: 5.2393 - val_accuracy: 0.1084\n",
      "Epoch 21/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9645 - accuracy: 0.3581 - val_loss: 2.3320 - val_accuracy: 0.2286\n",
      "Epoch 22/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9604 - accuracy: 0.3622 - val_loss: 2.0092 - val_accuracy: 0.2629\n",
      "Epoch 23/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9504 - accuracy: 0.3656 - val_loss: 3.4245 - val_accuracy: 0.1198\n",
      "Epoch 24/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9486 - accuracy: 0.3653 - val_loss: 3.6367 - val_accuracy: 0.1852\n",
      "Epoch 25/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9392 - accuracy: 0.3751 - val_loss: 7.3133 - val_accuracy: 0.1008\n",
      "Epoch 26/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.9330 - accuracy: 0.3753 - val_loss: 3.6986 - val_accuracy: 0.1397\n",
      "Epoch 27/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9363 - accuracy: 0.3716 - val_loss: 2.2397 - val_accuracy: 0.2776\n",
      "Epoch 28/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.9280 - accuracy: 0.3793 - val_loss: 4.2997 - val_accuracy: 0.1193\n",
      "Epoch 29/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.9148 - accuracy: 0.3861 - val_loss: 3.6773 - val_accuracy: 0.1733\n",
      "Epoch 30/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.9204 - accuracy: 0.3791 - val_loss: 2.3351 - val_accuracy: 0.2025\n",
      "Epoch 31/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.9067 - accuracy: 0.3867 - val_loss: 2.0232 - val_accuracy: 0.2500\n",
      "Epoch 32/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8961 - accuracy: 0.3944 - val_loss: 6.3548 - val_accuracy: 0.1089\n",
      "Epoch 33/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8941 - accuracy: 0.3905 - val_loss: 2.3163 - val_accuracy: 0.2328\n",
      "Epoch 34/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8842 - accuracy: 0.3994 - val_loss: 2.7849 - val_accuracy: 0.1915\n",
      "Epoch 35/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8809 - accuracy: 0.3991 - val_loss: 7.4057 - val_accuracy: 0.1005\n",
      "Epoch 36/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8642 - accuracy: 0.4080 - val_loss: 16.7121 - val_accuracy: 0.1000\n",
      "Epoch 37/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8646 - accuracy: 0.4051 - val_loss: 5.7028 - val_accuracy: 0.1550\n",
      "Epoch 38/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8584 - accuracy: 0.4086 - val_loss: 7.4758 - val_accuracy: 0.1026\n",
      "Epoch 39/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8475 - accuracy: 0.4129 - val_loss: 2.2807 - val_accuracy: 0.2614\n",
      "Epoch 40/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8424 - accuracy: 0.4154 - val_loss: 2.4320 - val_accuracy: 0.2817\n",
      "Epoch 41/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8344 - accuracy: 0.4180 - val_loss: 3.6817 - val_accuracy: 0.1185\n",
      "Epoch 42/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8294 - accuracy: 0.4183 - val_loss: 3.5644 - val_accuracy: 0.2188\n",
      "Epoch 43/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8310 - accuracy: 0.4189 - val_loss: 3.7138 - val_accuracy: 0.1692\n",
      "Epoch 44/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8205 - accuracy: 0.4220 - val_loss: 6.6926 - val_accuracy: 0.1058\n",
      "Epoch 45/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8164 - accuracy: 0.4266 - val_loss: 2.1119 - val_accuracy: 0.2646\n",
      "Epoch 46/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.8127 - accuracy: 0.4289 - val_loss: 2.1027 - val_accuracy: 0.2484\n",
      "Epoch 47/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.8105 - accuracy: 0.4255 - val_loss: 2.0468 - val_accuracy: 0.2945\n",
      "Epoch 48/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.8021 - accuracy: 0.4267 - val_loss: 4.6297 - val_accuracy: 0.1276\n",
      "Epoch 49/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7939 - accuracy: 0.4305 - val_loss: 3.6527 - val_accuracy: 0.1645\n",
      "Epoch 50/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.7940 - accuracy: 0.4302 - val_loss: 3.6025 - val_accuracy: 0.1322\n",
      "Epoch 51/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.7817 - accuracy: 0.4394 - val_loss: 5.0333 - val_accuracy: 0.1031\n",
      "Epoch 52/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7781 - accuracy: 0.4379 - val_loss: 2.2557 - val_accuracy: 0.2654\n",
      "Epoch 53/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7716 - accuracy: 0.4394 - val_loss: 2.3726 - val_accuracy: 0.2312\n",
      "Epoch 54/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7645 - accuracy: 0.4451 - val_loss: 2.0705 - val_accuracy: 0.2868\n",
      "Epoch 55/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.7597 - accuracy: 0.4470 - val_loss: 2.6783 - val_accuracy: 0.2605\n",
      "Epoch 56/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.7474 - accuracy: 0.4519 - val_loss: 2.5106 - val_accuracy: 0.2361\n",
      "Epoch 57/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7430 - accuracy: 0.4552 - val_loss: 2.3508 - val_accuracy: 0.2088\n",
      "Epoch 58/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7375 - accuracy: 0.4573 - val_loss: 2.2454 - val_accuracy: 0.2344\n",
      "Epoch 59/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.7363 - accuracy: 0.4578 - val_loss: 3.3878 - val_accuracy: 0.1422\n",
      "Epoch 60/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7294 - accuracy: 0.4609 - val_loss: 2.2966 - val_accuracy: 0.2450\n",
      "Epoch 61/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7244 - accuracy: 0.4621 - val_loss: 1.6170 - val_accuracy: 0.4351\n",
      "Epoch 62/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.7131 - accuracy: 0.4662 - val_loss: 1.9504 - val_accuracy: 0.3086\n",
      "Epoch 63/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.7038 - accuracy: 0.4712 - val_loss: 2.1988 - val_accuracy: 0.2529\n",
      "Epoch 64/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.7026 - accuracy: 0.4689 - val_loss: 1.8966 - val_accuracy: 0.3552\n",
      "Epoch 65/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6900 - accuracy: 0.4780 - val_loss: 1.8158 - val_accuracy: 0.3616\n",
      "Epoch 66/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6902 - accuracy: 0.4754 - val_loss: 1.8070 - val_accuracy: 0.3732\n",
      "Epoch 67/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6815 - accuracy: 0.4784 - val_loss: 2.1815 - val_accuracy: 0.3286\n",
      "Epoch 68/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6797 - accuracy: 0.4791 - val_loss: 1.6701 - val_accuracy: 0.4191\n",
      "Epoch 69/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.6721 - accuracy: 0.4800 - val_loss: 2.1118 - val_accuracy: 0.2905\n",
      "Epoch 70/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6627 - accuracy: 0.4887 - val_loss: 1.6288 - val_accuracy: 0.4151\n",
      "Epoch 71/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6570 - accuracy: 0.4905 - val_loss: 2.3725 - val_accuracy: 0.2594\n",
      "Epoch 72/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6576 - accuracy: 0.4916 - val_loss: 1.6796 - val_accuracy: 0.4147\n",
      "Epoch 73/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6457 - accuracy: 0.4915 - val_loss: 3.0705 - val_accuracy: 0.2026\n",
      "Epoch 74/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6404 - accuracy: 0.4973 - val_loss: 1.8005 - val_accuracy: 0.3684\n",
      "Epoch 75/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6402 - accuracy: 0.4966 - val_loss: 2.0212 - val_accuracy: 0.3370\n",
      "Epoch 76/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6258 - accuracy: 0.5020 - val_loss: 2.1056 - val_accuracy: 0.2856\n",
      "Epoch 77/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6298 - accuracy: 0.5008 - val_loss: 1.7724 - val_accuracy: 0.3786\n",
      "Epoch 78/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6228 - accuracy: 0.5045 - val_loss: 1.7958 - val_accuracy: 0.3811\n",
      "Epoch 79/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6129 - accuracy: 0.5076 - val_loss: 2.2706 - val_accuracy: 0.2457\n",
      "Epoch 80/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.6097 - accuracy: 0.5103 - val_loss: 2.4346 - val_accuracy: 0.2760\n",
      "Epoch 81/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.6051 - accuracy: 0.5102 - val_loss: 1.6462 - val_accuracy: 0.4319\n",
      "Epoch 82/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5992 - accuracy: 0.5131 - val_loss: 1.6128 - val_accuracy: 0.4283\n",
      "Epoch 83/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5973 - accuracy: 0.5157 - val_loss: 1.4602 - val_accuracy: 0.4820\n",
      "Epoch 84/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5868 - accuracy: 0.5187 - val_loss: 1.7216 - val_accuracy: 0.4026\n",
      "Epoch 85/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5864 - accuracy: 0.5191 - val_loss: 1.8381 - val_accuracy: 0.3575\n",
      "Epoch 86/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5872 - accuracy: 0.5157 - val_loss: 1.5100 - val_accuracy: 0.4729\n",
      "Epoch 87/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5776 - accuracy: 0.5241 - val_loss: 1.7051 - val_accuracy: 0.4088\n",
      "Epoch 88/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5732 - accuracy: 0.5221 - val_loss: 1.9857 - val_accuracy: 0.3485\n",
      "Epoch 89/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5741 - accuracy: 0.5223 - val_loss: 1.4256 - val_accuracy: 0.5034\n",
      "Epoch 90/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5720 - accuracy: 0.5225 - val_loss: 1.6210 - val_accuracy: 0.4320\n",
      "Epoch 91/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.5644 - accuracy: 0.5273 - val_loss: 1.5417 - val_accuracy: 0.4506\n",
      "Epoch 92/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.5573 - accuracy: 0.5338 - val_loss: 1.5157 - val_accuracy: 0.4610\n",
      "Epoch 93/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5615 - accuracy: 0.5278 - val_loss: 1.5305 - val_accuracy: 0.4707\n",
      "Epoch 94/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5521 - accuracy: 0.5351 - val_loss: 1.5835 - val_accuracy: 0.4595\n",
      "Epoch 95/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5490 - accuracy: 0.5341 - val_loss: 1.5836 - val_accuracy: 0.4453\n",
      "Epoch 96/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5506 - accuracy: 0.5323 - val_loss: 1.4186 - val_accuracy: 0.5037\n",
      "Epoch 97/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5437 - accuracy: 0.5356 - val_loss: 1.4605 - val_accuracy: 0.4877\n",
      "Epoch 98/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5438 - accuracy: 0.5363 - val_loss: 1.6982 - val_accuracy: 0.4172\n",
      "Epoch 99/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.5442 - accuracy: 0.5353 - val_loss: 1.3150 - val_accuracy: 0.5449\n",
      "Epoch 100/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.5424 - accuracy: 0.5375 - val_loss: 1.7398 - val_accuracy: 0.4179\n",
      "Epoch 101/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5352 - accuracy: 0.5420 - val_loss: 1.3679 - val_accuracy: 0.5178\n",
      "Epoch 102/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5385 - accuracy: 0.5381 - val_loss: 1.3154 - val_accuracy: 0.5412\n",
      "Epoch 103/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5301 - accuracy: 0.5430 - val_loss: 1.3588 - val_accuracy: 0.5187\n",
      "Epoch 104/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5338 - accuracy: 0.5412 - val_loss: 1.3859 - val_accuracy: 0.5100\n",
      "Epoch 105/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.5350 - accuracy: 0.5419 - val_loss: 1.3506 - val_accuracy: 0.5226\n",
      "Epoch 106/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.5339 - accuracy: 0.5413 - val_loss: 1.3552 - val_accuracy: 0.5248\n",
      "Epoch 107/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.5378 - accuracy: 0.5405 - val_loss: 1.3612 - val_accuracy: 0.5222\n",
      "Epoch 108/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.5310 - accuracy: 0.5424 - val_loss: 1.3587 - val_accuracy: 0.5231\n",
      "10000/10000 [==============================] - 2s 249us/step\n",
      "Test loss: 1.3587213262557984\n",
      "Test accuracy: 0.5231000185012817\n",
      "x_train shape: (40000, 32, 32, 3)\n",
      "y_train shape: (40000, 10)\n",
      "40000 train samples\n",
      "10000 test samples\n",
      "10000 validation samples\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "ResNet20v1\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/108\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 2.2640 - accuracy: 0.2744 - val_loss: 2.9319 - val_accuracy: 0.2119\n",
      "Epoch 2/108\n",
      "  1/156 [..............................] - ETA: 9s - loss: 1.9809 - accuracy: 0.3906"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheverazo/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 9s 61ms/step - loss: 1.8291 - accuracy: 0.4329 - val_loss: 1.7724 - val_accuracy: 0.3967\n",
      "Epoch 3/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.6232 - accuracy: 0.5432 - val_loss: 1.6188 - val_accuracy: 0.4600\n",
      "Epoch 4/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.5106 - accuracy: 0.6040 - val_loss: 1.4682 - val_accuracy: 0.5413\n",
      "Epoch 5/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.4378 - accuracy: 0.6456 - val_loss: 1.5938 - val_accuracy: 0.4762\n",
      "Epoch 6/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.3734 - accuracy: 0.6760 - val_loss: 1.4364 - val_accuracy: 0.5549\n",
      "Epoch 7/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.3229 - accuracy: 0.7021 - val_loss: 1.2123 - val_accuracy: 0.6357\n",
      "Epoch 8/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.2942 - accuracy: 0.7168 - val_loss: 1.1495 - val_accuracy: 0.6697\n",
      "Epoch 9/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.2669 - accuracy: 0.7323 - val_loss: 1.2268 - val_accuracy: 0.6397\n",
      "Epoch 10/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.2502 - accuracy: 0.7433 - val_loss: 1.1374 - val_accuracy: 0.6709\n",
      "Epoch 11/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.2380 - accuracy: 0.7471 - val_loss: 1.2910 - val_accuracy: 0.6292\n",
      "Epoch 12/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.2228 - accuracy: 0.7545 - val_loss: 1.2978 - val_accuracy: 0.6277\n",
      "Epoch 13/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.2125 - accuracy: 0.7600 - val_loss: 0.9844 - val_accuracy: 0.7409\n",
      "Epoch 14/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.2040 - accuracy: 0.7651 - val_loss: 1.1686 - val_accuracy: 0.6630\n",
      "Epoch 15/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1916 - accuracy: 0.7695 - val_loss: 1.4935 - val_accuracy: 0.5688\n",
      "Epoch 16/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1839 - accuracy: 0.7730 - val_loss: 1.4429 - val_accuracy: 0.5725\n",
      "Epoch 17/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.1758 - accuracy: 0.7758 - val_loss: 1.0738 - val_accuracy: 0.7070\n",
      "Epoch 18/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.1698 - accuracy: 0.7785 - val_loss: 1.1207 - val_accuracy: 0.6841\n",
      "Epoch 19/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1554 - accuracy: 0.7863 - val_loss: 1.7205 - val_accuracy: 0.5446\n",
      "Epoch 20/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.1556 - accuracy: 0.7861 - val_loss: 1.2545 - val_accuracy: 0.6229\n",
      "Epoch 21/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.1539 - accuracy: 0.7855 - val_loss: 1.0966 - val_accuracy: 0.6852\n",
      "Epoch 22/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.1420 - accuracy: 0.7897 - val_loss: 0.9766 - val_accuracy: 0.7263\n",
      "Epoch 23/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1375 - accuracy: 0.7935 - val_loss: 1.1025 - val_accuracy: 0.7016\n",
      "Epoch 24/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1325 - accuracy: 0.7953 - val_loss: 1.2980 - val_accuracy: 0.6289\n",
      "Epoch 25/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1206 - accuracy: 0.8002 - val_loss: 1.1443 - val_accuracy: 0.6829\n",
      "Epoch 26/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1227 - accuracy: 0.8009 - val_loss: 0.9631 - val_accuracy: 0.7347\n",
      "Epoch 27/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1134 - accuracy: 0.8029 - val_loss: 0.9738 - val_accuracy: 0.7331\n",
      "Epoch 28/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1160 - accuracy: 0.8035 - val_loss: 0.9241 - val_accuracy: 0.7542\n",
      "Epoch 29/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.1097 - accuracy: 0.8046 - val_loss: 1.2422 - val_accuracy: 0.6387\n",
      "Epoch 30/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.0965 - accuracy: 0.8092 - val_loss: 1.3595 - val_accuracy: 0.6059\n",
      "Epoch 31/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.0957 - accuracy: 0.8129 - val_loss: 1.1953 - val_accuracy: 0.6657\n",
      "Epoch 32/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.0934 - accuracy: 0.8101 - val_loss: 1.4205 - val_accuracy: 0.5853\n",
      "Epoch 33/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.0873 - accuracy: 0.8149 - val_loss: 0.8976 - val_accuracy: 0.7641\n",
      "Epoch 34/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.0894 - accuracy: 0.8111 - val_loss: 1.2838 - val_accuracy: 0.6410\n",
      "Epoch 35/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0787 - accuracy: 0.8137 - val_loss: 1.0906 - val_accuracy: 0.6879\n",
      "Epoch 36/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0733 - accuracy: 0.8176 - val_loss: 0.9438 - val_accuracy: 0.7383\n",
      "Epoch 37/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0703 - accuracy: 0.8188 - val_loss: 1.2907 - val_accuracy: 0.6413\n",
      "Epoch 38/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0640 - accuracy: 0.8232 - val_loss: 1.1327 - val_accuracy: 0.6803\n",
      "Epoch 39/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.0582 - accuracy: 0.8235 - val_loss: 1.1401 - val_accuracy: 0.6902\n",
      "Epoch 40/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.0525 - accuracy: 0.8239 - val_loss: 0.9213 - val_accuracy: 0.7616\n",
      "Epoch 41/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0509 - accuracy: 0.8255 - val_loss: 0.9719 - val_accuracy: 0.7433\n",
      "Epoch 42/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.0457 - accuracy: 0.8277 - val_loss: 0.8902 - val_accuracy: 0.7640\n",
      "Epoch 43/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0349 - accuracy: 0.8313 - val_loss: 0.9572 - val_accuracy: 0.7342\n",
      "Epoch 44/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.0371 - accuracy: 0.8293 - val_loss: 0.9711 - val_accuracy: 0.7388\n",
      "Epoch 45/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0296 - accuracy: 0.8314 - val_loss: 1.2033 - val_accuracy: 0.6634\n",
      "Epoch 46/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0257 - accuracy: 0.8363 - val_loss: 1.0805 - val_accuracy: 0.6815\n",
      "Epoch 47/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0164 - accuracy: 0.8386 - val_loss: 1.1721 - val_accuracy: 0.6655\n",
      "Epoch 48/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0129 - accuracy: 0.8364 - val_loss: 0.8463 - val_accuracy: 0.7726\n",
      "Epoch 49/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0100 - accuracy: 0.8389 - val_loss: 1.3684 - val_accuracy: 0.5855\n",
      "Epoch 50/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9987 - accuracy: 0.8427 - val_loss: 0.8634 - val_accuracy: 0.7595\n",
      "Epoch 51/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0020 - accuracy: 0.8419 - val_loss: 1.1965 - val_accuracy: 0.6569\n",
      "Epoch 52/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9943 - accuracy: 0.8445 - val_loss: 1.0034 - val_accuracy: 0.7228\n",
      "Epoch 53/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9868 - accuracy: 0.8464 - val_loss: 0.7994 - val_accuracy: 0.7855\n",
      "Epoch 54/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9776 - accuracy: 0.8511 - val_loss: 0.9306 - val_accuracy: 0.7365\n",
      "Epoch 55/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9826 - accuracy: 0.8472 - val_loss: 0.9048 - val_accuracy: 0.7451\n",
      "Epoch 56/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.9731 - accuracy: 0.8510 - val_loss: 0.8630 - val_accuracy: 0.7618\n",
      "Epoch 57/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.9640 - accuracy: 0.8535 - val_loss: 0.7605 - val_accuracy: 0.7966\n",
      "Epoch 58/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9587 - accuracy: 0.8568 - val_loss: 0.8298 - val_accuracy: 0.7728\n",
      "Epoch 59/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9552 - accuracy: 0.8554 - val_loss: 0.8015 - val_accuracy: 0.7791\n",
      "Epoch 60/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9459 - accuracy: 0.8595 - val_loss: 0.7274 - val_accuracy: 0.8038\n",
      "Epoch 61/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.9430 - accuracy: 0.8606 - val_loss: 0.9311 - val_accuracy: 0.7417\n",
      "Epoch 62/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.9377 - accuracy: 0.8625 - val_loss: 0.6918 - val_accuracy: 0.8139\n",
      "Epoch 63/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.9312 - accuracy: 0.8658 - val_loss: 0.7388 - val_accuracy: 0.8056\n",
      "Epoch 64/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9270 - accuracy: 0.8650 - val_loss: 0.8113 - val_accuracy: 0.7779\n",
      "Epoch 65/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9186 - accuracy: 0.8683 - val_loss: 0.8090 - val_accuracy: 0.7747\n",
      "Epoch 66/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9107 - accuracy: 0.8733 - val_loss: 1.1137 - val_accuracy: 0.6829\n",
      "Epoch 67/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9088 - accuracy: 0.8729 - val_loss: 0.7033 - val_accuracy: 0.8143\n",
      "Epoch 68/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9032 - accuracy: 0.8731 - val_loss: 0.7760 - val_accuracy: 0.7887\n",
      "Epoch 69/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8949 - accuracy: 0.8781 - val_loss: 0.6693 - val_accuracy: 0.8212\n",
      "Epoch 70/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.8863 - accuracy: 0.8789 - val_loss: 0.8077 - val_accuracy: 0.7804\n",
      "Epoch 71/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8871 - accuracy: 0.8798 - val_loss: 0.7369 - val_accuracy: 0.7992\n",
      "Epoch 72/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8721 - accuracy: 0.8851 - val_loss: 0.6752 - val_accuracy: 0.8173\n",
      "Epoch 73/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.8700 - accuracy: 0.8853 - val_loss: 0.6468 - val_accuracy: 0.8307\n",
      "Epoch 74/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.8661 - accuracy: 0.8871 - val_loss: 0.5822 - val_accuracy: 0.8484\n",
      "Epoch 75/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.8593 - accuracy: 0.8882 - val_loss: 0.6315 - val_accuracy: 0.8362\n",
      "Epoch 76/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.8529 - accuracy: 0.8924 - val_loss: 0.6253 - val_accuracy: 0.8360\n",
      "Epoch 77/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8511 - accuracy: 0.8923 - val_loss: 0.6352 - val_accuracy: 0.8330\n",
      "Epoch 78/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8455 - accuracy: 0.8931 - val_loss: 0.5651 - val_accuracy: 0.8508\n",
      "Epoch 79/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8348 - accuracy: 0.8969 - val_loss: 0.6183 - val_accuracy: 0.8345\n",
      "Epoch 80/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8333 - accuracy: 0.8980 - val_loss: 0.6763 - val_accuracy: 0.8139\n",
      "Epoch 81/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.8219 - accuracy: 0.9021 - val_loss: 0.5555 - val_accuracy: 0.8594\n",
      "Epoch 82/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8181 - accuracy: 0.9040 - val_loss: 0.6601 - val_accuracy: 0.8185\n",
      "Epoch 83/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.8155 - accuracy: 0.9046 - val_loss: 0.6525 - val_accuracy: 0.8283\n",
      "Epoch 84/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8092 - accuracy: 0.9056 - val_loss: 0.5893 - val_accuracy: 0.8450\n",
      "Epoch 85/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8024 - accuracy: 0.9095 - val_loss: 0.5820 - val_accuracy: 0.8456\n",
      "Epoch 86/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.7994 - accuracy: 0.9097 - val_loss: 0.5678 - val_accuracy: 0.8515\n",
      "Epoch 87/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7981 - accuracy: 0.9095 - val_loss: 0.5580 - val_accuracy: 0.8521\n",
      "Epoch 88/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.7879 - accuracy: 0.9145 - val_loss: 0.5484 - val_accuracy: 0.8575\n",
      "Epoch 89/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.7826 - accuracy: 0.9160 - val_loss: 0.5986 - val_accuracy: 0.8438\n",
      "Epoch 90/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7824 - accuracy: 0.9151 - val_loss: 0.5475 - val_accuracy: 0.8594\n",
      "Epoch 91/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7758 - accuracy: 0.9206 - val_loss: 0.5372 - val_accuracy: 0.8598\n",
      "Epoch 92/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7741 - accuracy: 0.9197 - val_loss: 0.5279 - val_accuracy: 0.8647\n",
      "Epoch 93/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.7671 - accuracy: 0.9221 - val_loss: 0.5304 - val_accuracy: 0.8637\n",
      "Epoch 94/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7656 - accuracy: 0.9230 - val_loss: 0.5144 - val_accuracy: 0.8694\n",
      "Epoch 95/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7587 - accuracy: 0.9245 - val_loss: 0.5019 - val_accuracy: 0.8735\n",
      "Epoch 96/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7612 - accuracy: 0.9246 - val_loss: 0.4989 - val_accuracy: 0.8743\n",
      "Epoch 97/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7581 - accuracy: 0.9259 - val_loss: 0.5105 - val_accuracy: 0.8725\n",
      "Epoch 98/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7517 - accuracy: 0.9283 - val_loss: 0.5167 - val_accuracy: 0.8703\n",
      "Epoch 99/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7518 - accuracy: 0.9285 - val_loss: 0.5014 - val_accuracy: 0.8749\n",
      "Epoch 100/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.7501 - accuracy: 0.9290 - val_loss: 0.5018 - val_accuracy: 0.8742\n",
      "Epoch 101/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7477 - accuracy: 0.9297 - val_loss: 0.5066 - val_accuracy: 0.8730\n",
      "Epoch 102/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7460 - accuracy: 0.9305 - val_loss: 0.4999 - val_accuracy: 0.8781\n",
      "Epoch 103/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.7418 - accuracy: 0.9333 - val_loss: 0.4956 - val_accuracy: 0.8783\n",
      "Epoch 104/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7435 - accuracy: 0.9318 - val_loss: 0.4986 - val_accuracy: 0.8771\n",
      "Epoch 105/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7431 - accuracy: 0.9312 - val_loss: 0.4972 - val_accuracy: 0.8778\n",
      "Epoch 106/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7448 - accuracy: 0.9305 - val_loss: 0.4982 - val_accuracy: 0.8777\n",
      "Epoch 107/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7434 - accuracy: 0.9319 - val_loss: 0.4989 - val_accuracy: 0.8773\n",
      "Epoch 108/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7415 - accuracy: 0.9337 - val_loss: 0.4992 - val_accuracy: 0.8782\n",
      "10000/10000 [==============================] - 2s 183us/step\n",
      "Test loss: 0.4991846408367157\n",
      "Test accuracy: 0.8781999945640564\n",
      "x_train shape: (40000, 32, 32, 3)\n",
      "y_train shape: (40000, 10)\n",
      "40000 train samples\n",
      "10000 test samples\n",
      "10000 validation samples\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "ResNet20v1\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/108\n",
      "156/156 [==============================] - 14s 87ms/step - loss: 2.2343 - accuracy: 0.2772 - val_loss: 2.8502 - val_accuracy: 0.2724\n",
      "Epoch 2/108\n",
      "  1/156 [..............................] - ETA: 10s - loss: 1.9449 - accuracy: 0.3633"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheverazo/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 10s 62ms/step - loss: 1.8324 - accuracy: 0.4281 - val_loss: 1.9494 - val_accuracy: 0.3137\n",
      "Epoch 3/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.6555 - accuracy: 0.5241 - val_loss: 1.5803 - val_accuracy: 0.4875\n",
      "Epoch 4/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.5354 - accuracy: 0.5905 - val_loss: 1.8417 - val_accuracy: 0.3920\n",
      "Epoch 5/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.4508 - accuracy: 0.6341 - val_loss: 1.3663 - val_accuracy: 0.5728\n",
      "Epoch 6/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.3903 - accuracy: 0.6672 - val_loss: 1.5891 - val_accuracy: 0.4967\n",
      "Epoch 7/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.3412 - accuracy: 0.6945 - val_loss: 1.5259 - val_accuracy: 0.5521\n",
      "Epoch 8/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.3091 - accuracy: 0.7086 - val_loss: 1.6051 - val_accuracy: 0.5362\n",
      "Epoch 9/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.2794 - accuracy: 0.7235 - val_loss: 1.5677 - val_accuracy: 0.5277\n",
      "Epoch 10/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.2616 - accuracy: 0.7349 - val_loss: 1.5035 - val_accuracy: 0.5564\n",
      "Epoch 11/108\n",
      "156/156 [==============================] - 10s 63ms/step - loss: 1.2461 - accuracy: 0.7419 - val_loss: 1.0719 - val_accuracy: 0.6970\n",
      "Epoch 12/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.2285 - accuracy: 0.7476 - val_loss: 1.3516 - val_accuracy: 0.6055\n",
      "Epoch 13/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.2148 - accuracy: 0.7556 - val_loss: 1.1532 - val_accuracy: 0.6783\n",
      "Epoch 14/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.2057 - accuracy: 0.7596 - val_loss: 1.4236 - val_accuracy: 0.5937\n",
      "Epoch 15/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.2013 - accuracy: 0.7645 - val_loss: 1.0792 - val_accuracy: 0.6932\n",
      "Epoch 16/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1921 - accuracy: 0.7649 - val_loss: 1.1605 - val_accuracy: 0.6697\n",
      "Epoch 17/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1763 - accuracy: 0.7762 - val_loss: 1.1468 - val_accuracy: 0.6524\n",
      "Epoch 18/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.1692 - accuracy: 0.7775 - val_loss: 1.3860 - val_accuracy: 0.5786\n",
      "Epoch 19/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1665 - accuracy: 0.7788 - val_loss: 1.2228 - val_accuracy: 0.6472\n",
      "Epoch 20/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.1610 - accuracy: 0.7812 - val_loss: 1.1630 - val_accuracy: 0.6812\n",
      "Epoch 21/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.1518 - accuracy: 0.7850 - val_loss: 0.9739 - val_accuracy: 0.7248\n",
      "Epoch 22/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.1506 - accuracy: 0.7866 - val_loss: 1.0935 - val_accuracy: 0.6840\n",
      "Epoch 23/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1447 - accuracy: 0.7881 - val_loss: 1.1130 - val_accuracy: 0.6893\n",
      "Epoch 24/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1315 - accuracy: 0.7926 - val_loss: 1.0212 - val_accuracy: 0.7108\n",
      "Epoch 25/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1316 - accuracy: 0.7922 - val_loss: 1.2303 - val_accuracy: 0.6579\n",
      "Epoch 26/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.1253 - accuracy: 0.7976 - val_loss: 1.5764 - val_accuracy: 0.5684\n",
      "Epoch 27/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1244 - accuracy: 0.7984 - val_loss: 1.0860 - val_accuracy: 0.6900\n",
      "Epoch 28/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.1123 - accuracy: 0.8005 - val_loss: 1.0207 - val_accuracy: 0.7113\n",
      "Epoch 29/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1124 - accuracy: 0.8015 - val_loss: 0.9396 - val_accuracy: 0.7421\n",
      "Epoch 30/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.1106 - accuracy: 0.8004 - val_loss: 1.4752 - val_accuracy: 0.5563\n",
      "Epoch 31/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.1014 - accuracy: 0.8068 - val_loss: 1.4362 - val_accuracy: 0.6221\n",
      "Epoch 32/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0968 - accuracy: 0.8075 - val_loss: 0.9758 - val_accuracy: 0.7222\n",
      "Epoch 33/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.0947 - accuracy: 0.8079 - val_loss: 0.9681 - val_accuracy: 0.7350\n",
      "Epoch 34/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.0912 - accuracy: 0.8074 - val_loss: 1.0661 - val_accuracy: 0.7034\n",
      "Epoch 35/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0785 - accuracy: 0.8147 - val_loss: 0.9189 - val_accuracy: 0.7480\n",
      "Epoch 36/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0836 - accuracy: 0.8121 - val_loss: 1.7022 - val_accuracy: 0.4743\n",
      "Epoch 37/108\n",
      "156/156 [==============================] - 10s 63ms/step - loss: 1.0696 - accuracy: 0.8180 - val_loss: 1.2459 - val_accuracy: 0.6619\n",
      "Epoch 38/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.0658 - accuracy: 0.8210 - val_loss: 1.3681 - val_accuracy: 0.6063\n",
      "Epoch 39/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0665 - accuracy: 0.8157 - val_loss: 0.8501 - val_accuracy: 0.7741\n",
      "Epoch 40/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0615 - accuracy: 0.8202 - val_loss: 1.1667 - val_accuracy: 0.6560\n",
      "Epoch 41/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0530 - accuracy: 0.8224 - val_loss: 1.0850 - val_accuracy: 0.6838\n",
      "Epoch 42/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0501 - accuracy: 0.8230 - val_loss: 1.2570 - val_accuracy: 0.6263\n",
      "Epoch 43/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0437 - accuracy: 0.8270 - val_loss: 0.9412 - val_accuracy: 0.7300\n",
      "Epoch 44/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0382 - accuracy: 0.8261 - val_loss: 0.9867 - val_accuracy: 0.7229\n",
      "Epoch 45/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.0349 - accuracy: 0.8303 - val_loss: 0.9337 - val_accuracy: 0.7404\n",
      "Epoch 46/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.0266 - accuracy: 0.8330 - val_loss: 0.8233 - val_accuracy: 0.7752\n",
      "Epoch 47/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 1.0255 - accuracy: 0.8321 - val_loss: 1.0743 - val_accuracy: 0.7074\n",
      "Epoch 48/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0185 - accuracy: 0.8342 - val_loss: 0.9719 - val_accuracy: 0.7316\n",
      "Epoch 49/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0214 - accuracy: 0.8328 - val_loss: 1.1062 - val_accuracy: 0.6833\n",
      "Epoch 50/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0063 - accuracy: 0.8388 - val_loss: 0.9028 - val_accuracy: 0.7412\n",
      "Epoch 51/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0036 - accuracy: 0.8400 - val_loss: 1.1371 - val_accuracy: 0.6713\n",
      "Epoch 52/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9973 - accuracy: 0.8415 - val_loss: 1.0559 - val_accuracy: 0.7018\n",
      "Epoch 53/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9937 - accuracy: 0.8423 - val_loss: 1.3444 - val_accuracy: 0.6199\n",
      "Epoch 54/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.9871 - accuracy: 0.8445 - val_loss: 0.9512 - val_accuracy: 0.7331\n",
      "Epoch 55/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9836 - accuracy: 0.8446 - val_loss: 0.8938 - val_accuracy: 0.7492\n",
      "Epoch 56/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9744 - accuracy: 0.8499 - val_loss: 0.8122 - val_accuracy: 0.7820\n",
      "Epoch 57/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.9754 - accuracy: 0.8479 - val_loss: 1.0202 - val_accuracy: 0.7050\n",
      "Epoch 58/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9660 - accuracy: 0.8512 - val_loss: 0.8355 - val_accuracy: 0.7704\n",
      "Epoch 59/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9641 - accuracy: 0.8497 - val_loss: 0.7230 - val_accuracy: 0.8123\n",
      "Epoch 60/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9535 - accuracy: 0.8568 - val_loss: 0.8202 - val_accuracy: 0.7677\n",
      "Epoch 61/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9502 - accuracy: 0.8567 - val_loss: 0.7402 - val_accuracy: 0.7999\n",
      "Epoch 62/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9454 - accuracy: 0.8583 - val_loss: 0.7273 - val_accuracy: 0.8080\n",
      "Epoch 63/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9350 - accuracy: 0.8617 - val_loss: 0.8019 - val_accuracy: 0.7781\n",
      "Epoch 64/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9291 - accuracy: 0.8628 - val_loss: 0.7006 - val_accuracy: 0.8069\n",
      "Epoch 65/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9277 - accuracy: 0.8643 - val_loss: 0.9080 - val_accuracy: 0.7346\n",
      "Epoch 66/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.9175 - accuracy: 0.8679 - val_loss: 0.8585 - val_accuracy: 0.7603\n",
      "Epoch 67/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9146 - accuracy: 0.8691 - val_loss: 0.8397 - val_accuracy: 0.7617\n",
      "Epoch 68/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.9093 - accuracy: 0.8688 - val_loss: 0.7138 - val_accuracy: 0.8040\n",
      "Epoch 69/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9023 - accuracy: 0.8727 - val_loss: 1.0225 - val_accuracy: 0.7019\n",
      "Epoch 70/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8971 - accuracy: 0.8762 - val_loss: 0.8816 - val_accuracy: 0.7571\n",
      "Epoch 71/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.8963 - accuracy: 0.8725 - val_loss: 0.6680 - val_accuracy: 0.8225\n",
      "Epoch 72/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8864 - accuracy: 0.8764 - val_loss: 0.6528 - val_accuracy: 0.8241\n",
      "Epoch 73/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8849 - accuracy: 0.8790 - val_loss: 0.7036 - val_accuracy: 0.8120\n",
      "Epoch 74/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8738 - accuracy: 0.8817 - val_loss: 0.8850 - val_accuracy: 0.7445\n",
      "Epoch 75/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8696 - accuracy: 0.8823 - val_loss: 0.6161 - val_accuracy: 0.8406\n",
      "Epoch 76/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.8592 - accuracy: 0.8863 - val_loss: 0.6763 - val_accuracy: 0.8186\n",
      "Epoch 77/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8553 - accuracy: 0.8904 - val_loss: 0.6526 - val_accuracy: 0.8248\n",
      "Epoch 78/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8530 - accuracy: 0.8890 - val_loss: 0.7264 - val_accuracy: 0.7995\n",
      "Epoch 79/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8466 - accuracy: 0.8912 - val_loss: 0.5977 - val_accuracy: 0.8409\n",
      "Epoch 80/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8393 - accuracy: 0.8943 - val_loss: 0.7802 - val_accuracy: 0.7895\n",
      "Epoch 81/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8331 - accuracy: 0.8962 - val_loss: 0.6875 - val_accuracy: 0.8134\n",
      "Epoch 82/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8315 - accuracy: 0.8981 - val_loss: 0.6514 - val_accuracy: 0.8189\n",
      "Epoch 83/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.8260 - accuracy: 0.8988 - val_loss: 0.6012 - val_accuracy: 0.8457\n",
      "Epoch 84/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8171 - accuracy: 0.9021 - val_loss: 0.5777 - val_accuracy: 0.8505\n",
      "Epoch 85/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8134 - accuracy: 0.9031 - val_loss: 0.5579 - val_accuracy: 0.8555\n",
      "Epoch 86/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.8083 - accuracy: 0.9049 - val_loss: 0.5777 - val_accuracy: 0.8459\n",
      "Epoch 87/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8029 - accuracy: 0.9057 - val_loss: 0.5781 - val_accuracy: 0.8449\n",
      "Epoch 88/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8013 - accuracy: 0.9065 - val_loss: 0.5575 - val_accuracy: 0.8513\n",
      "Epoch 89/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7937 - accuracy: 0.9111 - val_loss: 0.5381 - val_accuracy: 0.8635\n",
      "Epoch 90/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7911 - accuracy: 0.9105 - val_loss: 0.5656 - val_accuracy: 0.8533\n",
      "Epoch 91/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7862 - accuracy: 0.9120 - val_loss: 0.5411 - val_accuracy: 0.8612\n",
      "Epoch 92/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7845 - accuracy: 0.9140 - val_loss: 0.5337 - val_accuracy: 0.8602\n",
      "Epoch 93/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7798 - accuracy: 0.9160 - val_loss: 0.5238 - val_accuracy: 0.8635\n",
      "Epoch 94/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7742 - accuracy: 0.9175 - val_loss: 0.5379 - val_accuracy: 0.8610\n",
      "Epoch 95/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7740 - accuracy: 0.9181 - val_loss: 0.5312 - val_accuracy: 0.8608\n",
      "Epoch 96/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7693 - accuracy: 0.9193 - val_loss: 0.5524 - val_accuracy: 0.8595\n",
      "Epoch 97/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7677 - accuracy: 0.9202 - val_loss: 0.5233 - val_accuracy: 0.8657\n",
      "Epoch 98/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7664 - accuracy: 0.9198 - val_loss: 0.5223 - val_accuracy: 0.8672\n",
      "Epoch 99/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.7619 - accuracy: 0.9232 - val_loss: 0.5121 - val_accuracy: 0.8674\n",
      "Epoch 100/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7607 - accuracy: 0.9233 - val_loss: 0.5140 - val_accuracy: 0.8700\n",
      "Epoch 101/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7594 - accuracy: 0.9236 - val_loss: 0.5215 - val_accuracy: 0.8689\n",
      "Epoch 102/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7609 - accuracy: 0.9231 - val_loss: 0.5071 - val_accuracy: 0.8704\n",
      "Epoch 103/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7584 - accuracy: 0.9256 - val_loss: 0.5109 - val_accuracy: 0.8694\n",
      "Epoch 104/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7566 - accuracy: 0.9241 - val_loss: 0.5096 - val_accuracy: 0.8704\n",
      "Epoch 105/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7575 - accuracy: 0.9239 - val_loss: 0.5097 - val_accuracy: 0.8708\n",
      "Epoch 106/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.7558 - accuracy: 0.9256 - val_loss: 0.5106 - val_accuracy: 0.8711\n",
      "Epoch 107/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7533 - accuracy: 0.9273 - val_loss: 0.5106 - val_accuracy: 0.8698\n",
      "Epoch 108/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7575 - accuracy: 0.9248 - val_loss: 0.5109 - val_accuracy: 0.8700\n",
      "10000/10000 [==============================] - 2s 177us/step\n",
      "Test loss: 0.5108863615512848\n",
      "Test accuracy: 0.8700000047683716\n",
      "x_train shape: (40000, 32, 32, 3)\n",
      "y_train shape: (40000, 10)\n",
      "40000 train samples\n",
      "10000 test samples\n",
      "10000 validation samples\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "ResNet20v1\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/108\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 2.2888 - accuracy: 0.2779 - val_loss: 1.8842 - val_accuracy: 0.3184\n",
      "Epoch 2/108\n",
      "  1/156 [..............................] - ETA: 13s - loss: 2.0013 - accuracy: 0.3516"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheverazo/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 9s 60ms/step - loss: 1.8269 - accuracy: 0.4312 - val_loss: 1.8764 - val_accuracy: 0.3842\n",
      "Epoch 3/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.6302 - accuracy: 0.5371 - val_loss: 1.5928 - val_accuracy: 0.5021\n",
      "Epoch 4/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.5083 - accuracy: 0.6061 - val_loss: 1.5090 - val_accuracy: 0.5069\n",
      "Epoch 5/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.4160 - accuracy: 0.6522 - val_loss: 1.8191 - val_accuracy: 0.4607\n",
      "Epoch 6/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.3571 - accuracy: 0.6849 - val_loss: 1.4826 - val_accuracy: 0.5342\n",
      "Epoch 7/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.3138 - accuracy: 0.7097 - val_loss: 1.1045 - val_accuracy: 0.6996\n",
      "Epoch 8/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.2896 - accuracy: 0.7230 - val_loss: 1.2064 - val_accuracy: 0.6546\n",
      "Epoch 9/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.2662 - accuracy: 0.7334 - val_loss: 2.2242 - val_accuracy: 0.3981\n",
      "Epoch 10/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.2457 - accuracy: 0.7444 - val_loss: 1.1320 - val_accuracy: 0.6731\n",
      "Epoch 11/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.2361 - accuracy: 0.7478 - val_loss: 1.1364 - val_accuracy: 0.6720\n",
      "Epoch 12/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.2176 - accuracy: 0.7599 - val_loss: 1.3840 - val_accuracy: 0.5892\n",
      "Epoch 13/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.2094 - accuracy: 0.7615 - val_loss: 1.1636 - val_accuracy: 0.6691\n",
      "Epoch 14/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1979 - accuracy: 0.7689 - val_loss: 1.1889 - val_accuracy: 0.6587\n",
      "Epoch 15/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.1930 - accuracy: 0.7699 - val_loss: 1.3708 - val_accuracy: 0.6045\n",
      "Epoch 16/108\n",
      "156/156 [==============================] - 10s 63ms/step - loss: 1.1838 - accuracy: 0.7730 - val_loss: 1.1657 - val_accuracy: 0.6675\n",
      "Epoch 17/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.1753 - accuracy: 0.7774 - val_loss: 0.9771 - val_accuracy: 0.7272\n",
      "Epoch 18/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.1693 - accuracy: 0.7815 - val_loss: 1.1422 - val_accuracy: 0.6684\n",
      "Epoch 19/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1643 - accuracy: 0.7830 - val_loss: 2.0362 - val_accuracy: 0.4029\n",
      "Epoch 20/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1536 - accuracy: 0.7894 - val_loss: 1.5237 - val_accuracy: 0.5702\n",
      "Epoch 21/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1546 - accuracy: 0.7874 - val_loss: 0.9285 - val_accuracy: 0.7557\n",
      "Epoch 22/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1462 - accuracy: 0.7914 - val_loss: 0.9433 - val_accuracy: 0.7417\n",
      "Epoch 23/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1397 - accuracy: 0.7941 - val_loss: 1.3986 - val_accuracy: 0.6115\n",
      "Epoch 24/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.1370 - accuracy: 0.7937 - val_loss: 1.3254 - val_accuracy: 0.6177\n",
      "Epoch 25/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1301 - accuracy: 0.7988 - val_loss: 1.3826 - val_accuracy: 0.5959\n",
      "Epoch 26/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1253 - accuracy: 0.7990 - val_loss: 1.3647 - val_accuracy: 0.6155\n",
      "Epoch 27/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.1231 - accuracy: 0.7998 - val_loss: 1.2464 - val_accuracy: 0.6439\n",
      "Epoch 28/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.1170 - accuracy: 0.8025 - val_loss: 1.0061 - val_accuracy: 0.7230\n",
      "Epoch 29/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1125 - accuracy: 0.8056 - val_loss: 1.3147 - val_accuracy: 0.6412\n",
      "Epoch 30/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.1068 - accuracy: 0.8052 - val_loss: 0.9502 - val_accuracy: 0.7323\n",
      "Epoch 31/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1001 - accuracy: 0.8082 - val_loss: 1.0815 - val_accuracy: 0.6921\n",
      "Epoch 32/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0972 - accuracy: 0.8102 - val_loss: 1.0664 - val_accuracy: 0.6999\n",
      "Epoch 33/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0917 - accuracy: 0.8132 - val_loss: 1.1722 - val_accuracy: 0.6602\n",
      "Epoch 34/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0836 - accuracy: 0.8152 - val_loss: 1.1029 - val_accuracy: 0.6902\n",
      "Epoch 35/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0813 - accuracy: 0.8159 - val_loss: 0.9101 - val_accuracy: 0.7588\n",
      "Epoch 36/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.0831 - accuracy: 0.8121 - val_loss: 1.0345 - val_accuracy: 0.7179\n",
      "Epoch 37/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0707 - accuracy: 0.8194 - val_loss: 1.2490 - val_accuracy: 0.6502\n",
      "Epoch 38/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0707 - accuracy: 0.8185 - val_loss: 1.0913 - val_accuracy: 0.6908\n",
      "Epoch 39/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0633 - accuracy: 0.8205 - val_loss: 1.1013 - val_accuracy: 0.6992\n",
      "Epoch 40/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0604 - accuracy: 0.8216 - val_loss: 1.1019 - val_accuracy: 0.6751\n",
      "Epoch 41/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0531 - accuracy: 0.8233 - val_loss: 0.9018 - val_accuracy: 0.7531\n",
      "Epoch 42/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.0522 - accuracy: 0.8247 - val_loss: 1.1144 - val_accuracy: 0.6772\n",
      "Epoch 43/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0405 - accuracy: 0.8307 - val_loss: 1.1186 - val_accuracy: 0.6810\n",
      "Epoch 44/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.0412 - accuracy: 0.8290 - val_loss: 1.0091 - val_accuracy: 0.7241\n",
      "Epoch 45/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0313 - accuracy: 0.8320 - val_loss: 0.9626 - val_accuracy: 0.7263\n",
      "Epoch 46/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0313 - accuracy: 0.8322 - val_loss: 1.0219 - val_accuracy: 0.7249\n",
      "Epoch 47/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0187 - accuracy: 0.8365 - val_loss: 0.7829 - val_accuracy: 0.7955\n",
      "Epoch 48/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0205 - accuracy: 0.8342 - val_loss: 1.0842 - val_accuracy: 0.6735\n",
      "Epoch 49/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0115 - accuracy: 0.8375 - val_loss: 1.3006 - val_accuracy: 0.6181\n",
      "Epoch 50/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0088 - accuracy: 0.8385 - val_loss: 0.8605 - val_accuracy: 0.7605\n",
      "Epoch 51/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.0017 - accuracy: 0.8405 - val_loss: 0.9270 - val_accuracy: 0.7344\n",
      "Epoch 52/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9941 - accuracy: 0.8445 - val_loss: 0.8067 - val_accuracy: 0.7860\n",
      "Epoch 53/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9920 - accuracy: 0.8444 - val_loss: 0.9293 - val_accuracy: 0.7447\n",
      "Epoch 54/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.9858 - accuracy: 0.8459 - val_loss: 0.8873 - val_accuracy: 0.7543\n",
      "Epoch 55/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.9791 - accuracy: 0.8485 - val_loss: 0.9211 - val_accuracy: 0.7382\n",
      "Epoch 56/108\n",
      "156/156 [==============================] - 10s 63ms/step - loss: 0.9791 - accuracy: 0.8485 - val_loss: 0.8130 - val_accuracy: 0.7730\n",
      "Epoch 57/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.9677 - accuracy: 0.8531 - val_loss: 0.8676 - val_accuracy: 0.7591\n",
      "Epoch 58/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9591 - accuracy: 0.8535 - val_loss: 0.8162 - val_accuracy: 0.7677\n",
      "Epoch 59/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.9596 - accuracy: 0.8557 - val_loss: 1.1563 - val_accuracy: 0.6535\n",
      "Epoch 60/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9496 - accuracy: 0.8584 - val_loss: 0.8025 - val_accuracy: 0.7775\n",
      "Epoch 61/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9473 - accuracy: 0.8587 - val_loss: 0.7409 - val_accuracy: 0.8088\n",
      "Epoch 62/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9421 - accuracy: 0.8596 - val_loss: 0.8242 - val_accuracy: 0.7740\n",
      "Epoch 63/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9335 - accuracy: 0.8638 - val_loss: 0.6970 - val_accuracy: 0.8142\n",
      "Epoch 64/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9270 - accuracy: 0.8646 - val_loss: 0.7923 - val_accuracy: 0.7957\n",
      "Epoch 65/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9246 - accuracy: 0.8657 - val_loss: 0.8859 - val_accuracy: 0.7617\n",
      "Epoch 66/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9174 - accuracy: 0.8678 - val_loss: 0.6786 - val_accuracy: 0.8209\n",
      "Epoch 67/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9062 - accuracy: 0.8725 - val_loss: 0.7993 - val_accuracy: 0.7759\n",
      "Epoch 68/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9042 - accuracy: 0.8740 - val_loss: 0.7437 - val_accuracy: 0.7951\n",
      "Epoch 69/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8960 - accuracy: 0.8755 - val_loss: 0.7236 - val_accuracy: 0.7995\n",
      "Epoch 70/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.8931 - accuracy: 0.8751 - val_loss: 0.7924 - val_accuracy: 0.7891\n",
      "Epoch 71/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8843 - accuracy: 0.8800 - val_loss: 0.7201 - val_accuracy: 0.8063\n",
      "Epoch 72/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8817 - accuracy: 0.8798 - val_loss: 0.6554 - val_accuracy: 0.8286\n",
      "Epoch 73/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8784 - accuracy: 0.8811 - val_loss: 0.7884 - val_accuracy: 0.7859\n",
      "Epoch 74/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.8699 - accuracy: 0.8844 - val_loss: 0.6429 - val_accuracy: 0.8306\n",
      "Epoch 75/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8606 - accuracy: 0.8886 - val_loss: 0.6445 - val_accuracy: 0.8302\n",
      "Epoch 76/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8588 - accuracy: 0.8904 - val_loss: 0.6694 - val_accuracy: 0.8208\n",
      "Epoch 77/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8501 - accuracy: 0.8895 - val_loss: 0.6002 - val_accuracy: 0.8447\n",
      "Epoch 78/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8457 - accuracy: 0.8930 - val_loss: 0.6165 - val_accuracy: 0.8426\n",
      "Epoch 79/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8435 - accuracy: 0.8942 - val_loss: 0.5952 - val_accuracy: 0.8448\n",
      "Epoch 80/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8331 - accuracy: 0.8977 - val_loss: 0.6027 - val_accuracy: 0.8430\n",
      "Epoch 81/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8286 - accuracy: 0.8997 - val_loss: 0.7133 - val_accuracy: 0.8033\n",
      "Epoch 82/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8232 - accuracy: 0.9006 - val_loss: 0.5754 - val_accuracy: 0.8487\n",
      "Epoch 83/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8184 - accuracy: 0.9021 - val_loss: 0.5751 - val_accuracy: 0.8494\n",
      "Epoch 84/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8137 - accuracy: 0.9050 - val_loss: 0.6378 - val_accuracy: 0.8311\n",
      "Epoch 85/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8050 - accuracy: 0.9091 - val_loss: 0.6041 - val_accuracy: 0.8353\n",
      "Epoch 86/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.8034 - accuracy: 0.9078 - val_loss: 0.5752 - val_accuracy: 0.8481\n",
      "Epoch 87/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8002 - accuracy: 0.9095 - val_loss: 0.5234 - val_accuracy: 0.8653\n",
      "Epoch 88/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7897 - accuracy: 0.9127 - val_loss: 0.5203 - val_accuracy: 0.8646\n",
      "Epoch 89/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7896 - accuracy: 0.9126 - val_loss: 0.5587 - val_accuracy: 0.8571\n",
      "Epoch 90/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7807 - accuracy: 0.9178 - val_loss: 0.5782 - val_accuracy: 0.8495\n",
      "Epoch 91/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.7818 - accuracy: 0.9174 - val_loss: 0.5281 - val_accuracy: 0.8626\n",
      "Epoch 92/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.7806 - accuracy: 0.9152 - val_loss: 0.5276 - val_accuracy: 0.8612\n",
      "Epoch 93/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7728 - accuracy: 0.9205 - val_loss: 0.5157 - val_accuracy: 0.8692\n",
      "Epoch 94/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7675 - accuracy: 0.9223 - val_loss: 0.5171 - val_accuracy: 0.8675\n",
      "Epoch 95/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7661 - accuracy: 0.9220 - val_loss: 0.5046 - val_accuracy: 0.8715\n",
      "Epoch 96/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7652 - accuracy: 0.9231 - val_loss: 0.4993 - val_accuracy: 0.8733\n",
      "Epoch 97/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.7622 - accuracy: 0.9240 - val_loss: 0.5117 - val_accuracy: 0.8721\n",
      "Epoch 98/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7598 - accuracy: 0.9253 - val_loss: 0.5010 - val_accuracy: 0.8735\n",
      "Epoch 99/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7551 - accuracy: 0.9253 - val_loss: 0.4941 - val_accuracy: 0.8759\n",
      "Epoch 100/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7535 - accuracy: 0.9272 - val_loss: 0.4971 - val_accuracy: 0.8743\n",
      "Epoch 101/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7543 - accuracy: 0.9276 - val_loss: 0.4993 - val_accuracy: 0.8735\n",
      "Epoch 102/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7528 - accuracy: 0.9272 - val_loss: 0.4944 - val_accuracy: 0.8759\n",
      "Epoch 103/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7504 - accuracy: 0.9281 - val_loss: 0.4961 - val_accuracy: 0.8757\n",
      "Epoch 104/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7521 - accuracy: 0.9287 - val_loss: 0.4949 - val_accuracy: 0.8755\n",
      "Epoch 105/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7471 - accuracy: 0.9296 - val_loss: 0.4941 - val_accuracy: 0.8750\n",
      "Epoch 106/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7513 - accuracy: 0.9283 - val_loss: 0.4974 - val_accuracy: 0.8747\n",
      "Epoch 107/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7459 - accuracy: 0.9309 - val_loss: 0.4958 - val_accuracy: 0.8754\n",
      "Epoch 108/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7481 - accuracy: 0.9299 - val_loss: 0.4965 - val_accuracy: 0.8750\n",
      "10000/10000 [==============================] - 3s 286us/step\n",
      "Test loss: 0.4965029071331024\n",
      "Test accuracy: 0.875\n",
      "x_train shape: (40000, 32, 32, 3)\n",
      "y_train shape: (40000, 10)\n",
      "40000 train samples\n",
      "10000 test samples\n",
      "10000 validation samples\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "ResNet20v1\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/108\n",
      "156/156 [==============================] - 13s 82ms/step - loss: 2.2211 - accuracy: 0.2822 - val_loss: 2.0611 - val_accuracy: 0.2622\n",
      "Epoch 2/108\n",
      "  1/156 [..............................] - ETA: 10s - loss: 1.8892 - accuracy: 0.3711"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheverazo/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 10s 61ms/step - loss: 1.7869 - accuracy: 0.4553 - val_loss: 1.7284 - val_accuracy: 0.4380\n",
      "Epoch 3/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.5951 - accuracy: 0.5614 - val_loss: 2.0836 - val_accuracy: 0.3888\n",
      "Epoch 4/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.4570 - accuracy: 0.6386 - val_loss: 1.5161 - val_accuracy: 0.5294\n",
      "Epoch 5/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.3871 - accuracy: 0.6759 - val_loss: 1.6639 - val_accuracy: 0.4666\n",
      "Epoch 6/108\n",
      "156/156 [==============================] - 10s 65ms/step - loss: 1.3401 - accuracy: 0.7004 - val_loss: 1.2236 - val_accuracy: 0.6301\n",
      "Epoch 7/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.3061 - accuracy: 0.7191 - val_loss: 1.2778 - val_accuracy: 0.6221\n",
      "Epoch 8/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.2791 - accuracy: 0.7307 - val_loss: 1.0735 - val_accuracy: 0.6979\n",
      "Epoch 9/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.2539 - accuracy: 0.7432 - val_loss: 1.1823 - val_accuracy: 0.6668\n",
      "Epoch 10/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.2428 - accuracy: 0.7486 - val_loss: 1.3557 - val_accuracy: 0.6137\n",
      "Epoch 11/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.2272 - accuracy: 0.7553 - val_loss: 1.3370 - val_accuracy: 0.6237\n",
      "Epoch 12/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.2114 - accuracy: 0.7646 - val_loss: 1.3255 - val_accuracy: 0.6070\n",
      "Epoch 13/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.2048 - accuracy: 0.7686 - val_loss: 1.8334 - val_accuracy: 0.4854\n",
      "Epoch 14/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.1950 - accuracy: 0.7718 - val_loss: 1.5164 - val_accuracy: 0.5622\n",
      "Epoch 15/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1846 - accuracy: 0.7767 - val_loss: 1.0964 - val_accuracy: 0.6880\n",
      "Epoch 16/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1800 - accuracy: 0.7786 - val_loss: 1.2027 - val_accuracy: 0.6415\n",
      "Epoch 17/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1701 - accuracy: 0.7844 - val_loss: 1.2299 - val_accuracy: 0.6376\n",
      "Epoch 18/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1692 - accuracy: 0.7847 - val_loss: 1.2166 - val_accuracy: 0.6560\n",
      "Epoch 19/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.1569 - accuracy: 0.7909 - val_loss: 1.4727 - val_accuracy: 0.5725\n",
      "Epoch 20/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.1555 - accuracy: 0.7884 - val_loss: 1.1439 - val_accuracy: 0.6982\n",
      "Epoch 21/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.1547 - accuracy: 0.7895 - val_loss: 1.4553 - val_accuracy: 0.5823\n",
      "Epoch 22/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.1430 - accuracy: 0.7934 - val_loss: 1.1392 - val_accuracy: 0.6835\n",
      "Epoch 23/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1389 - accuracy: 0.7968 - val_loss: 1.2730 - val_accuracy: 0.6402\n",
      "Epoch 24/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.1382 - accuracy: 0.7988 - val_loss: 1.0390 - val_accuracy: 0.7124\n",
      "Epoch 25/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1244 - accuracy: 0.8037 - val_loss: 0.9783 - val_accuracy: 0.7382\n",
      "Epoch 26/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1231 - accuracy: 0.8033 - val_loss: 1.3400 - val_accuracy: 0.6270\n",
      "Epoch 27/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.1206 - accuracy: 0.8042 - val_loss: 1.2147 - val_accuracy: 0.6573\n",
      "Epoch 28/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1100 - accuracy: 0.8076 - val_loss: 1.3967 - val_accuracy: 0.5987\n",
      "Epoch 29/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1115 - accuracy: 0.8063 - val_loss: 1.0681 - val_accuracy: 0.7108\n",
      "Epoch 30/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.1054 - accuracy: 0.8086 - val_loss: 1.3928 - val_accuracy: 0.6113\n",
      "Epoch 31/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.0996 - accuracy: 0.8098 - val_loss: 1.0058 - val_accuracy: 0.7257\n",
      "Epoch 32/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0921 - accuracy: 0.8147 - val_loss: 0.9215 - val_accuracy: 0.7567\n",
      "Epoch 33/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0885 - accuracy: 0.8158 - val_loss: 1.1623 - val_accuracy: 0.6754\n",
      "Epoch 34/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0897 - accuracy: 0.8146 - val_loss: 1.4401 - val_accuracy: 0.6044\n",
      "Epoch 35/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0805 - accuracy: 0.8180 - val_loss: 1.2094 - val_accuracy: 0.6551\n",
      "Epoch 36/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.0727 - accuracy: 0.8233 - val_loss: 0.9260 - val_accuracy: 0.7435\n",
      "Epoch 37/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0692 - accuracy: 0.8252 - val_loss: 1.2267 - val_accuracy: 0.6557\n",
      "Epoch 38/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0643 - accuracy: 0.8234 - val_loss: 1.5621 - val_accuracy: 0.5738\n",
      "Epoch 39/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0639 - accuracy: 0.8229 - val_loss: 1.0165 - val_accuracy: 0.7249\n",
      "Epoch 40/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.0551 - accuracy: 0.8270 - val_loss: 0.9959 - val_accuracy: 0.7224\n",
      "Epoch 41/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.0509 - accuracy: 0.8286 - val_loss: 1.0378 - val_accuracy: 0.7122\n",
      "Epoch 42/108\n",
      "156/156 [==============================] - 10s 64ms/step - loss: 1.0468 - accuracy: 0.8310 - val_loss: 0.9574 - val_accuracy: 0.7324\n",
      "Epoch 43/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0437 - accuracy: 0.8315 - val_loss: 0.9676 - val_accuracy: 0.7367\n",
      "Epoch 44/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.0345 - accuracy: 0.8331 - val_loss: 0.9810 - val_accuracy: 0.7266\n",
      "Epoch 45/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.0265 - accuracy: 0.8362 - val_loss: 0.9782 - val_accuracy: 0.7259\n",
      "Epoch 46/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0279 - accuracy: 0.8360 - val_loss: 0.9073 - val_accuracy: 0.7602\n",
      "Epoch 47/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0255 - accuracy: 0.8365 - val_loss: 1.1522 - val_accuracy: 0.6753\n",
      "Epoch 48/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.0124 - accuracy: 0.8404 - val_loss: 0.8853 - val_accuracy: 0.7569\n",
      "Epoch 49/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0071 - accuracy: 0.8429 - val_loss: 0.9175 - val_accuracy: 0.7451\n",
      "Epoch 50/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0054 - accuracy: 0.8458 - val_loss: 1.0044 - val_accuracy: 0.7250\n",
      "Epoch 51/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9991 - accuracy: 0.8444 - val_loss: 0.8461 - val_accuracy: 0.7707\n",
      "Epoch 52/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9947 - accuracy: 0.8481 - val_loss: 0.9316 - val_accuracy: 0.7452\n",
      "Epoch 53/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9875 - accuracy: 0.8494 - val_loss: 0.7808 - val_accuracy: 0.7953\n",
      "Epoch 54/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9808 - accuracy: 0.8511 - val_loss: 0.8781 - val_accuracy: 0.7605\n",
      "Epoch 55/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9765 - accuracy: 0.8524 - val_loss: 0.8983 - val_accuracy: 0.7465\n",
      "Epoch 56/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9695 - accuracy: 0.8548 - val_loss: 0.7114 - val_accuracy: 0.8205\n",
      "Epoch 57/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9692 - accuracy: 0.8550 - val_loss: 1.2142 - val_accuracy: 0.6585\n",
      "Epoch 58/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9520 - accuracy: 0.8600 - val_loss: 0.8735 - val_accuracy: 0.7583\n",
      "Epoch 59/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9555 - accuracy: 0.8590 - val_loss: 0.9809 - val_accuracy: 0.7205\n",
      "Epoch 60/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9487 - accuracy: 0.8622 - val_loss: 1.1252 - val_accuracy: 0.6784\n",
      "Epoch 61/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9409 - accuracy: 0.8644 - val_loss: 0.9472 - val_accuracy: 0.7416\n",
      "Epoch 62/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9386 - accuracy: 0.8628 - val_loss: 0.9561 - val_accuracy: 0.7272\n",
      "Epoch 63/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9249 - accuracy: 0.8707 - val_loss: 1.0193 - val_accuracy: 0.7202\n",
      "Epoch 64/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9209 - accuracy: 0.8694 - val_loss: 0.7316 - val_accuracy: 0.8033\n",
      "Epoch 65/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9201 - accuracy: 0.8694 - val_loss: 0.9116 - val_accuracy: 0.7504\n",
      "Epoch 66/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9117 - accuracy: 0.8738 - val_loss: 0.7782 - val_accuracy: 0.7855\n",
      "Epoch 67/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9057 - accuracy: 0.8766 - val_loss: 0.9679 - val_accuracy: 0.7210\n",
      "Epoch 68/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.8972 - accuracy: 0.8788 - val_loss: 0.7158 - val_accuracy: 0.8051\n",
      "Epoch 69/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8938 - accuracy: 0.8792 - val_loss: 0.7364 - val_accuracy: 0.8013\n",
      "Epoch 70/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8858 - accuracy: 0.8816 - val_loss: 0.7297 - val_accuracy: 0.7980\n",
      "Epoch 71/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8792 - accuracy: 0.8842 - val_loss: 0.7184 - val_accuracy: 0.8166\n",
      "Epoch 72/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8728 - accuracy: 0.8851 - val_loss: 0.7133 - val_accuracy: 0.8093\n",
      "Epoch 73/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.8715 - accuracy: 0.8845 - val_loss: 0.6698 - val_accuracy: 0.8237\n",
      "Epoch 74/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8619 - accuracy: 0.8907 - val_loss: 0.6587 - val_accuracy: 0.8259\n",
      "Epoch 75/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8595 - accuracy: 0.8894 - val_loss: 0.7996 - val_accuracy: 0.7787\n",
      "Epoch 76/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8502 - accuracy: 0.8951 - val_loss: 0.6135 - val_accuracy: 0.8438\n",
      "Epoch 77/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8442 - accuracy: 0.8961 - val_loss: 0.6389 - val_accuracy: 0.8299\n",
      "Epoch 78/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8378 - accuracy: 0.8985 - val_loss: 0.6408 - val_accuracy: 0.8319\n",
      "Epoch 79/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8312 - accuracy: 0.9001 - val_loss: 0.6048 - val_accuracy: 0.8392\n",
      "Epoch 80/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8225 - accuracy: 0.9031 - val_loss: 0.7293 - val_accuracy: 0.8038\n",
      "Epoch 81/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.8236 - accuracy: 0.9040 - val_loss: 0.6421 - val_accuracy: 0.8307\n",
      "Epoch 82/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.8128 - accuracy: 0.9066 - val_loss: 0.6156 - val_accuracy: 0.8402\n",
      "Epoch 83/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8089 - accuracy: 0.9067 - val_loss: 0.5661 - val_accuracy: 0.8514\n",
      "Epoch 84/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8027 - accuracy: 0.9105 - val_loss: 0.7286 - val_accuracy: 0.8070\n",
      "Epoch 85/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8004 - accuracy: 0.9109 - val_loss: 0.5695 - val_accuracy: 0.8508\n",
      "Epoch 86/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7918 - accuracy: 0.9144 - val_loss: 0.5606 - val_accuracy: 0.8557\n",
      "Epoch 87/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7839 - accuracy: 0.9171 - val_loss: 0.5668 - val_accuracy: 0.8507\n",
      "Epoch 88/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.7821 - accuracy: 0.9169 - val_loss: 0.5627 - val_accuracy: 0.8521\n",
      "Epoch 89/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7746 - accuracy: 0.9207 - val_loss: 0.5243 - val_accuracy: 0.8698\n",
      "Epoch 90/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7715 - accuracy: 0.9223 - val_loss: 0.5421 - val_accuracy: 0.8626\n",
      "Epoch 91/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7704 - accuracy: 0.9216 - val_loss: 0.5404 - val_accuracy: 0.8607\n",
      "Epoch 92/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7632 - accuracy: 0.9248 - val_loss: 0.5079 - val_accuracy: 0.8770\n",
      "Epoch 93/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7631 - accuracy: 0.9242 - val_loss: 0.5048 - val_accuracy: 0.8762\n",
      "Epoch 94/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7560 - accuracy: 0.9264 - val_loss: 0.5003 - val_accuracy: 0.8760\n",
      "Epoch 95/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7515 - accuracy: 0.9294 - val_loss: 0.4930 - val_accuracy: 0.8764\n",
      "Epoch 96/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7497 - accuracy: 0.9290 - val_loss: 0.5005 - val_accuracy: 0.8761\n",
      "Epoch 97/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7485 - accuracy: 0.9293 - val_loss: 0.5294 - val_accuracy: 0.8653\n",
      "Epoch 98/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7413 - accuracy: 0.9336 - val_loss: 0.4991 - val_accuracy: 0.8748\n",
      "Epoch 99/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7423 - accuracy: 0.9335 - val_loss: 0.5060 - val_accuracy: 0.8740\n",
      "Epoch 100/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.7415 - accuracy: 0.9333 - val_loss: 0.4955 - val_accuracy: 0.8780\n",
      "Epoch 101/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7397 - accuracy: 0.9338 - val_loss: 0.4933 - val_accuracy: 0.8789\n",
      "Epoch 102/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7371 - accuracy: 0.9364 - val_loss: 0.4951 - val_accuracy: 0.8783\n",
      "Epoch 103/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.7362 - accuracy: 0.9365 - val_loss: 0.4926 - val_accuracy: 0.8803\n",
      "Epoch 104/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7353 - accuracy: 0.9359 - val_loss: 0.4920 - val_accuracy: 0.8802\n",
      "Epoch 105/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.7324 - accuracy: 0.9372 - val_loss: 0.4911 - val_accuracy: 0.8814\n",
      "Epoch 106/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7347 - accuracy: 0.9366 - val_loss: 0.4947 - val_accuracy: 0.8789\n",
      "Epoch 107/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.7326 - accuracy: 0.9371 - val_loss: 0.4930 - val_accuracy: 0.8803\n",
      "Epoch 108/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7347 - accuracy: 0.9364 - val_loss: 0.4939 - val_accuracy: 0.8809\n",
      "10000/10000 [==============================] - 2s 235us/step\n",
      "Test loss: 0.49387975397109984\n",
      "Test accuracy: 0.8809000253677368\n",
      "x_train shape: (40000, 32, 32, 3)\n",
      "y_train shape: (40000, 10)\n",
      "40000 train samples\n",
      "10000 test samples\n",
      "10000 validation samples\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "ResNet20v1\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/108\n",
      "156/156 [==============================] - 13s 82ms/step - loss: 2.2084 - accuracy: 0.2830 - val_loss: 2.1928 - val_accuracy: 0.2802\n",
      "Epoch 2/108\n",
      "  1/156 [..............................] - ETA: 7s - loss: 1.9050 - accuracy: 0.4102"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheverazo/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 9s 60ms/step - loss: 1.8088 - accuracy: 0.4409 - val_loss: 1.7609 - val_accuracy: 0.4069\n",
      "Epoch 3/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.6287 - accuracy: 0.5399 - val_loss: 1.8150 - val_accuracy: 0.4149\n",
      "Epoch 4/108\n",
      "156/156 [==============================] - 10s 65ms/step - loss: 1.5011 - accuracy: 0.6119 - val_loss: 1.5105 - val_accuracy: 0.5303\n",
      "Epoch 5/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.4181 - accuracy: 0.6565 - val_loss: 1.2638 - val_accuracy: 0.6104\n",
      "Epoch 6/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.3581 - accuracy: 0.6870 - val_loss: 1.1927 - val_accuracy: 0.6635\n",
      "Epoch 7/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.3180 - accuracy: 0.7107 - val_loss: 1.3648 - val_accuracy: 0.5837\n",
      "Epoch 8/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.2903 - accuracy: 0.7244 - val_loss: 1.3061 - val_accuracy: 0.6149\n",
      "Epoch 9/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.2716 - accuracy: 0.7371 - val_loss: 1.3249 - val_accuracy: 0.6054\n",
      "Epoch 10/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.2517 - accuracy: 0.7443 - val_loss: 1.4389 - val_accuracy: 0.5840\n",
      "Epoch 11/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.2355 - accuracy: 0.7523 - val_loss: 1.3942 - val_accuracy: 0.5676\n",
      "Epoch 12/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.2239 - accuracy: 0.7588 - val_loss: 1.4685 - val_accuracy: 0.5862\n",
      "Epoch 13/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.2179 - accuracy: 0.7614 - val_loss: 1.0550 - val_accuracy: 0.7090\n",
      "Epoch 14/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.2063 - accuracy: 0.7669 - val_loss: 1.1286 - val_accuracy: 0.6702\n",
      "Epoch 15/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.2001 - accuracy: 0.7710 - val_loss: 1.0175 - val_accuracy: 0.7315\n",
      "Epoch 16/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.1890 - accuracy: 0.7757 - val_loss: 1.2826 - val_accuracy: 0.6462\n",
      "Epoch 17/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1800 - accuracy: 0.7790 - val_loss: 1.5597 - val_accuracy: 0.5532\n",
      "Epoch 18/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1742 - accuracy: 0.7818 - val_loss: 1.7468 - val_accuracy: 0.5122\n",
      "Epoch 19/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1694 - accuracy: 0.7835 - val_loss: 1.0871 - val_accuracy: 0.6937\n",
      "Epoch 20/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.1632 - accuracy: 0.7865 - val_loss: 1.0589 - val_accuracy: 0.7119\n",
      "Epoch 21/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1546 - accuracy: 0.7932 - val_loss: 1.2836 - val_accuracy: 0.6344\n",
      "Epoch 22/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1518 - accuracy: 0.7900 - val_loss: 1.3124 - val_accuracy: 0.6163\n",
      "Epoch 23/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1407 - accuracy: 0.7959 - val_loss: 1.1040 - val_accuracy: 0.6963\n",
      "Epoch 24/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1408 - accuracy: 0.7940 - val_loss: 1.1726 - val_accuracy: 0.6866\n",
      "Epoch 25/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.1291 - accuracy: 0.7998 - val_loss: 1.2266 - val_accuracy: 0.6489\n",
      "Epoch 26/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.1295 - accuracy: 0.7996 - val_loss: 1.0473 - val_accuracy: 0.7085\n",
      "Epoch 27/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1217 - accuracy: 0.8024 - val_loss: 1.0519 - val_accuracy: 0.7086\n",
      "Epoch 28/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1132 - accuracy: 0.8056 - val_loss: 1.1433 - val_accuracy: 0.6801\n",
      "Epoch 29/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1118 - accuracy: 0.8061 - val_loss: 1.0319 - val_accuracy: 0.7084\n",
      "Epoch 30/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1069 - accuracy: 0.8073 - val_loss: 0.9856 - val_accuracy: 0.7376\n",
      "Epoch 31/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1033 - accuracy: 0.8095 - val_loss: 1.2176 - val_accuracy: 0.6523\n",
      "Epoch 32/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.0969 - accuracy: 0.8117 - val_loss: 1.0495 - val_accuracy: 0.7104\n",
      "Epoch 33/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.0944 - accuracy: 0.8136 - val_loss: 1.0282 - val_accuracy: 0.7153\n",
      "Epoch 34/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.0841 - accuracy: 0.8181 - val_loss: 1.2688 - val_accuracy: 0.6533\n",
      "Epoch 35/108\n",
      "156/156 [==============================] - 10s 63ms/step - loss: 1.0865 - accuracy: 0.8146 - val_loss: 0.9083 - val_accuracy: 0.7509\n",
      "Epoch 36/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0838 - accuracy: 0.8162 - val_loss: 1.1917 - val_accuracy: 0.6607\n",
      "Epoch 37/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.0770 - accuracy: 0.8177 - val_loss: 1.1121 - val_accuracy: 0.6832\n",
      "Epoch 38/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.0644 - accuracy: 0.8253 - val_loss: 1.0824 - val_accuracy: 0.6949\n",
      "Epoch 39/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0661 - accuracy: 0.8231 - val_loss: 1.3304 - val_accuracy: 0.5936\n",
      "Epoch 40/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.0557 - accuracy: 0.8270 - val_loss: 0.9937 - val_accuracy: 0.7282\n",
      "Epoch 41/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.0583 - accuracy: 0.8244 - val_loss: 0.9210 - val_accuracy: 0.7528\n",
      "Epoch 42/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0469 - accuracy: 0.8306 - val_loss: 1.1253 - val_accuracy: 0.6808\n",
      "Epoch 43/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0421 - accuracy: 0.8306 - val_loss: 0.8689 - val_accuracy: 0.7694\n",
      "Epoch 44/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.0422 - accuracy: 0.8296 - val_loss: 0.9662 - val_accuracy: 0.7289\n",
      "Epoch 45/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0310 - accuracy: 0.8365 - val_loss: 0.8910 - val_accuracy: 0.7495\n",
      "Epoch 46/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.0274 - accuracy: 0.8384 - val_loss: 0.9237 - val_accuracy: 0.7567\n",
      "Epoch 47/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0261 - accuracy: 0.8375 - val_loss: 0.8219 - val_accuracy: 0.7859\n",
      "Epoch 48/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0172 - accuracy: 0.8374 - val_loss: 1.0562 - val_accuracy: 0.7024\n",
      "Epoch 49/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0108 - accuracy: 0.8419 - val_loss: 0.8130 - val_accuracy: 0.7818\n",
      "Epoch 50/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 1.0098 - accuracy: 0.8412 - val_loss: 0.7590 - val_accuracy: 0.7945\n",
      "Epoch 51/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0006 - accuracy: 0.8456 - val_loss: 0.8493 - val_accuracy: 0.7711\n",
      "Epoch 52/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9952 - accuracy: 0.8466 - val_loss: 0.9487 - val_accuracy: 0.7404\n",
      "Epoch 53/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9952 - accuracy: 0.8453 - val_loss: 1.0151 - val_accuracy: 0.7076\n",
      "Epoch 54/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9869 - accuracy: 0.8474 - val_loss: 0.9080 - val_accuracy: 0.7482\n",
      "Epoch 55/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9786 - accuracy: 0.8531 - val_loss: 1.1113 - val_accuracy: 0.6702\n",
      "Epoch 56/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9728 - accuracy: 0.8543 - val_loss: 0.7576 - val_accuracy: 0.7933\n",
      "Epoch 57/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.9673 - accuracy: 0.8560 - val_loss: 0.8204 - val_accuracy: 0.7836\n",
      "Epoch 58/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9612 - accuracy: 0.8571 - val_loss: 0.8574 - val_accuracy: 0.7542\n",
      "Epoch 59/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9598 - accuracy: 0.8560 - val_loss: 1.1285 - val_accuracy: 0.6815\n",
      "Epoch 60/108\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.9502 - accuracy: 0.8616 - val_loss: 0.7882 - val_accuracy: 0.7839\n",
      "Epoch 61/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.9394 - accuracy: 0.8652 - val_loss: 0.7580 - val_accuracy: 0.7937\n",
      "Epoch 62/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9361 - accuracy: 0.8672 - val_loss: 0.8499 - val_accuracy: 0.7739\n",
      "Epoch 63/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9295 - accuracy: 0.8669 - val_loss: 0.9020 - val_accuracy: 0.7548\n",
      "Epoch 64/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9281 - accuracy: 0.8689 - val_loss: 0.7300 - val_accuracy: 0.8048\n",
      "Epoch 65/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.9246 - accuracy: 0.8696 - val_loss: 0.7582 - val_accuracy: 0.7988\n",
      "Epoch 66/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9118 - accuracy: 0.8732 - val_loss: 0.7201 - val_accuracy: 0.8050\n",
      "Epoch 67/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.9070 - accuracy: 0.8757 - val_loss: 0.7075 - val_accuracy: 0.8112\n",
      "Epoch 68/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9008 - accuracy: 0.8765 - val_loss: 0.6897 - val_accuracy: 0.8187\n",
      "Epoch 69/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8948 - accuracy: 0.8796 - val_loss: 0.7828 - val_accuracy: 0.7895\n",
      "Epoch 70/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8887 - accuracy: 0.8825 - val_loss: 0.6958 - val_accuracy: 0.8142\n",
      "Epoch 71/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8840 - accuracy: 0.8833 - val_loss: 0.6111 - val_accuracy: 0.8475\n",
      "Epoch 72/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8742 - accuracy: 0.8856 - val_loss: 0.6716 - val_accuracy: 0.8188\n",
      "Epoch 73/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8684 - accuracy: 0.8884 - val_loss: 0.7315 - val_accuracy: 0.7999\n",
      "Epoch 74/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8650 - accuracy: 0.8888 - val_loss: 0.6951 - val_accuracy: 0.8117\n",
      "Epoch 75/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8596 - accuracy: 0.8909 - val_loss: 0.6738 - val_accuracy: 0.8249\n",
      "Epoch 76/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.8466 - accuracy: 0.8963 - val_loss: 0.7212 - val_accuracy: 0.8058\n",
      "Epoch 77/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8515 - accuracy: 0.8938 - val_loss: 0.5985 - val_accuracy: 0.8470\n",
      "Epoch 78/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8402 - accuracy: 0.8977 - val_loss: 0.6256 - val_accuracy: 0.8382\n",
      "Epoch 79/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8307 - accuracy: 0.9021 - val_loss: 0.6133 - val_accuracy: 0.8392\n",
      "Epoch 80/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8286 - accuracy: 0.9022 - val_loss: 0.6608 - val_accuracy: 0.8262\n",
      "Epoch 81/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.8238 - accuracy: 0.9036 - val_loss: 0.6236 - val_accuracy: 0.8397\n",
      "Epoch 82/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8181 - accuracy: 0.9057 - val_loss: 0.5984 - val_accuracy: 0.8455\n",
      "Epoch 83/108\n",
      "156/156 [==============================] - 10s 63ms/step - loss: 0.8133 - accuracy: 0.9081 - val_loss: 0.5743 - val_accuracy: 0.8517\n",
      "Epoch 84/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.8029 - accuracy: 0.9108 - val_loss: 0.5940 - val_accuracy: 0.8479\n",
      "Epoch 85/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8022 - accuracy: 0.9107 - val_loss: 0.5687 - val_accuracy: 0.8517\n",
      "Epoch 86/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7967 - accuracy: 0.9145 - val_loss: 0.5467 - val_accuracy: 0.8588\n",
      "Epoch 87/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7881 - accuracy: 0.9167 - val_loss: 0.5108 - val_accuracy: 0.8736\n",
      "Epoch 88/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7860 - accuracy: 0.9177 - val_loss: 0.5454 - val_accuracy: 0.8614\n",
      "Epoch 89/108\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7813 - accuracy: 0.9185 - val_loss: 0.5345 - val_accuracy: 0.8652\n",
      "Epoch 90/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7758 - accuracy: 0.9211 - val_loss: 0.5373 - val_accuracy: 0.8641\n",
      "Epoch 91/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.7695 - accuracy: 0.9237 - val_loss: 0.5466 - val_accuracy: 0.8576\n",
      "Epoch 92/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7676 - accuracy: 0.9232 - val_loss: 0.5114 - val_accuracy: 0.8719\n",
      "Epoch 93/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7652 - accuracy: 0.9250 - val_loss: 0.5297 - val_accuracy: 0.8678\n",
      "Epoch 94/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7589 - accuracy: 0.9276 - val_loss: 0.5083 - val_accuracy: 0.8724\n",
      "Epoch 95/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7574 - accuracy: 0.9281 - val_loss: 0.5325 - val_accuracy: 0.8657\n",
      "Epoch 96/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7531 - accuracy: 0.9310 - val_loss: 0.5104 - val_accuracy: 0.8751\n",
      "Epoch 97/108\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.7512 - accuracy: 0.9303 - val_loss: 0.4988 - val_accuracy: 0.8776\n",
      "Epoch 98/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.7493 - accuracy: 0.9315 - val_loss: 0.5050 - val_accuracy: 0.8747\n",
      "Epoch 99/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7429 - accuracy: 0.9351 - val_loss: 0.5032 - val_accuracy: 0.8773\n",
      "Epoch 100/108\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.7440 - accuracy: 0.9337 - val_loss: 0.5158 - val_accuracy: 0.8723\n",
      "Epoch 101/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7387 - accuracy: 0.9373 - val_loss: 0.5039 - val_accuracy: 0.8760\n",
      "Epoch 102/108\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.7395 - accuracy: 0.9342 - val_loss: 0.4927 - val_accuracy: 0.8821\n",
      "Epoch 103/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7416 - accuracy: 0.9350 - val_loss: 0.4952 - val_accuracy: 0.8796\n",
      "Epoch 104/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7386 - accuracy: 0.9359 - val_loss: 0.4950 - val_accuracy: 0.8799\n",
      "Epoch 105/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7345 - accuracy: 0.9386 - val_loss: 0.4973 - val_accuracy: 0.8796\n",
      "Epoch 106/108\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7362 - accuracy: 0.9364 - val_loss: 0.4972 - val_accuracy: 0.8799\n",
      "Epoch 107/108\n",
      "156/156 [==============================] - 10s 64ms/step - loss: 0.7352 - accuracy: 0.9390 - val_loss: 0.4964 - val_accuracy: 0.8806\n",
      "Epoch 108/108\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7358 - accuracy: 0.9375 - val_loss: 0.4965 - val_accuracy: 0.8807\n",
      "10000/10000 [==============================] - 2s 200us/step\n",
      "Test loss: 0.4965403371810913\n",
      "Test accuracy: 0.8806999921798706\n"
     ]
    }
   ],
   "source": [
    "iterations = 5\n",
    "# results = {}\n",
    "for smooth in [0, 0.1]:\n",
    "    for base_lr in [0.1, 0.01]:\n",
    "        for sc in ['nasbench']:\n",
    "            results[(sc, smooth, base_lr)] = []\n",
    "            for i in range(iterations):\n",
    "                if sc == 'nasbench':\n",
    "                    optimizer = 'RMSProp'\n",
    "                    sc_ = 'cosine'\n",
    "                else:\n",
    "                    optimizer = 'Adam'\n",
    "                    sc_ = sc\n",
    "                model, callbacks, score = evaluate_training(valid_split, data_augmentation, smooth, sc_, batch_size, epochs, base_lr, optimizer)\n",
    "                results[(sc, smooth, base_lr)].append(score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nasbench', 0, 0.1)\n",
      "mean: 0.4795\n",
      "std: 0.0276\n",
      "\n",
      "('cosine', 0, 0.1)\n",
      "mean: 0.5704\n",
      "std: 0.0276\n",
      "\n",
      "('warmup_cosine', 0, 0.1)\n",
      "mean: 0.7582\n",
      "std: 0.0084\n",
      "\n",
      "('clrs', 0, 0.1)\n",
      "mean: 0.7615\n",
      "std: 0.0044\n",
      "\n",
      "('nasbench', 0, 0.01)\n",
      "mean: 0.8781\n",
      "std: 0.0042\n",
      "\n",
      "('cosine', 0, 0.01)\n",
      "mean: 0.8907\n",
      "std: 0.0010\n",
      "\n",
      "('warmup_cosine', 0, 0.01)\n",
      "mean: 0.8998\n",
      "std: 0.0023\n",
      "\n",
      "('clrs', 0, 0.01)\n",
      "mean: 0.9012\n",
      "std: 0.0024\n",
      "\n",
      "('nasbench', 0.1, 0.1)\n",
      "mean: 0.4848\n",
      "std: 0.0244\n",
      "\n",
      "('cosine', 0.1, 0.1)\n",
      "mean: 0.5883\n",
      "std: 0.0363\n",
      "\n",
      "('warmup_cosine', 0.1, 0.1)\n",
      "mean: 0.7596\n",
      "std: 0.0052\n",
      "\n",
      "('clrs', 0.1, 0.1)\n",
      "mean: 0.7567\n",
      "std: 0.0057\n",
      "\n",
      "('nasbench', 0.1, 0.01)\n",
      "mean: 0.8770\n",
      "std: 0.0041\n",
      "\n",
      "('cosine', 0.1, 0.01)\n",
      "mean: 0.8907\n",
      "std: 0.0030\n",
      "\n",
      "('warmup_cosine', 0.1, 0.01)\n",
      "mean: 0.8992\n",
      "std: 0.0019\n",
      "\n",
      "('clrs', 0.1, 0.01)\n",
      "mean: 0.8994\n",
      "std: 0.0019\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sc in results.keys():\n",
    "    print(sc)\n",
    "    print(\"mean: %0.4f\" % np.mean(results[sc]))\n",
    "    print(\"std: %0.4f\" % np.std(results[sc]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set is the test set\n",
      "x_train shape: (50000, 32, 32, 3)\n",
      "y_train shape: (50000, 10)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "10000 validation samples\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "ResNet20v1\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "195/195 [==============================] - 17s 85ms/step - loss: 2.2320 - accuracy: 0.2678 - val_loss: 2.7474 - val_accuracy: 0.1505\n",
      "Epoch 2/200\n",
      "  1/195 [..............................] - ETA: 12s - loss: 1.8165 - accuracy: 0.4219"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheverazo/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - 12s 60ms/step - loss: 1.6898 - accuracy: 0.4438 - val_loss: 1.7389 - val_accuracy: 0.4205\n",
      "Epoch 3/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 1.5169 - accuracy: 0.5159 - val_loss: 1.6272 - val_accuracy: 0.4722\n",
      "Epoch 4/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 1.3796 - accuracy: 0.5684 - val_loss: 1.6096 - val_accuracy: 0.5143\n",
      "Epoch 5/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 1.2713 - accuracy: 0.6126 - val_loss: 1.7837 - val_accuracy: 0.4962\n",
      "Epoch 6/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 1.1741 - accuracy: 0.6482 - val_loss: 1.2275 - val_accuracy: 0.6148\n",
      "Epoch 7/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 1.0922 - accuracy: 0.6797 - val_loss: 1.1328 - val_accuracy: 0.6595\n",
      "Epoch 8/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 1.0167 - accuracy: 0.7130 - val_loss: 1.2999 - val_accuracy: 0.6105\n",
      "Epoch 9/200\n",
      "195/195 [==============================] - 12s 63ms/step - loss: 0.9535 - accuracy: 0.7341 - val_loss: 1.1284 - val_accuracy: 0.6597\n",
      "Epoch 10/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.9126 - accuracy: 0.7498 - val_loss: 1.2271 - val_accuracy: 0.6601\n",
      "Epoch 11/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.8782 - accuracy: 0.7641 - val_loss: 1.3809 - val_accuracy: 0.6002\n",
      "Epoch 12/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.8452 - accuracy: 0.7746 - val_loss: 0.9760 - val_accuracy: 0.7170\n",
      "Epoch 13/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.8114 - accuracy: 0.7892 - val_loss: 0.9211 - val_accuracy: 0.7343\n",
      "Epoch 14/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.7955 - accuracy: 0.7973 - val_loss: 1.0589 - val_accuracy: 0.7052\n",
      "Epoch 15/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.7724 - accuracy: 0.8078 - val_loss: 0.8441 - val_accuracy: 0.7675\n",
      "Epoch 16/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.7642 - accuracy: 0.8105 - val_loss: 1.0606 - val_accuracy: 0.7021\n",
      "Epoch 17/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.7545 - accuracy: 0.8154 - val_loss: 1.0784 - val_accuracy: 0.7060\n",
      "Epoch 18/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.7419 - accuracy: 0.8215 - val_loss: 1.3049 - val_accuracy: 0.6465\n",
      "Epoch 19/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.7425 - accuracy: 0.8242 - val_loss: 1.0084 - val_accuracy: 0.7223\n",
      "Epoch 20/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.7225 - accuracy: 0.8326 - val_loss: 1.1058 - val_accuracy: 0.7116\n",
      "Epoch 21/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.7342 - accuracy: 0.8331 - val_loss: 0.9992 - val_accuracy: 0.7369\n",
      "Epoch 22/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.7331 - accuracy: 0.8327 - val_loss: 0.9004 - val_accuracy: 0.7679\n",
      "Epoch 23/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.7377 - accuracy: 0.8330 - val_loss: 1.0963 - val_accuracy: 0.7193\n",
      "Epoch 24/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.7291 - accuracy: 0.8391 - val_loss: 0.9899 - val_accuracy: 0.7415\n",
      "Epoch 25/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.7297 - accuracy: 0.8413 - val_loss: 0.9161 - val_accuracy: 0.7598\n",
      "Epoch 26/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.7381 - accuracy: 0.8393 - val_loss: 1.4244 - val_accuracy: 0.6360\n",
      "Epoch 27/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.7484 - accuracy: 0.8410 - val_loss: 1.4032 - val_accuracy: 0.6365\n",
      "Epoch 28/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.7567 - accuracy: 0.8386 - val_loss: 0.8837 - val_accuracy: 0.7816\n",
      "Epoch 29/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.7579 - accuracy: 0.8387 - val_loss: 0.8492 - val_accuracy: 0.7863\n",
      "Epoch 30/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.7553 - accuracy: 0.8444 - val_loss: 0.9763 - val_accuracy: 0.7567\n",
      "Epoch 31/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.7644 - accuracy: 0.8411 - val_loss: 2.5218 - val_accuracy: 0.4831\n",
      "Epoch 32/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.7702 - accuracy: 0.8440 - val_loss: 0.8900 - val_accuracy: 0.7867\n",
      "Epoch 33/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.7755 - accuracy: 0.8423 - val_loss: 1.4756 - val_accuracy: 0.6301\n",
      "Epoch 34/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.7807 - accuracy: 0.8418 - val_loss: 1.2338 - val_accuracy: 0.6806\n",
      "Epoch 35/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.7894 - accuracy: 0.8397 - val_loss: 0.8986 - val_accuracy: 0.7899\n",
      "Epoch 36/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.7879 - accuracy: 0.8415 - val_loss: 1.1529 - val_accuracy: 0.7185\n",
      "Epoch 37/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.7995 - accuracy: 0.8376 - val_loss: 1.1093 - val_accuracy: 0.7340\n",
      "Epoch 38/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.8073 - accuracy: 0.8351 - val_loss: 1.1540 - val_accuracy: 0.7023\n",
      "Epoch 39/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.8116 - accuracy: 0.8369 - val_loss: 2.3254 - val_accuracy: 0.4867\n",
      "Epoch 40/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.8116 - accuracy: 0.8381 - val_loss: 1.0513 - val_accuracy: 0.7400\n",
      "Epoch 41/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.8104 - accuracy: 0.8395 - val_loss: 1.2065 - val_accuracy: 0.7042\n",
      "Epoch 42/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.8236 - accuracy: 0.8372 - val_loss: 1.1362 - val_accuracy: 0.7233\n",
      "Epoch 43/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.8242 - accuracy: 0.8373 - val_loss: 1.3406 - val_accuracy: 0.6796\n",
      "Epoch 44/200\n",
      "195/195 [==============================] - 12s 63ms/step - loss: 0.8311 - accuracy: 0.8357 - val_loss: 1.3035 - val_accuracy: 0.6713\n",
      "Epoch 45/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.8403 - accuracy: 0.8331 - val_loss: 1.6306 - val_accuracy: 0.6233\n",
      "Epoch 46/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.8359 - accuracy: 0.8352 - val_loss: 1.1501 - val_accuracy: 0.7184\n",
      "Epoch 47/200\n",
      "195/195 [==============================] - 12s 64ms/step - loss: 0.8471 - accuracy: 0.8315 - val_loss: 1.1343 - val_accuracy: 0.7239\n",
      "Epoch 48/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.8442 - accuracy: 0.8342 - val_loss: 1.0968 - val_accuracy: 0.7346\n",
      "Epoch 49/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.8493 - accuracy: 0.8312 - val_loss: 1.2610 - val_accuracy: 0.6932\n",
      "Epoch 50/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.8523 - accuracy: 0.8297 - val_loss: 1.1734 - val_accuracy: 0.6998\n",
      "Epoch 51/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.8534 - accuracy: 0.8298 - val_loss: 1.9912 - val_accuracy: 0.5053\n",
      "Epoch 52/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.8589 - accuracy: 0.8293 - val_loss: 1.0921 - val_accuracy: 0.7389\n",
      "Epoch 53/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.8635 - accuracy: 0.8300 - val_loss: 1.4537 - val_accuracy: 0.6505\n",
      "Epoch 54/200\n",
      "195/195 [==============================] - 12s 63ms/step - loss: 0.8674 - accuracy: 0.8256 - val_loss: 1.4907 - val_accuracy: 0.6348\n",
      "Epoch 55/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.8608 - accuracy: 0.8298 - val_loss: 1.5278 - val_accuracy: 0.6355\n",
      "Epoch 56/200\n",
      "195/195 [==============================] - 12s 63ms/step - loss: 0.8546 - accuracy: 0.8293 - val_loss: 1.5889 - val_accuracy: 0.6135\n",
      "Epoch 57/200\n",
      "195/195 [==============================] - 12s 63ms/step - loss: 0.8439 - accuracy: 0.8313 - val_loss: 1.4298 - val_accuracy: 0.6654\n",
      "Epoch 58/200\n",
      "195/195 [==============================] - 12s 59ms/step - loss: 0.8403 - accuracy: 0.8289 - val_loss: 2.1175 - val_accuracy: 0.5504\n",
      "Epoch 59/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.8333 - accuracy: 0.8321 - val_loss: 1.2163 - val_accuracy: 0.6985\n",
      "Epoch 60/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.8120 - accuracy: 0.8380 - val_loss: 1.3328 - val_accuracy: 0.6761\n",
      "Epoch 61/200\n",
      "195/195 [==============================] - 12s 59ms/step - loss: 0.8158 - accuracy: 0.8354 - val_loss: 0.9759 - val_accuracy: 0.7528\n",
      "Epoch 62/200\n",
      "195/195 [==============================] - 12s 59ms/step - loss: 0.8056 - accuracy: 0.8376 - val_loss: 1.0856 - val_accuracy: 0.7351\n",
      "Epoch 63/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.7981 - accuracy: 0.8408 - val_loss: 1.3136 - val_accuracy: 0.6716\n",
      "Epoch 64/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.7846 - accuracy: 0.8433 - val_loss: 1.1578 - val_accuracy: 0.7083\n",
      "Epoch 65/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.7786 - accuracy: 0.8441 - val_loss: 0.9626 - val_accuracy: 0.7645\n",
      "Epoch 66/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.7657 - accuracy: 0.8470 - val_loss: 0.9249 - val_accuracy: 0.7741\n",
      "Epoch 67/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.7616 - accuracy: 0.8445 - val_loss: 1.3820 - val_accuracy: 0.6522\n",
      "Epoch 68/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.7554 - accuracy: 0.8476 - val_loss: 1.0386 - val_accuracy: 0.7430\n",
      "Epoch 69/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.7358 - accuracy: 0.8527 - val_loss: 0.9224 - val_accuracy: 0.7680\n",
      "Epoch 70/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.7418 - accuracy: 0.8506 - val_loss: 1.1119 - val_accuracy: 0.7122\n",
      "Epoch 71/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.7365 - accuracy: 0.8515 - val_loss: 1.0378 - val_accuracy: 0.7406\n",
      "Epoch 72/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.7157 - accuracy: 0.8567 - val_loss: 0.9888 - val_accuracy: 0.7539\n",
      "Epoch 73/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.7112 - accuracy: 0.8556 - val_loss: 1.1918 - val_accuracy: 0.6953\n",
      "Epoch 74/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.7018 - accuracy: 0.8581 - val_loss: 0.9498 - val_accuracy: 0.7617\n",
      "Epoch 75/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.6964 - accuracy: 0.8593 - val_loss: 0.9338 - val_accuracy: 0.7632\n",
      "Epoch 76/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.6770 - accuracy: 0.8645 - val_loss: 0.9433 - val_accuracy: 0.7558\n",
      "Epoch 77/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.6725 - accuracy: 0.8651 - val_loss: 1.0857 - val_accuracy: 0.7174\n",
      "Epoch 78/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.6628 - accuracy: 0.8688 - val_loss: 0.7351 - val_accuracy: 0.8189\n",
      "Epoch 79/200\n",
      "195/195 [==============================] - 12s 59ms/step - loss: 0.6517 - accuracy: 0.8699 - val_loss: 0.7800 - val_accuracy: 0.8057\n",
      "Epoch 80/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.6529 - accuracy: 0.8675 - val_loss: 0.9551 - val_accuracy: 0.7716\n",
      "Epoch 81/200\n",
      "195/195 [==============================] - 12s 59ms/step - loss: 0.6281 - accuracy: 0.8767 - val_loss: 0.8823 - val_accuracy: 0.7666\n",
      "Epoch 82/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.6299 - accuracy: 0.8755 - val_loss: 0.8047 - val_accuracy: 0.8030\n",
      "Epoch 83/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.6179 - accuracy: 0.8777 - val_loss: 0.8261 - val_accuracy: 0.7906\n",
      "Epoch 84/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.6131 - accuracy: 0.8776 - val_loss: 0.9149 - val_accuracy: 0.7620\n",
      "Epoch 85/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.5971 - accuracy: 0.8827 - val_loss: 0.7549 - val_accuracy: 0.8096\n",
      "Epoch 86/200\n",
      "195/195 [==============================] - 12s 59ms/step - loss: 0.5937 - accuracy: 0.8829 - val_loss: 0.8423 - val_accuracy: 0.7830\n",
      "Epoch 87/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.5809 - accuracy: 0.8846 - val_loss: 1.1213 - val_accuracy: 0.7293\n",
      "Epoch 88/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.5668 - accuracy: 0.8904 - val_loss: 0.6931 - val_accuracy: 0.8289\n",
      "Epoch 89/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.5606 - accuracy: 0.8917 - val_loss: 0.6466 - val_accuracy: 0.8410\n",
      "Epoch 90/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.5421 - accuracy: 0.8964 - val_loss: 0.6757 - val_accuracy: 0.8346\n",
      "Epoch 91/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.5388 - accuracy: 0.8945 - val_loss: 0.6434 - val_accuracy: 0.8394\n",
      "Epoch 92/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.5269 - accuracy: 0.9000 - val_loss: 0.6461 - val_accuracy: 0.8353\n",
      "Epoch 93/200\n",
      "195/195 [==============================] - 12s 59ms/step - loss: 0.5150 - accuracy: 0.9024 - val_loss: 0.6121 - val_accuracy: 0.8506\n",
      "Epoch 94/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.5028 - accuracy: 0.9056 - val_loss: 0.6663 - val_accuracy: 0.8328\n",
      "Epoch 95/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.4958 - accuracy: 0.9061 - val_loss: 0.7167 - val_accuracy: 0.8164\n",
      "Epoch 96/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.4845 - accuracy: 0.9101 - val_loss: 0.5958 - val_accuracy: 0.8474\n",
      "Epoch 97/200\n",
      "195/195 [==============================] - 12s 59ms/step - loss: 0.4619 - accuracy: 0.9167 - val_loss: 0.6915 - val_accuracy: 0.8158\n",
      "Epoch 98/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.4521 - accuracy: 0.9190 - val_loss: 0.6087 - val_accuracy: 0.8448\n",
      "Epoch 99/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.4392 - accuracy: 0.9234 - val_loss: 0.5532 - val_accuracy: 0.8648\n",
      "Epoch 100/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.4285 - accuracy: 0.9239 - val_loss: 0.5339 - val_accuracy: 0.8713\n",
      "Epoch 101/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.4101 - accuracy: 0.9316 - val_loss: 0.5421 - val_accuracy: 0.8697\n",
      "Epoch 102/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3997 - accuracy: 0.9343 - val_loss: 0.4831 - val_accuracy: 0.8837\n",
      "Epoch 103/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3864 - accuracy: 0.9378 - val_loss: 0.4537 - val_accuracy: 0.8938\n",
      "Epoch 104/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3701 - accuracy: 0.9426 - val_loss: 0.4505 - val_accuracy: 0.8924\n",
      "Epoch 105/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3566 - accuracy: 0.9477 - val_loss: 0.4269 - val_accuracy: 0.8994\n",
      "Epoch 106/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.3409 - accuracy: 0.9519 - val_loss: 0.4529 - val_accuracy: 0.8924\n",
      "Epoch 107/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3324 - accuracy: 0.9555 - val_loss: 0.4101 - val_accuracy: 0.9052\n",
      "Epoch 108/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.3196 - accuracy: 0.9594 - val_loss: 0.4084 - val_accuracy: 0.9066\n",
      "Epoch 109/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3143 - accuracy: 0.9608 - val_loss: 0.4086 - val_accuracy: 0.9077\n",
      "Epoch 110/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.3160 - accuracy: 0.9602 - val_loss: 0.4084 - val_accuracy: 0.9073\n",
      "Epoch 111/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3152 - accuracy: 0.9618 - val_loss: 0.4097 - val_accuracy: 0.9075\n",
      "Epoch 112/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.3157 - accuracy: 0.9620 - val_loss: 0.4089 - val_accuracy: 0.9075\n",
      "Epoch 113/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.3167 - accuracy: 0.9605 - val_loss: 0.4090 - val_accuracy: 0.9073\n",
      "Epoch 114/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - 12s 60ms/step - loss: 0.3162 - accuracy: 0.9606 - val_loss: 0.4085 - val_accuracy: 0.9076\n",
      "Epoch 115/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3157 - accuracy: 0.9609 - val_loss: 0.4091 - val_accuracy: 0.9078\n",
      "Epoch 116/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3155 - accuracy: 0.9605 - val_loss: 0.4087 - val_accuracy: 0.9078\n",
      "Epoch 117/200\n",
      "195/195 [==============================] - 12s 59ms/step - loss: 0.3168 - accuracy: 0.9604 - val_loss: 0.4087 - val_accuracy: 0.9071\n",
      "Epoch 118/200\n",
      "195/195 [==============================] - 12s 59ms/step - loss: 0.3159 - accuracy: 0.9610 - val_loss: 0.4088 - val_accuracy: 0.9074\n",
      "Epoch 119/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3172 - accuracy: 0.9595 - val_loss: 0.4091 - val_accuracy: 0.9078\n",
      "Epoch 120/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.3149 - accuracy: 0.9620 - val_loss: 0.4088 - val_accuracy: 0.9075\n",
      "Epoch 121/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3169 - accuracy: 0.9602 - val_loss: 0.4085 - val_accuracy: 0.9071\n",
      "Epoch 122/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3152 - accuracy: 0.9615 - val_loss: 0.4088 - val_accuracy: 0.9076\n",
      "Epoch 123/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3136 - accuracy: 0.9612 - val_loss: 0.4085 - val_accuracy: 0.9074\n",
      "Epoch 124/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.3164 - accuracy: 0.9608 - val_loss: 0.4086 - val_accuracy: 0.9077\n",
      "Epoch 125/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3156 - accuracy: 0.9609 - val_loss: 0.4092 - val_accuracy: 0.9073\n",
      "Epoch 126/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3155 - accuracy: 0.9615 - val_loss: 0.4088 - val_accuracy: 0.9076\n",
      "Epoch 127/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.3145 - accuracy: 0.9619 - val_loss: 0.4086 - val_accuracy: 0.9072\n",
      "Epoch 128/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3154 - accuracy: 0.9605 - val_loss: 0.4093 - val_accuracy: 0.9074\n",
      "Epoch 129/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.3155 - accuracy: 0.9611 - val_loss: 0.4085 - val_accuracy: 0.9074\n",
      "Epoch 130/200\n",
      "195/195 [==============================] - 12s 59ms/step - loss: 0.3132 - accuracy: 0.9621 - val_loss: 0.4088 - val_accuracy: 0.9072\n",
      "Epoch 131/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3166 - accuracy: 0.9604 - val_loss: 0.4091 - val_accuracy: 0.9075\n",
      "Epoch 132/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3172 - accuracy: 0.9599 - val_loss: 0.4094 - val_accuracy: 0.9072\n",
      "Epoch 133/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.3142 - accuracy: 0.9624 - val_loss: 0.4082 - val_accuracy: 0.9079\n",
      "Epoch 134/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3153 - accuracy: 0.9614 - val_loss: 0.4093 - val_accuracy: 0.9075\n",
      "Epoch 135/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.3155 - accuracy: 0.9610 - val_loss: 0.4092 - val_accuracy: 0.9076\n",
      "Epoch 136/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3175 - accuracy: 0.9607 - val_loss: 0.4091 - val_accuracy: 0.9075\n",
      "Epoch 137/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3179 - accuracy: 0.9596 - val_loss: 0.4090 - val_accuracy: 0.9075\n",
      "Epoch 138/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3147 - accuracy: 0.9615 - val_loss: 0.4091 - val_accuracy: 0.9077\n",
      "Epoch 139/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.3148 - accuracy: 0.9610 - val_loss: 0.4087 - val_accuracy: 0.9074\n",
      "Epoch 140/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3181 - accuracy: 0.9600 - val_loss: 0.4084 - val_accuracy: 0.9077\n",
      "Epoch 141/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3154 - accuracy: 0.9618 - val_loss: 0.4083 - val_accuracy: 0.9074\n",
      "Epoch 142/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3128 - accuracy: 0.9626 - val_loss: 0.4090 - val_accuracy: 0.9073\n",
      "Epoch 143/200\n",
      "195/195 [==============================] - 12s 59ms/step - loss: 0.3169 - accuracy: 0.9608 - val_loss: 0.4092 - val_accuracy: 0.9071\n",
      "Epoch 144/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3147 - accuracy: 0.9625 - val_loss: 0.4087 - val_accuracy: 0.9074\n",
      "Epoch 145/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3148 - accuracy: 0.9614 - val_loss: 0.4084 - val_accuracy: 0.9074\n",
      "Epoch 146/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.3153 - accuracy: 0.9614 - val_loss: 0.4087 - val_accuracy: 0.9074\n",
      "Epoch 147/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3143 - accuracy: 0.9618 - val_loss: 0.4089 - val_accuracy: 0.9070\n",
      "Epoch 148/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3148 - accuracy: 0.9619 - val_loss: 0.4089 - val_accuracy: 0.9073\n",
      "Epoch 149/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.3147 - accuracy: 0.9621 - val_loss: 0.4090 - val_accuracy: 0.9073\n",
      "Epoch 150/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3163 - accuracy: 0.9610 - val_loss: 0.4093 - val_accuracy: 0.9076\n",
      "Epoch 151/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.3151 - accuracy: 0.9613 - val_loss: 0.4089 - val_accuracy: 0.9075\n",
      "Epoch 152/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.3158 - accuracy: 0.9620 - val_loss: 0.4080 - val_accuracy: 0.9077\n",
      "Epoch 153/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.3139 - accuracy: 0.9614 - val_loss: 0.4086 - val_accuracy: 0.9076\n",
      "Epoch 154/200\n",
      "195/195 [==============================] - 11s 57ms/step - loss: 0.3163 - accuracy: 0.9613 - val_loss: 0.4087 - val_accuracy: 0.9075\n",
      "Epoch 155/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3169 - accuracy: 0.9614 - val_loss: 0.4092 - val_accuracy: 0.9074\n",
      "Epoch 156/200\n",
      "195/195 [==============================] - 12s 59ms/step - loss: 0.3173 - accuracy: 0.9603 - val_loss: 0.4083 - val_accuracy: 0.9075\n",
      "Epoch 157/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3164 - accuracy: 0.9618 - val_loss: 0.4093 - val_accuracy: 0.9076\n",
      "Epoch 158/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3143 - accuracy: 0.9619 - val_loss: 0.4087 - val_accuracy: 0.9076\n",
      "Epoch 159/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.3147 - accuracy: 0.9613 - val_loss: 0.4087 - val_accuracy: 0.9070\n",
      "Epoch 160/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.3164 - accuracy: 0.9605 - val_loss: 0.4080 - val_accuracy: 0.9077\n",
      "Epoch 161/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.3158 - accuracy: 0.9610 - val_loss: 0.4085 - val_accuracy: 0.9076\n",
      "Epoch 162/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.3133 - accuracy: 0.9623 - val_loss: 0.4089 - val_accuracy: 0.9075\n",
      "Epoch 163/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.3162 - accuracy: 0.9606 - val_loss: 0.4089 - val_accuracy: 0.9073\n",
      "Epoch 164/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.3137 - accuracy: 0.9618 - val_loss: 0.4094 - val_accuracy: 0.9071\n",
      "Epoch 165/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.3170 - accuracy: 0.9604 - val_loss: 0.4092 - val_accuracy: 0.9073\n",
      "Epoch 166/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.3144 - accuracy: 0.9622 - val_loss: 0.4081 - val_accuracy: 0.9076\n",
      "Epoch 167/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.3154 - accuracy: 0.9610 - val_loss: 0.4089 - val_accuracy: 0.9076\n",
      "Epoch 168/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.3177 - accuracy: 0.9601 - val_loss: 0.4090 - val_accuracy: 0.9077\n",
      "Epoch 169/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.3157 - accuracy: 0.9614 - val_loss: 0.4090 - val_accuracy: 0.9077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.3121 - accuracy: 0.9635 - val_loss: 0.4087 - val_accuracy: 0.9076\n",
      "Epoch 171/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.3148 - accuracy: 0.9612 - val_loss: 0.4089 - val_accuracy: 0.9072\n",
      "Epoch 172/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.3139 - accuracy: 0.9617 - val_loss: 0.4083 - val_accuracy: 0.9074\n",
      "Epoch 173/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.3150 - accuracy: 0.9617 - val_loss: 0.4087 - val_accuracy: 0.9078\n",
      "Epoch 174/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.3147 - accuracy: 0.9618 - val_loss: 0.4087 - val_accuracy: 0.9074\n",
      "Epoch 175/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.3129 - accuracy: 0.9627 - val_loss: 0.4088 - val_accuracy: 0.9073\n",
      "Epoch 176/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.3165 - accuracy: 0.9608 - val_loss: 0.4087 - val_accuracy: 0.9074\n",
      "Epoch 177/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.3161 - accuracy: 0.9601 - val_loss: 0.4084 - val_accuracy: 0.9075\n",
      "Epoch 178/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.3159 - accuracy: 0.9605 - val_loss: 0.4086 - val_accuracy: 0.9073\n",
      "Epoch 179/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.3157 - accuracy: 0.9605 - val_loss: 0.4087 - val_accuracy: 0.9074\n",
      "Epoch 180/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.3165 - accuracy: 0.9608 - val_loss: 0.4091 - val_accuracy: 0.9074\n",
      "Epoch 181/200\n",
      "195/195 [==============================] - 12s 62ms/step - loss: 0.3141 - accuracy: 0.9614 - val_loss: 0.4090 - val_accuracy: 0.9074\n",
      "Epoch 182/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.3167 - accuracy: 0.9602 - val_loss: 0.4087 - val_accuracy: 0.9077\n",
      "Epoch 183/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.3162 - accuracy: 0.9610 - val_loss: 0.4086 - val_accuracy: 0.9073\n",
      "Epoch 184/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.3141 - accuracy: 0.9619 - val_loss: 0.4084 - val_accuracy: 0.9075\n",
      "Epoch 185/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.3167 - accuracy: 0.9609 - val_loss: 0.4086 - val_accuracy: 0.9073\n",
      "Epoch 186/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.3137 - accuracy: 0.9624 - val_loss: 0.4092 - val_accuracy: 0.9072\n",
      "Epoch 187/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.3171 - accuracy: 0.9610 - val_loss: 0.4090 - val_accuracy: 0.9078\n",
      "Epoch 188/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.3166 - accuracy: 0.9612 - val_loss: 0.4085 - val_accuracy: 0.9079\n",
      "Epoch 189/200\n",
      "195/195 [==============================] - 12s 59ms/step - loss: 0.3136 - accuracy: 0.9616 - val_loss: 0.4086 - val_accuracy: 0.9074\n",
      "Epoch 190/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3150 - accuracy: 0.9620 - val_loss: 0.4092 - val_accuracy: 0.9071\n",
      "Epoch 191/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3145 - accuracy: 0.9613 - val_loss: 0.4092 - val_accuracy: 0.9077\n",
      "Epoch 192/200\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.3154 - accuracy: 0.9608 - val_loss: 0.4086 - val_accuracy: 0.9078\n",
      "Epoch 193/200\n",
      "195/195 [==============================] - 12s 59ms/step - loss: 0.3158 - accuracy: 0.9608 - val_loss: 0.4090 - val_accuracy: 0.9076\n",
      "Epoch 194/200\n",
      "195/195 [==============================] - 11s 57ms/step - loss: 0.3146 - accuracy: 0.9629 - val_loss: 0.4084 - val_accuracy: 0.9079\n",
      "Epoch 195/200\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.3160 - accuracy: 0.9607 - val_loss: 0.4091 - val_accuracy: 0.9073\n",
      "Epoch 196/200\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.3155 - accuracy: 0.9613 - val_loss: 0.4087 - val_accuracy: 0.9073\n",
      "Epoch 197/200\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.3158 - accuracy: 0.9609 - val_loss: 0.4092 - val_accuracy: 0.9076\n",
      "Epoch 198/200\n",
      "195/195 [==============================] - 12s 63ms/step - loss: 0.3158 - accuracy: 0.9612 - val_loss: 0.4090 - val_accuracy: 0.9072\n",
      "Epoch 199/200\n",
      "195/195 [==============================] - 12s 64ms/step - loss: 0.3154 - accuracy: 0.9614 - val_loss: 0.4085 - val_accuracy: 0.9072\n",
      "Epoch 200/200\n",
      "195/195 [==============================] - 12s 63ms/step - loss: 0.3165 - accuracy: 0.9607 - val_loss: 0.4093 - val_accuracy: 0.9074\n",
      "10000/10000 [==============================] - 2s 211us/step\n",
      "Test loss: 0.4092515065670013\n",
      "Test accuracy: 0.9074000120162964\n"
     ]
    }
   ],
   "source": [
    "model, callbacks, score = evaluate_training(False, data_augmentation, 0.01, 'clrs', batch_size, 200, 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(callbacks[-1].learning_rates, label='learning rate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(callbacks[-1].accs, label='train acuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(callbacks[-1].losses, label='losses')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training like NAS Bench 101 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (40000, 32, 32, 3)\n",
      "y_train shape: (40000, 10)\n",
      "40000 train samples\n",
      "10000 test samples\n",
      "10000 validation samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "            #featurewise_center=True,\n",
    "            width_shift_range=4,\n",
    "            height_shift_range=4,\n",
    "            fill_mode='constant',\n",
    "            cval=0,\n",
    "            horizontal_flip=True,\n",
    "            rotation_range=30)\n",
    "\n",
    "        # Compute quantities required for featurewise normalization\n",
    "        # (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.preprocessing.image.NumpyArrayIterator'>\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAACECAYAAACJbXCEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO19aYxc2XXeue/V3tX73mySzaVnRHJmyJnhzGhs7dJYsuRAdmIDshNDiRUIBhLARmLAgn8lQILYf5wNQeKxLUixFSsCZMCDxJA8Hs9E1jZDakiKw+Fwa27N7mYv1Uvty3s3P6r4vu8Wq7qbZLGH1X0/gODp6ldvO/fefud753xHaa3FwsLCwqL94LzfJ2BhYWFhcX+wC7iFhYVFm8Iu4BYWFhZtCruAW1hYWLQp7AJuYWFh0aawC7iFhYVFm+KBFnCl1GeUUheUUpeVUl9p1UlZvL+wft2+sL7dXlD3mweulHJF5KKIvCQi0yJyQkR+VWv9butOz2KrYf26fWF9u/0QeoDvPi8il7XWUyIiSqlvisjnRaTpYHAdR4dD1UP6xh8OvZEpss4fGqVUw8817UGJMn5zB4mOjsCOxSIND2fuv/GxREQc2k4pBDeeV6H9YseOSwGQcgPTdWH7nk/b4/P6M/HpGOI4/Iu7jr2yuia5fL7ZhdyzXx1Hade5O5hr5rIm7np0wdfR5Nz1JjZSuvEYFLXeQ1SzsXf3dzzfF9/X693de/JtPBbW3Z3Ruw4dC4VxTBp3hXwxsF3aRkQkHMbYLRVL+AUNBieEbXgu8dgqFguBrf3Gc4k/L3sY/yKmnzoTCRyD5panMef4ZjpEWCheW8JYRkN03Q7tR0RE08+VckUagc+v5GOb6bncotZ6sH77B1nAd4nITfp5WkReWO8L4VBIxgeq51ColIPPfb5hdPN9uvm+bw5YXgQct8kCzoslfUELjvf8C8cD++DkPjoetnHdKI5bNzl5cY9GYoEdDuM7KytLgV2uYJAnO7uwn1AysHt6egI7k8lg++5e49ghuo70Ko7hRuLYbyEd2MVS9X7+8Z9/Q9bBPfvVdRzpSSbu+pzvId8nhyak09JCYF4U/eabbQJK0x8kWhM17denhde06Vp9TLGwR4uawiKmHcyF+u9rzVMUi4wSvr7qsZfT2QZXYuCefNvdGZVf/0dP1s4Rx/7A8HBgp5cXA/vcu1OB3dszZOxrdKQ/sG/cuIEzD+H6kn3dgR0P42GqK4m5dPnihcCulHDfOmgxLhex8N1eXjXOoyyYfx87/nRgd3bj2Gv0RyJKvo8pnJPr4Bihob7AHujfhfOuYD8iIqVyLrAXbmO+8lgtKVzTrTVs86/+4MR1aYAHWcAbrZp3TUel1JdF5MsiIqG6J0iLRxL37Fen7R6pdyw29C37tTMZabC5xaOEB1nAp0VkN/08LiIz9RtprV8WkZdFRDqTHXp8z4iIiFyfxoPA6hqeHEJMJZCt1nlU83WTpy36ikdP40YYRU/B3Z34S1oq4y+hS0/THUS5VHeGfd24iWtK0BPB8Cj+KnsUFimH/qKH8QTOkUM4guN5dZe5vHQrsLP0lF+gJxO3hCf4sledv+USnkIa4J79Gg65205QRysKcY2nbtqG10P+I0ZP40pxCG+QXvdzVk3sTWND37JfRwaTWmqRSNTFWPWLuKbVFMZXIomn2FAUc0ZE5OY0DhMOYV+dA5hza8V8YLsuopW51EpgF+g+RyI4Rncc88ejp+Nd44gWRESuTePp3y8gChrZhXM/OI5IfGkxFdipFD01OzjXxduIPMpED0XpqV5EZG0V+1pJLQd23sd8LSew5uVDGz8YPUgWygkRmVRK7VNKRUTkCyLyygPsz+LRgPXr9oX17TbDfT+Ba60rSql/KSLflSo591Wt9bmWnZnF+wLr1+0L69vthwehUERr/dci8teb3T4WjcrBg/tFRGQth5dry6sIkYzAkF5O1gcTxgtKh95Yu42DCo9eiCqiZmIxvPAL0cvKbBah0Gp2LbDjSVAuIuYL1AK9WU7QeUQSOIZXQegciYJmiSUQbi0u4sVQItnZ8FgiIjMruG8nT5zEuZfpGBSeSe36cvl1KZR79ut2hFaNKTct/B7H5S8EpuL0GyM5hegUI/OkbnQ3TSR5cKbqXnxb8TxZWKmG+q6Pa+33KSskz/QSlpP5ZVAEIiJuBfNpfHgksD0P17SWxUu+HFERDtFN3f14Oep6GNulLCiNvg7QKZW6VKjuTlCSlTLmgV/EsZMRJHtkIzj2gccwRzu78VL2jZNnAnvm5tXADi2DHhIRWctgvs7Mg07pG8P9WMrhOlRXHV3bALYS08LCwqJNYRdwCwsLizbFA1Eo9wrHcSVZe1M9OICwIZtFKFPMIyMlTDmi61WMKsV0CiXbc8YAfd+nVIIwJd6Hw8jjTiap8CCL0MevO494FPTIkSeeoN8g9NIUEkfpbXmUKJQyUR2aMm5KPuWNU363iEhnAvsKRUC1JLsRukXo3rjh6vfd67PSSmi9vn+2Cmbh1sZQRr513flz7jdNEyUYL1oThWIUlZAtRN05jesadP1z1COSlulpLZkaLagoK2vm9nxgFzP4fJby0J2omTK8dwC1DR0JUANTsxiLa0V8v6OLxjNlgeXzoFbWqPahh1K0QpSVlsmaOfZdfaBAQ1Rsl1rFsfuH8TkX+MSimIv7JrB+nTiPeXnlPWS5xE0GRVaLoGJHJsZh70Zi0Lkf/CCw+2JYj5rBPoFbWFhYtCnsAm5hYWHRpthSCsV1XensroZSY2MobmHdkLlZFMNQcokUWT9BTKrEdVhDAdt4mnRAKCz1KwhfY3GEZz39CPNYWqSrQm+1K2ZI5tGb7DSVvQ8OjtBWlKFA2SluCOdRKFBZMGW6ePSmPRwxQ+tIBAURToiyaTpQch8L4zuh2jaO23q3N6JQmmnUtPJYfAyjTMaQQti4Atip03LhoikuKHM0VyfiPvIt9XzKJKAiIG3QKevQN9LsfIl22QLKyvd9year1xINYZ54dLFh0g9KUNm/GzPHWJnu4WoaGR+K9E/iEVCKFSp4K2Rw39xw42fOIt2PEM3pUaJiRETCcTpfB9sNjT2O8+gYDWymcjKF84G9vLAQ2L0dmHtdZNd7iAudwpRddvq9i4GdzqH8PpLZUBrBPoFbWFhYtCvsAm5hYWHRpthSCkULwgoOa1lfJEYaCixVmUiYGRgO0S4hB5kB2RzCDi4MMFNS8N0sKY9lSYshHgedEqVsGCmaCmNcIBQKIwRULt4g+0TllIhyUSG6BpbPpHBVEU1SKqH4SUQkl8f1VahAyC/gOkKkOxG6U0iyrnzp/WEjuqSVdEo93UFHgUmXyNsztcIFYPV5Ky7dI4+KULjwJMz6ILSN1hgjsQ6MnRId2/M4W8q8HlNdqrGv6lUxHwq0FqdSnYORLoT/JaIs4x2gAnb1Y47mKzT3RKRMhTJUZya9vZhnyQSuaWHmdmAPEN3QR9sv5zDHSjnsn4v54p45X0NFUJIjE/sDe5TsBF2fJqnYVBn34OZN6J/sJurVOYYxsXgL2WsiIjO3kYXy7lso1lv1MXYGRlGoVCyYtHEj2CdwCwsLizaFXcAtLCws2hR2AbewsLBoU2wpB66UknC4ylczB87pcDGqPiqXmVc0ucBmrclKxEkXS+DQw1QxGSKO2aXUpdklcFb5Ijp5hEM4p+eeQwcfEZEocfaXLl0K7Bz9beRuIWGFlMR8Bpx2jFKzmH/N0DbFoplWVCrh/nDVaqIXaYQdSbwfuMNXhkKtdbtS98aBm9vWV0BufDzmtM0KW2xjXiOlkBo8tNDnZuutEPHjnR24h4ko3tdkqfIwEcd4/vjHfiawDz91OLC/9r/+b2BPXYU+tuPUN05odn820duthQg5rnTXqia5zdgy8c0h6h4Vp9TB/qiZvjfSh9Q8P4t7fXORulXlwE/vH9oT2E/shj53JIL7PHULTWqKUfjCz6JSVFVMHrmvH/rgSlNXnQilLdK7pu//AEJVV+h4B0dwHo9PYr75IUohNV/biSf4XTlPXYOWkH7cOQiRrL5B8x42gn0Ct7CwsGhT2AXcwsLCok2xpRSKiDbS7oKToGqsMDUz5c/rv8ehcCelOOUoTShNwjehMELfZBJh37UZVFSVZ6HRKy7in+df+BC+O4QKUhGRCxfQZHXV49JRChPn0fpsdGggsAd6YOdXcGyfGq9GqNKtnqXgDthMJaTTCMl8Elwq1VLCKl7jjtgPgvumUNbtLNaYTzEbVPOuGnf95uP1Er0UJupueMhswrtnHEpEhx+D8FBmjSpvV5EWlojj2AMDuOfHjiI97eg5iJ1dJQqlQcfR+g/u+vhhVbkyXMeRnkQ1jE8M4n4Ui0RD8VDSmG+7KLVORGQggvkwdQPV1hOjE4HdNwpRpzClayaIvikT1TUxBpplfg4iUlmiaLqTpqZ2Tz+u4+oMzuNmCvNvcADryZkL7wb2xRvQON+774OBPTx5JLDdOewznjAbfZcof7JMqabXpkH/pEgnvLfHbEvXCPYJ3MLCwqJNYRdwCwsLizbFFlMoCPWZAomEQQVEIjglZk1isbpwgiLIRBJ0hxvBvnxh4ShkkhRKJCJFIeDRDz4X2E8//+HAHqGu8md+irfSIiLzq8hW6e6jrvYetYSiCzl5Bi0IX/oIqJnuAbx9XkxxhgLplcfN19ohyuQpFhHal1287dZ0n4uFatjmV+6msR4USq+/T6NykFuG6VDddsQnUAsy1qMql6iqjyiznk6MkbFR0FPHnzsW2IcOQbRodASCY8srZguwtUWEwi8cAw1y7RayJmYX4JsQVdV6ZWQxFLK4/ywIpVhLXJviVUwR+QZTgn0pqiZuQae1JlBB1XKY9Msr5Mv5RWR8DO8dC+zrVy8be7qRAVW5hzrFf/xTyOo69x7GvaKKao9EwCixTGI8H0g3P0x67Lm6asa5FOZrKg0/3V7F+VUE1/GBI1ShOQjK7JkXX8D2NIT3HZwM7BmauyIiF95BFkucKqSfngRFVyHWZfYSqNdmsE/gFhYWFm0Ku4BbWFhYtCm2nEK5o9vDOsA+6R9rB2+NNWkCV3wzzORCIMdBcUx3L20XRmZBLIZtwiEcY2QY4VJ/D4oNdAVh4u1ZhFfnfvqecR4zMwj7Rsbwfe6GHaXwWpMG8f/74YnA/uzPfQTXE4Me+Cy1nOqK1yX2x/BzJInrc4ltikXh4nitpVpzMaj7BcuU3SPuyqYw80ruoFRCiDzYD7rp6JNPBvaRx+HL0RFkEnDmSYHEyOZmIUh069a0cRbTl5FdVE5DeOhTn/18YK9kfxrYCWoTVsxgDA4Ogn7btQt2hESSKp55D7ggiSkl4w5vQQc7X/uSr+nwh1KgmFaoTVmcaIIIFT9lS6ZuvkPzdfdB+O/8+e8H9ltvgnZ5ZvIDOA8aB04U2S3pHArv5lPwX2UZ/kp0m9kwyyHQJt2k2Z8coe08ZHF99nMfC+xCBWOnpw8U3R3NdBGR27dwDWdOnjWOPTuN8+2MYnwencS4zZIo3Q9nQNc1w4YzWSn1VaXUvFLqHfqsTyn1qlLqUu3/3vX2YfHowfp1+8L6dudgM49iXxORz9R99hUReU1rPSkir9V+tmgvfE2sX7crvibWtzsCG1IoWuvvKaUm6j7+vIh8rGZ/XUTeEJHf3fBoSolbo04ilOQeD4MyyOTwN6VAegERohVERHr7iR4h+qCbuoaLC9uhYgBdYfoGsejUFMKr6zdOYzf0Z66UNmPXzBJC+xUXb85jceo4TwUOvga/kaY2aq9/DwUDHnWoZ31zXTH1heOkn7LnsWcCu1Ci0HL+WmBXytlg/y316z3CUPqoz17hFmT0u6NHDwX2Z37u44HNBTixKLbPZhDyF1irm7S3OUPhBz8yw11NmTrpHGizD//8PwzsqesoHtlN9MjkBApS8llkLnR1w/cV0qn2tfkcpaj9mG6icb4eWuVbxwlJsrNKLUSIAkkSpRGK4NzLGuN2gDQ9RETKBVzHlZvQ+p4me2aGsneosO3I49CTGRiDvzOUAZYnfZYSZSmVwIaIiNmyzFvCdoMjuLljA4OBHVGYx5Eorq+SwXkvz8K+eBq02rVrJgWSL1GRokv6/TTOs8u4B6q0cbHW/ZKhw1rrWRGR2v9DG2xv0R6wft2+sL7dhnjoLzGVUl8WkS+LiHR3d22wtUW7gP3qbEFZt8XWgP2a7Ni4lNvi/cX9LuC3lVKjWutZpdSoiMw321Br/bKIvCwiMr5rl45EqyFJoYgwfy2FUEPTW+2eYbwlLtRpM6Y0fvbprX+RQt+IC7srxroqCJ3TGdglyjzxqHO9roDGqM/giMVJz4TUXotZxG7NOpO7RPFcuoQ350Vq7VaugKLRHmwRkUqB2kjReblhKt4p4zwqqloYUKnbD+G+/BoOOfeWE2G0jzMXf24j9dxzyET4J/8Y1EU4TH4qI6zlcL5Qxj28eAWZAdevojjiox/9RGDHEqZmRoY6p/thUHTTcxirxRJ8ee3atcDeM4ZxW6axZircUku1uyR16WcuelIPlD20Kd+yX0eHunV3TYK4QNo+0QjOaZU4igFqMbecMtv/Lczhvs8lMO7XUrjPXoU1T3CMtTwVQ+UwdjN50CH5Ames0X5oHIiY3esXUtz2Dd9/8gCyQmZvoBN9qYJrYIrNVSgS7KKsl/EJM2us4GPszMwiey1PCwe73omZmXeNcL8j4hUR+WLN/qKI/NV97sfi0YL16/aF9e02xGbSCP9CRH4kIo8rpaaVUl8Skd8XkZeUUpdE5KXazxZtBOvX7Qvr252DzWSh/GqTX33yXg+mRcSrFeTk6a308gp1pOhEgnvfMN6zXLxJUq8iMreEnyOCMDpJHe4PTk4EdtzBNj7FKSnqyvHeBRQD5ApEoXAUW3dNPmW3cAWG9jjzhI7tG/qbgVUi+dOKh+37SZazM2lykqk8MhzSqwgnSxQ2lny8IdeRVG3/lZb6tRmaSZ5q3bhAR0Rk717QD7/xG78W2JEQQtbLl5EV0kUZSGUPIXwiieyBRRorR59Bts7gMAqvBgfNd3pOGBkOE49B32I1jXtbJPpGSH9jbmEusHfvh87Fk08hk2ZsF/RApqfNsa1k49B5ve48LfOt9sWrjaU48T9FolOIpZSpK9CPKaYwNkVEYmFcU2oN8z2Xwbjt64bPfIEvF1KgW3MenjnzJVAP+Qp11yHNmUydFkrPIBW8RTD2xjqIhk1BryhDXYbCRGn0D0AjpW/wYGA/9hT0kE68edE49t+8cSqwvRLmgKvoWqkrWKayrtayiNhSegsLC4u2hV3ALSwsLNoUW6qForVIuRZydSSR6J8o8FYIG9IreGvrVPiNsUjUQ4gWqyAEHehFWHz2rVexr2XomWSz/IYc4VbfAMJdP4/Y0Fesu2L+zdMUynIg6zDtYsiiwnYoeSPKsSgl9sddhJtLs9eMY2cyCPOHh3DuIZLOTWUQZi5l79ynVqf9qYAuYdrE8yhMpGoolhJO1KWqfeELv4jtqNjlzHkU2iSiyGLI53F/hqm7yyJpdxw4gBD3+eeeD+x3zoKK6ew0U1wvX0PGwewcxk5qEXZfNzKhJg/i2A4xIAXSBPnAk0cD+9lnnwrs6ZuvG8duVrvDXZc08XoPK4tTax1kbFGCj3TGqUCOOmjdLmA8epSZISKSK4F6ujVHmSEkU+uRv8sl+K9IrGMPFbm5RKsZ0sMkT72aMhuBlxXWiu4kHBUqIQvMpYK5tQVQuvPzoF8+8mlQYJE41rLLl0GfJXtAp4iI/NKvQEfn9IlA5UBmbl7DMVZwvr0jWMtEzKyeO7BP4BYWFhZtCruAW1hYWLQptpRCqVQqMr9QDVVYAjYWY80SvDUukIxnTxShmojI6ATClsFe6FCcPos3yDNzyCo59Phjgb2yDGrm1CloFwyMIYH/c5+E3oamrBButFy9DqehzXE0f8yFPJy1H1FwRSaDcOn11/8usLsSZljaGQf98NyzT+P7pL9x/jJCxlS6ctdxW4VGGSf8GYf//PknP/Gzxncem4SOyK1bkHvt7UZRRJRC5HwWYbciyeH0MkLRfmoefeZt0rihwpNnnwa9IWI2pJ1fxHiJhXEdYfKrX8EYOf4iOi31kPStGwLlUqLkCKZDREQcLvJRjTsdcYejh9Xg2PN9Wc1W6Y6Cg/s8OoD7mSQ9ngzRYfv3Y76JiPz0bXSyypbhJ9Y7cmhuOIp0VQaoWXUe1EqEpkMyjntQJsngjrpimHQuT7/D+Q70JciGn069g/O4NUeytgkU+Bx9Gvt54/WfBPaFS5eMY0/sPxDYg32g7Mo5CEPO3MZYe+YZdAj77glk+DDsE7iFhYVFm8Iu4BYWFhZtii2lUFzXld7eaihcou6kGUrs5+yNJBVp1EeJ47vxFnh0BGH3j08gRB6l4ozUIqiEnj68We6ggo9SAdTDgX0oKNFU7MNFKCJmRoVPb9R9n7JY6DteBX8ziyWSFKU38FevowBgeRUh1eQkGvKKmNk05y+BCro9j3BrfpE6ldTOo9UMipLGXX5cF06rUKFFPI7Y98UXnzW+s5xC4dHkwX2BXaSuJ9zQd2kRGQPLSxhHTx0BpbREhTypBWx//NknAnt11XzLHyOqLEL+G+1HuOv7VDBFYXuCxm0PyR5fvgj52bOnkYUQjZjUmOcxPdKYQnEMeqrhJg8Mxw1JRy2TopjG3Li9BBrDISrn1izGWlc/Z1CI7D0I+kCRNPT8HHyTXUDmTyyE8bK8hDkQ7yd6icZXnLpeuWX4q7fLpF6ZquztBVXS3UsdnAawtiynMR7fmYL/1sqQkilnceyuGPYz0GVK6l4nSsWQVCbdlxEqXvzohz8a2P/hj/5SGsE+gVtYWFi0KewCbmFhYdGm2FIKJRIOy+7RaqaH5yE0zGcQnr13Ac1kl0hmtlAwqn1kz569gR3vQMg6P48mwIMDCOMKeege+GWELNEIQqx8FuH1+XffDmxN2hSlkqmtwOdVoDB/bQ37WlmBzZRLkXRAeD9zswjPYjFQPI4yY2U+htBb+1gM1zRIIbyuFdbkFq5LS6FUUKgTpsKOcpnPCTTBs88+Q99lbRiRAulbZIgiKpGM6Noq7jPrSEQoO+X0T5D18MMf/iiwP/3plwL71E+QMXDxIsadiMjUFO5RLIIsg7CP86OhI7v3oJCqRHQRa3H80f/4amCn05Q9c9c0NLoaNzKNLJT1PnsQaNFSrhWV5Uj2xXFwvo4iv6zBfu8sMohERA4fQfPp4X7cn4lhUFKnTmC+R0iCOU9zN0cdu5KkrxKla49Qdkmk7tbuHkbGWl8n1ofde3GBS3k49vW3Ifu6SnpDHQmc39R1jJVIBPNVa7Oxc7ILv8sVcU0L85jHhw4dC+w9Y1jjmsE+gVtYWFi0KewCbmFhYdGmsAu4hYWFRZtiSzlwpZSEaxWKLnFWhx+HTvLQMFJ43jpxIrBf/ztUJIqITE2BY5s4AK6oUAC3/uQT4JPCIeKmSJPb6D6eRnrUK698O7C9Mv7OcTpc9Wd83/PwO26Fxnle0Sjxc1RdytxxTzc4/TAJUy0vQUhJRGRsBPx2dw++E6cKzQRxz+Fa5eH/nPszaSWUiNzRqoqS6pFLPOaeveAbP/QzR/BdZeqBc1rnuXPgpbmyskjVe329qArct28isC/T+EitwK9/+xqEo4qFxj4SEenthRBRZxc05l3iV5P0ef8Q0k65zdt1auf23kWIZ/nUwkvqUjAVnYtDz1gepTP6XGCojf9aBq9ckZXbVXGmELWVUxG8a8hRZWM6Db6/XDQ1zqM8Jos40/5OXMihD2CMTF+/FtiaUiZLdOE+vf/w+b0BabPzeYuI7J9Am75CBvd2cAzX9OO/vRLYU9PUMo70yoX0yrm6M0ICbiHXrAItU/prKkut4ag13/D4nsD2N9Gp0D6BW1hYWLQp7AJuYWFh0abYWjErryKp1WrKzNoaUgdTywhxOdzNU1reEHWoFzFTzFZXEeYUiyQ+M4PKqccmETqdOYUUM69CFXSUFxYJIaSKJlCxFa0T1YrH8btEB0KsJNudTG/g8xhTKHTsUIhsN0qfm+6KRulno1SVQkiXU76qIV0kYl7DgyIUDsnQUJXKSHaAshmhFLEnn0RbskQcIWO5bFIonUlQIsOU8nX2FCi0kAPfdCQgcrWyinSsJ46iyrJEQlM3r6Pab5jGVCJmdqXv6sHPkRiec4YGcX4/+5EPB/b+g2ixNXUJVIlLQlXxTqLP4thniCgJERHu1MZCVYUC5kOFqoPRIrC1JIqrlCRrtBuH83nS9vY9fM5ju1gwdbhnZq4GdgemjFBBtuwaB32aXYGfFPmvIwyKQvmgbHic56id4bUps7XbR38O1cyhQfjg9jKoru+8CgqlTFnDMWqv5hPttbSGa/U11qKhwUHj2ELpqNNzWL8cRfRSF8bXrQVUgTaDfQK3sLCwaFPYBdzCwsKiTbHFWSiOuHdoAxKfCVGLLBY6GhiAGEx3N0RiRERK1KqqVG5sC73pn5lBWLRnN7I3nn8Ora26utABvoMokHicqJU6+iFE1X8hahvm8vWFGt9mzmjRPrcfC5ON4+m6TAmP2kt53MGa9aVJ8OdOZ7hGwlMPAkcpidWya0ZGEQY/cQi0yegIfJlMwse35hBai4g4dA/5NHtJgGxuBpWqTCusEoWSySGsHST96l7SBt83AbGsRNzMVrgjuiYismcvdOJHRkG79Aziml77zl8Hdoju+d6DaOcWjSAroa8f11Myi4wll0fc7tJN8Hy0FdRl0levZaeoFlMolXJFFuerlKZD2TeaMkpu3kRmlF/AeNSeSY299x7m39goqIUc7XeUxKX6ujEX+8K4QV0u5oxXxv3ooDmaSpHgVdhslTcwhgyo+Tloev/wR8haunID40iRuh5XFmfyuNc5om15Tjt1ImVDY6AEcznajrJpCkX4vlQyhfMaYcOZrJTarZR6XSl1Xil1Tin1W7XP+5RSryqlLtX+791oXxaPDqxftyesX3cWNvMoVhGRf621PiQiHxSRf6GUOiwiXxGR17TWkyLyWu1ni/aB9ev2hPXrDsKGFIrWelZEZmt2Wil1XunpeeQAABUlSURBVER2icjnReRjtc2+LiJviMjvrnuwUEgGai2mursQ2rB2dqXM4QjZXp0uMkWLnIDhUvJ8iDWCKRTloplwmLI0qA2aUo1D+XqwOBWfh/n9xp3auXUWSxUxxWGyJqZYEVMtvkvnQdtp86Rq/6mW+tXzPFmuZQ8Vi3gL/9hBFCWsrKBjOV9UX6/5pj5F3eTXVpBBMDiEwpp8FrTJ0hLa7hVJkzuTpWyAIdA6x45BSGt8F3Tk/bpoNU6CYF0J0CsVFqf6z/81sG9cBxX0yU+hHV88gWyW8T0oVOntx9jO58yxzYVKvX2gEk6fhsBaZhX0wR1xqVS6JLpSaZlftXJEu1UagKaixKh1WkccVNMqtbcTbS4tly+RfnYZ13v4AHzAomG6g3S7k6BWxqhgrUT0WZrom/lF0Dcdoyb1qsK4nxeuIlvote+haMyneRWJ4FxZyK6isU2F5quiayj4JqWVWsYcKOSwr4E+0HrZNLaJRzdmuO+JDFVKTYjI0yLypogM1xaBO4vBUJPvfFkpdVIpdTKdXmu0icX7jAf1a8XbmKuz2Ho8qF+L5cbNJCweHWx6AVdKJUXk2yLy21rrTa/EWuuXtdbHtdbHOzu7Nv6CxZaiFX7ll7cWjwZa4ddo2N34CxbvKzaVhaKUCkt1MHxDa32nt89tpdSo1npWKTUqIvPN91CF4yiJ17Q5YqQJwlE+0yncvsz3m79hV9L4CdAgHJp07uZCCaZfXLcxjVEumxq//J1m2R3NOrJHo3hLvZnvenU0km9krvB5cEsumoQ1yubOsVrlV8/zZHV1rXZOoB74jfzKCgoXCnncw3jSvO5cHmuNpif7/n5QKHvHkT3y6t+gwCeXReYJZxHlc6Abbt5E26+FBehPe3Xjq7eLtD88ynIiDfepS8hc6KDMmvk5ZGZMZHDdXb3YZzSO483N4jxERPaNQ1u8i3Rx3DA0ta9cQJHaynI1a+KO31vlV1+L5CvV8xzsBfXAVGMXafGrMmirbBr3XMRsGxZPYIx0dGEJqihqrRjB2FnIYftr87ifw0OgHi5fwz1/9yp8dHy3Ob48Kv5ZowLA5RV8J0xZNjEawz5lfWkBhRKm7KIKrV8Lq0QbishiCsdjSjcRx76WU3DL0uKcbITNZKEoEflTETmvtf5D+tUrIvLFmv1FEfmrDY9m8cjA+nV7wvp1Z2EzT+A/KyK/LiJnlVJ3Ogb/noj8voh8Syn1JRG5ISK/8nBO0eIhwfp1e8L6dQdhM1ko35f69Afgk/dyMK11QEFw+M/0QROmw6Ae6r9jZnM0/w6fR6NteJ9M2fD2bp1E5Gaoj2bf5wKfZtsro/u4Geab9001sen8FNNCrfXrHXqHKTA+v2yG2s2tgOoYds3O3dkMtb4jTYvJAygKOvKBo4H9rW++QvsFFfH0MdANH3zhhcA++w6KNyo0Bit1LNx33zkX2EcPQz/j87/4DwL753/hFwKbb/PsrZuBvUw6Py51uk9nIbcaiZvT0A2T/13cz4OPgzo6sB/34/z56jXduH1Ccvm1lvnVdR3p6KpmgDiU0VWm7I/OJKit1CK0O1zHpPu6OnGN2TXQK74P+qAsoCjWqNP77BIu5+wFZB2FwxhHC0vIgAmRX1dXsL2IyPkzbwR2xAdV0teNTJfby0Td0nuAcIh0aaiQUJN8rYpgm9WcWcxUzJLULM3R1SzGSCwBKq5/oE5LpQHs2ycLCwuLNoVdwC0sLCzaFFuqhaK1vqujTT0a1J3UbDMq5AwVJhaaUQkMpjGYlqgYb5wp3CGqo36f/DN/xyzwaZzp4tdXjzQ4J7bX0zBpRrv4fJ/WyeR5IChkQPQPIKQuFhDWnj79U2xORRCl8gFjVxP7oDvi+wjVy6Q7Go0h3N03Aa2RyX0ooNlL+iUczO8/ANnXt36CwpiRXePCePFDHwrs0UFUnb974WJgO9zynIYFZwuFqSDo+jSoFe5e3tHJnV5EiqQj4mcRqu/dBx2PsWEUSU0eqt7D7/8Y59YKhMOujNckgecXoQ9SprE2OIhCmcwFXHfSrdMBoU70uSIolOtTkI119qAoaHEex7g1T9kp1KEqV8J8KFVo/JPccIoyP0REfvzWdwP78OThwN47hmKv2Tlk+Bw+AhnqqZuQnM2RnG+ZOgyFheZ32Zyv5SJrNlGxVi/Sq/sHSGun21IoFhYWFtsWdgG3sLCwaFNsKYUiooKsCM5W4JCTWQIz46OODuGMjHvMNmHqgmkTjzNjmmR/3CXpWq/Rcuf7nCVjZLdsXHbejPqpp1CaHZs/N26T83Aq65QocWsdUbgLyQB1r1kl7Y58lkNJ836OjYH6mL6FopvZWyhqmJ9Fsc+Rw+i8M9KPUJQ7LX3r20h5/qVf/uXA/tCHQZNcpQIfEZEu6qJ07PjxwL5Omif/5b/9SWDfuInPf+d3fiuwwyWWSgZVssiURMks5OnpBS2hFHz5NmmhXOrE8ZK1YhrWgmkFfK8iubXquWmiDHIl6s5TIXqDCtOK1E1LRGR8BPdzYRkZOKlFUCijg/B9jAqE8gXObqGslZKZ5RGcXxFzbHHFnG/Ts9cDe+8uqAkcnAD1V6D99o1iTE3NUBNlmlglkrXNr+G7yjc7LVHPc1EK3+/owriIRKGdk81SO6AmsE/gFhYWFm0Ku4BbWFhYtCm2mEIREV3T4eCPOFOCu8noZrKqJh3ga6JgjP1Shsk9Uh0Mpj3qKRT+OUzdeUxJ2I2zSprRJs0KjdY7RzMDhiR13YcvTsQdi/h4ExN7A/vv3zgR2KtrZqh9dQpURjoDqqSfOuScOnU2sKMhfH41hOv+1EugR/75l/5ZYH//zTcDe/d+ZKQUi2Y4XqBOMdOzs4F95CiKiCYOIoPmO9/5TmBfuYYwPdKBMdHdjWyWdBp0x9KS2byWm2N3dOB+5gsI1ZfXWGej6uN8iykURymJR6v7DncgtC+ncK8y1Jy8jzKQ5q7NGPsqljEWEiSxW9a4Dm52fX0WxS3cVco1ZJfh73gCSxmPqdsLpnbR6BD88d75dwN79xgKyl58DjTg9Vlor/RQA/NsiYtyqHCI5JSLOfPYvk96RTRWHSrwYvnnZmsWwz6BW1hYWLQp7AJuYWFh0abYegrlTtmN4kKZZgIoRFfUURdcoOJ7jYuDmtESTHUY+2xClWy2CXAz2mQz2zfLktmsFgrDvO5mshitgxKEtt2dyKCIUneSib0olKn8DK7j+g2TPvjRm8i0GB4GPTK/CE2LZALSprMzKLrIppHZceKddwL7N38TFMoKhfwr51Bc5DnmfWJdjoVVUB/d85AtHehD2P3Zz0EX5eTbJwN7bhH0CzeuHhrCdyt1jRNYX90h2dJO7gzkgWLIF6qUxmbG3L1AOUoiNdnnfA5URw9liJQ0ZWaQNLC/C4UxIiKXL8NPe3bhfvZ101z04ANuJlGqIBsjHMLxdu/BMZZW4PtIAvvJrZh+nZvBWtEVAeW0HMK48Mo4XlcM1/riM8haWU2D9rg5g6yaSxnQXJW66Vmk7B2+VpagzeZZun0Ta8iGW1hYWFhYPJKwC7iFhYVFm8Iu4BYWFhZtii3lwH3fk3yhmpbjkdZzRwfSc1QTnW+/nt8jYSbucm22LAMH20x7m6sWzZZqlPKzjkBWM968magWV35utuKy0bnWn2Mz3pz3Vd82rFVQSkm0dn/HR9BlPLVIKWZ9SDH7yEch2HP6zBVjX3///R8GthMG53j23KXA9jT4VJ9adXVSx/LVRfCSf/LnfxHY4+PY594x8PK+a96b6TmkwU1GIJhVoHTD6WlUh/YPoOr0AIkklR16l5LG9lqDYx8dgd64iMjiPPjm1AI0rwukyxRxMWc6eqrvCkJua6dzNleUt07VKj4VzjfRibkbTeDzaBLpj70DZv/bG9fBdV+5hvcZTzy+K7CXqEL3wH7c8yePIK2PU/9ClOpZrMAW0lN3tDmXFkmLvJMyFSMxbNcxgPcTLH524ybeZ1y8AmGyORLbKhY5ndc4tPgezUWP37FxGiG238xstU/gFhYWFm0Ku4BbWFhYtCm2lEJxHDeowuLu5a7bpAqR/r649X9qDIqicWf4ZhRDs3ZuhugUdUQXSjFjKuZ+jlH//Tto1nbtftIZ+TuGHvhDkgMPh8MyMlKlJtJphJM/fvPHgZ1KIV79+CdQJfnOOVRViog88dShwO7uRUjuUEf2vkHQIL29SEmbnoUwkq4QRUEh8aWLaJVWJCGmoXEz7c2l1NbTZ04FdvI5VBGGBGM4nkAIn6QO9ZNUrTk9A9+Xy6gWXF42xaxcGux9/UjL7B/AOa5mQBEtpaqUBFc0twKuG5LOvur99eh+ptKgdfqpg3t3DOeqtVn1e/Ax0FVTF0FFnDwPKuLIE6DfPAWqsa8Lxxjuhy/nU6BimJLwSCCto6dOv99DamqB1pcLVHFZjoD+WSqA9lpZRtphUSilM4R7ky9x2m/dukYUJrds5HWuRBTr8rLZDq4R7BO4hYWFRZvCLuAWFhYWbYotpVCUUhKqtVryifbgikumGNi+iz5ggSjh0LExT8AZHJwJwnRDuVxu+HnExRt01jGvniN+x6Evh0jNaB0+J6Y6mrV2u7utXONrNTNP8P07YWarK/aqB62e28VLaOu1uobquBUSX3rnHGgMzzc1j6nQTqLUrf3wE8hK2LUb7cQyVPkWjhNVReNg/x6E5r7G9mXSDM+kuQJORCLYV5I6li+mUDkapirEZep+PrEfwl2RGMbHE4eeCuwrV3CfSgVTSGtiL7rPp5ZwD9NEXeRyoH/ujKPWu9UXkeq5dXWCFqoIxicXsBZyJExWRx8kIhj3e/eCClpcAT2ykIJv0inMjZs33grssWHQG4UyxnaUdLSdMM41FDLna5y6xisSwVtcAoXy7kUIqnkVZCNRgab4VOG8QsJkhSL2GeHBLCK+xg4cqkRPLYEOW10B1ZhZawGFopSKKaXeUkqdUUqdU0r929rn+5RSbyqlLiml/rdSKrLRviweHVi/bk9Yv+4sbIZCKYrIJ7TWR0XkmIh8Rin1QRH5AxH5j1rrSRFZFpEvPbzTtHgIsH7dnrB+3UHYkELR1Xj7TrwWrv3TIvIJEfm12udfF5F/IyL/fb19KaXErdERTDcwBWK2MoPNb5lF6vXEG8eOTFeESXOX3/qaxTGNC3Ec2r7+WJUmRUSmsBW25+7qHtMxTcJf/rhZ4c/dx6O33V7jN9+t9KvWvpRK1TBydg5UBN//vj4IUBWLCCXjHVSAISJ5qlZJZ3F/8hWEuHlqYRUngaeBQWQoOD5rReNhc/IgCjN+eu5MYPcPmh3AUxlQFyWiWjq7EaqfO4WiklgYxyhXcH69VMA0vhvHPrj/Mey/TsebvdzTh6yJuXmE9tevoQCqWKyOqXKp1FK/Vsq+zM9U73siDnrRjeJasx5oE10huq5kCszFSEBudAT3ZGgP/HfjJuipHFEoQgV9N2bncQzKFAslQK1UaMzH46ZwXZkyjyrUJb6jE37lU3fClCHi41odQ2efj4Bzyte1lYuEeT3C0lvKg0JLkJ6+Cm+s37+pl5hKKVcpdVpE5kXkVRG5IiIrGo3ypkVkV5PvflkpdVIpdXJtba3RJhbvE1rl13JlY+F5i61Dy/zqtTYt0aL12NQCrrX2tNbHRGRcRJ4XkUONNmvy3Ze11se11se7uroabWLxPqFVfuXoxuL9R8v8elfxhcWjhnvKQtFaryil3hCRD4pIj1IqVPurPi4iM+t+Wardy0O1yW4UzejGT3DraWEb2/mNC3madYN3SXuA7Watz9bLhjHoiiYd5x3KuPEVv6UmfRZpfGxGvRaKomDbqzR7WmqkyXJXNssD+VWUklCkGqqWSZvdp2viDvW5PCiDeMykUJLdKN4Z34Mu5dk8KJRcEdkYkTgyDqanoZHCIbvjI1S+fg3d3DtI19qvo+jiCWSeRKlYhbNsLl+GPstnP/3pwL69gEIVzijKrCJUfvwQKJRDj5vr6ze++WeBrR18/9gxFDN1JvEwdPp0VdfcrftD+qB+DYVCMjxQpZZKpAFTJk0PTdkUXhnnmgibS0tXnN6ZlkAtpNOgm3o64fsnJ6EPs0rZN53UOi29gsym0d0omIrFsZ9Q2KQxCnnouWdXcexsHmOkkx40E6TT9PbbpwN7fgGZIx1JjDWeu6WiOScjpLcTJWYnQT/4pH1e9jZOK9pMFsqgUqqnZsdF5FMicl5EXheRX65t9kUR+asNj2bxyMD6dXvC+nVnYTNP4KMi8nVVrVd3RORbWuv/o5R6V0S+qZT6dyJySkT+9CGep0XrYf26PWH9uoOgHkpRR7ODKbUgIlkR2ThDffthQB6d696rtR7ceLPNoebX6/JoXeNW4VG6ZuvX1uFRu+aGvt3SBVxERCl1Umt9fEsP+ghgJ1z3TrjGeuyEa94J11iPdrlm+5rZwsLCok1hF3ALCwuLNsX7sYC//D4c81HATrjunXCN9dgJ17wTrrEebXHNW86BW1hYWFi0BpZCsbCwsGhTbOkCrpT6jFLqglLqslLqK1t57K2CUmq3Uup1pdT5mpznb9U+71NKvVqT83xVKdW70b7aBTvBryI7z7fWr4++X7eMQqkVFlwUkZekKqZzQkR+VWv97rpfbDMopUZFZFRr/bZSqlNEfiIivygi/1REUlrr369Nhl6t9e++j6faEuwUv4rsLN9av7aHX7fyCfx5EbmstZ7SWpdE5Jsi8vktPP6WQGs9q7V+u2anpVrGvEuq1/r12mZfl+oA2Q7YEX4V2XG+tX5tA79u5QK+S0Ru0s9NJS23C5RSEyLytIi8KSLDWutZkeqAEZGh5t9sK+w4v4rsCN9av7aBX7dyAW/UjWDbpsAopZIi8m0R+W2t9XYWQt9RfhXZMb61fm0DbOUCPi0iu+nnzUmVtiGUUmGpDoRvaK3/svbx7RrXdodzm2/2/TbDjvGryI7yrfVrG/h1KxfwEyIyqarNVSMi8gUReWULj78lUFXR7T8VkfNa6z+kX70iVRlPke0l57kj/Cqy43xr/doGft1qNcLPish/EhFXRL6qtf73W3bwLYJS6kMi8vciclbQIO/3pMqpfUtE9ojIDRH5Fa11quFO2gw7wa8iO8+31q+Pvl9tJaaFhYVFm8JWYlpYWFi0KewCbmFhYdGmsAu4hYWFRZvCLuAWFhYWbQq7gFtYWFi0KewCbmFhYdGmsAu4hYWFRZvCLuAWFhYWbYr/DydSel4bZX5cAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 3\n",
    "f = datagen.flow(x_train[0:N,...], y_train[0:N,...], batch_size=N)\n",
    "im = next(f)\n",
    "print(type(f))\n",
    "print(type(im[0]), type(im[1]))\n",
    "\n",
    "def show(im):\n",
    "    images = im[0]\n",
    "    labels = im[1]\n",
    "    N, w,h,c = images.shape\n",
    "    for i in range(N):\n",
    "        plt.subplot(1,N, i+1)\n",
    "        plt.imshow(images[i])\n",
    "    plt.show()\n",
    "        \n",
    "show((x_train[0:N,...], y_train[0:N,...]))\n",
    "        \n",
    "show(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = im[0]\n",
    "labels = im[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 32, 32, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
