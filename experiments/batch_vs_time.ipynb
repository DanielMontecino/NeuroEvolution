{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.codification_cnn import CNNLayer, NNLayer, ChromosomeCNN, FitnessCNN, FitnessCNNParallel\n",
    "from utils.datamanager import DataManager\n",
    "from utils.lr_finder import LRFinder\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_cnn = FitnessCNN()\n",
    "\n",
    "# dataset params:\n",
    "data_folder = '../../datasets'\n",
    "classes = []\n",
    "\n",
    "# Fitness params\n",
    "epochs = 15\n",
    "batch_size = 128\n",
    "verbose = 1\n",
    "redu_plat = False\n",
    "early_stop = 0\n",
    "warm_up_epochs= 5\n",
    "base_lr = 0.002\n",
    "smooth = 0.1\n",
    "cosine_dec = True\n",
    "lr_find = True\n",
    "\n",
    "dataset = 'MRDBI'\n",
    "dm = DataManager(dataset, clases=classes, folder_var_mnist=data_folder)\n",
    "data = dm.load_data()\n",
    "print(data[0][0].shape)\n",
    "\n",
    "fitness_cnn.set_params(data=data, verbose=verbose, batch_size=batch_size, reduce_plateau=redu_plat,\n",
    "                   epochs=epochs, cosine_decay=cosine_dec, early_stop=early_stop, \n",
    "                   warm_epochs=warm_up_epochs, base_lr=base_lr, smooth_label=smooth, find_lr=lr_find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_2 = CNNLayer(86, (3,5), 'leakyreLu', 0.262, 1)\n",
    "l2_2 = CNNLayer(84, (5,3), 'leakyreLu', 0.319, 1)\n",
    "l3_2 = CNNLayer(243, (1,3), 'prelu', 0.322, 1)\n",
    "l4_2 = NNLayer(948, 'sigmoid', 0.467)\n",
    "l5_2 = NNLayer(780, 'sigmoid', 0.441)\n",
    "base_model = ChromosomeCNN([l1_2, l2_2, l3_2], [l4_2, l5_2], fitness_cnn)\n",
    "\n",
    "l1_1 = CNNLayer(32, (3,3), 'relu', 0.2, 1)\n",
    "l2_1 = CNNLayer(64, (3,3), 'relu', 0.2, 1)\n",
    "l3_1 = NNLayer(512, 'relu', 0.5)\n",
    "l4_1 = NNLayer(512, 'relu', 0.5)\n",
    "simple_model = ChromosomeCNN([l1_1, l2_1], [l3_1, l4_1], fitness_cnn)\n",
    "\n",
    "models = {'simple': simple_model, 'base':base_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View how change the LR, TIME and ACC depending on the batch size, training on 75 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_up_epochs= 5\n",
    "lr_find = False\n",
    "base_lr = 0.002\n",
    "\n",
    "fitness_cnn.set_params(data=data, verbose=verbose, batch_size=batch_size, reduce_plateau=redu_plat,\n",
    "                   epochs=epochs, cosine_decay=cosine_dec, early_stop=early_stop, \n",
    "                   warm_epochs=warm_up_epochs, base_lr=base_lr, smooth_label=smooth, find_lr=lr_find)\n",
    "\n",
    "BATCH_SIZES = [32, 64, 128, 256, 512, 1024]\n",
    "#BATCH_SIZES = [BATCH_SIZES[-i-1] for i in range(len(BATCH_SIZES))]\n",
    "TIMES = []\n",
    "SCORES = []\n",
    "LRTS = []\n",
    "model_name = 'simple'\n",
    "\n",
    "\n",
    "for batch_size in BATCH_SIZES:\n",
    "    fitness_cnn.batch_size = batch_size\n",
    "    ti = time()\n",
    "    score = fitness_cnn.calc(models[model_name], fp=32)\n",
    "    lr = fitness_cnn.learning_rate_base\n",
    "    print(\"BATCH SIZE \", batch_size)\n",
    "    print(\"SCORE: %0.4f\" % score)\n",
    "    print(\"ELAPSED TIME %0.3f\" % (time() - ti))\n",
    "    print(\"LEARNING RATE: %0.5f\" % lr)\n",
    "    TIMES.append(time() - ti)\n",
    "    SCORES.append(score)\n",
    "    LRTS.append(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "BATCH_SIZES_ = BATCH_SIZES[k::]\n",
    "SCORES_ = SCORES[k::]\n",
    "LRTS_ = LRTS[k::]\n",
    "TIMES_ = TIMES[k::]\n",
    "\n",
    "plt.plot(BATCH_SIZES_, SCORES_, label='scores')\n",
    "plt.xlabel('Batch size')\n",
    "plt.xticks(BATCH_SIZES_)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(BATCH_SIZES_, LRTS_, label='learning rate')\n",
    "plt.xlabel('Batch size')\n",
    "plt.xticks(BATCH_SIZES_)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(BATCH_SIZES_, TIMES_, label=\"times\")\n",
    "plt.xlabel('Batch size')\n",
    "plt.xticks(BATCH_SIZES_)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View how change the LR, TIME and ACC depending on the batch size, training until plateus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness params\n",
    "epochs = 75\n",
    "batch_size = 128\n",
    "verbose = 1\n",
    "redu_plat = False\n",
    "early_stop = 15\n",
    "warm_up_epochs = 5\n",
    "base_lr = 0.002\n",
    "smooth = 0.1\n",
    "cosine_dec = True\n",
    "lr_find = True \n",
    "\n",
    "dataset = 'MBI'\n",
    "dm = DataManager(dataset, clases=classes, folder_var_mnist=data_folder)\n",
    "data = dm.load_data()\n",
    "print(data[0][0].shape)\n",
    "\n",
    "fitness_cnn.set_params(data=data, verbose=verbose, batch_size=batch_size, reduce_plateau=redu_plat,\n",
    "                   epochs=epochs, cosine_decay=cosine_dec, early_stop=early_stop, \n",
    "                   warm_epochs=warm_up_epochs, base_lr=base_lr, smooth_label=smooth, find_lr=lr_find)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure training time to reach a given acc %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import keras\n",
    "\n",
    "class StopAtGoal(keras.callbacks.Callback):\n",
    "    def __init__(self, goal, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.goal = goal\n",
    "        self.epochs = 0\n",
    "        self.ti = time()\n",
    "        self.final_acc = None\n",
    "        self.total_time = None\n",
    "        self.total_epochs = None\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        acc = logs['val_acc']\n",
    "        self.epochs += 1\n",
    "        \n",
    "        if acc >= self.goal:\n",
    "            self.model.stop_training = True\n",
    "            self.final_acc = acc\n",
    "            self.total_time = time() - self.ti\n",
    "            self.total_epochs = self.epochs\n",
    "        return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import WarmUpCosineDecayScheduler\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "class Fitness_callback(FitnessCNN):\n",
    "    def set_callbacks(self, file_model=None):\n",
    "        callbacks = []\n",
    "        # Create the Learning rate scheduler.\n",
    "        total_steps = int(self.epochs * self.y_train.shape[0] / self.batch_size)\n",
    "        warm_up_steps = int(self.warmup_epochs * self.y_train.shape[0] / self.batch_size)\n",
    "        base_steps = total_steps * (not self.cosine_decay)\n",
    "        warm_up_lr = WarmUpCosineDecayScheduler(learning_rate_base=self.learning_rate_base,\n",
    "                                                total_steps=total_steps,\n",
    "                                                warmup_learning_rate=0.0,\n",
    "                                                warmup_steps=warm_up_steps,\n",
    "                                                hold_base_rate_steps=base_steps)\n",
    "        callbacks.append(warm_up_lr)\n",
    "\n",
    "        if file_model is not None:\n",
    "            checkpoint_acc = ModelCheckpoint(file_model, monitor='val_acc', save_best_only=True)\n",
    "            callbacks.append(checkpoint_acc)\n",
    "\n",
    "        if self.early_stop > 0 and keras.__version__ == '2.2.4':\n",
    "            callbacks.append(EarlyStopping(monitor='val_acc', patience=self.early_stop, restore_best_weights=True))\n",
    "        elif self.early_stop > 0:\n",
    "            callbacks.append(EarlyStopping(monitor='val_acc', patience=self.early_stop))\n",
    "        if self.reduce_plateu:\n",
    "            callbacks.append(ReduceLROnPlateau(monitor='val_acc', factor=0.2,\n",
    "                                               patience=5, verbose=self.verb))\n",
    "        self.stopGoal = StopAtGoal(0.9)\n",
    "        callbacks.append(self.stopGoal)\n",
    "        return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_cnn = Fitness_callback()\n",
    "\n",
    "# Fitness params\n",
    "epochs = 75\n",
    "batch_size = 128\n",
    "verbose = 1\n",
    "redu_plat = False\n",
    "early_stop = 0\n",
    "warm_up_epochs= 0\n",
    "base_lr = 0.001\n",
    "smooth = 0.1\n",
    "cosine_dec = True\n",
    "lr_find = True\n",
    "\n",
    "fitness_cnn.set_params(data=data, verbose=verbose, batch_size=batch_size, reduce_plateau=redu_plat,\n",
    "                   epochs=epochs, cosine_decay=cosine_dec, early_stop=early_stop, \n",
    "                   warm_epochs=warm_up_epochs, base_lr=base_lr, smooth_label=smooth, find_lr=lr_find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti = time()\n",
    "score = fitness_cnn.calc(base_model, fp=32, test=True)\n",
    "print(\"SCORE: %0.4f\" % score)\n",
    "print(\"ELAPSED TIME %0.3f\" % (time() - ti))\n",
    "\n",
    "stopGoal = fitness_cnn.stopGoal\n",
    "print(\"Model reached at goal of %0.2f with %0.2f!\" % (stopGoal.goal, stopGoal.final_acc))\n",
    "print(\"Total time %0.3f [s] in %d epochs\" % (stopGoal.total_time, stopGoal.total_epochs))\n",
    "\n",
    "BATCH_SIZES = [32, 64, 128, 256, 512, 1024]\n",
    "TIMES = []\n",
    "SCORES = []\n",
    "LRTS = []\n",
    "EPOCHS = []\n",
    "model_name = 'base'\n",
    "\n",
    "\n",
    "for batch_size in BATCH_SIZES:\n",
    "    fitness_cnn.batch_size = batch_size\n",
    "    ti = time()\n",
    "    score = fitness_cnn.calc(models[model_name], fp=32, test=True)\n",
    "    \n",
    "    stopGoal = fitness_cnn.stopGoal\n",
    "    final_acc = stopGoal.final_acc\n",
    "    total_time = time() - ti\n",
    "    epochs = stopGoal.total_epochs\n",
    "    lr = fitness_cnn.learning_rate_base\n",
    "    \n",
    "    print(\"BATCH SIZE \", batch_size)\n",
    "    print(\"SCORE: %0.4f\" % score)\n",
    "    print(\"ELAPSED TIME %0.3f\" % total_time)\n",
    "    print(\"LEARNING RATE: %0.5f\" % lr)\n",
    "    TIMES.append(total_time)\n",
    "    SCORES.append(score)\n",
    "    LRTS.append(lr)\n",
    "    EPOCHS.append(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(BATCH_SIZES, TIMES, label=\"times\")\n",
    "plt.plot(BATCH_SIZES, SCORES, label='scores')\n",
    "plt.plot(BATCH_SIZES, LRTS, label='learning rate')\n",
    "plt.plot(BATCH_SIZES, EPOCHS, label='epochs')\n",
    "plt.xlabel('Batch size')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
