{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from time import time\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.codification_cnn import FitnessCNNParallel\n",
    "from utils.codification_grew import FitnessGrow, ChromosomeGrow, HyperParams, Merger\n",
    "from utils.codification_grew import Inputs, MaxPooling, AvPooling, OperationBlock, CNNGrow, IdentityGrow\n",
    "from utils.datamanager import DataManager\n",
    "from GA.geneticAlgorithm import TwoLevelGA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chromosome parameters\n",
    "ChromosomeGrow._max_initial_blocks = 5\n",
    "ChromosomeGrow._grow_prob = 0.15\n",
    "ChromosomeGrow._decrease_prob = 0.25\n",
    "\n",
    "\n",
    "Merger._projection_type = ['normal', 'extend'][1]\n",
    "\n",
    "HyperParams._GROW_RATE_LIMITS = [1.5, 5.]\n",
    "HyperParams._N_CELLS = [1, 2]\n",
    "HyperParams._N_BLOCKS = [2]\n",
    "\n",
    "#########################\n",
    "HyperParams._STEM = [16, 32]\n",
    "\n",
    "OperationBlock._change_op_prob = 0.15\n",
    "OperationBlock._change_concat_prob = 0.15\n",
    "\n",
    "CNNGrow.filters_mul_range = [0.1, 1.2]\n",
    "CNNGrow.possible_activations = ['relu', 'elu', 'prelu']\n",
    "CNNGrow.dropout_range = [0, 0.7]\n",
    "CNNGrow.possible_k = [1, 3, 5]\n",
    "CNNGrow.k_prob = 0.2\n",
    "CNNGrow.drop_prob = 0.2\n",
    "CNNGrow.filter_prob = 0.2\n",
    "CNNGrow.act_prob = 0.2\n",
    "\n",
    "Inputs._mutate_prob = 0.5\n",
    "\n",
    "    \n",
    "data_folder = '../../datasets/MNIST_variations'\n",
    "command = 'python3 ../train_gen.py'\n",
    "verbose = 0\n",
    "\n",
    "gpus = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset params:\n",
    "data_folder = data_folder\n",
    "classes = []\n",
    "\n",
    "######################333\n",
    "resize = 16\n",
    "\n",
    "# genetic algorithm params:\n",
    "generations = 30\n",
    "population_first_level = 20\n",
    "population_second_level = 8\n",
    "training_hours = 68\n",
    "save_progress = True\n",
    "maximize_fitness = False\n",
    "statistical_validation = False\n",
    "frequency_second_level = 3\n",
    "start_level2 = 1\n",
    "\n",
    "\n",
    "# Fitness params\n",
    "epochs = 15\n",
    "batch_size = 256\n",
    "verbose = verbose\n",
    "redu_plat = False\n",
    "early_stop = 0\n",
    "warm_up_epochs = 0\n",
    "base_lr = 0.05\n",
    "smooth = 0.1\n",
    "cosine_dec = False\n",
    "lr_find = False\n",
    "precise_eps = 60\n",
    "\n",
    "include_time = True\n",
    "test_eps = 100\n",
    "augment = False\n",
    "\n",
    "datasets = ['fashion_mnist', 'MB', 'MBI', 'MRB', 'MRD', 'MRDBI']\n",
    "datasets = ['fashion_mnist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EVOLVING IN DATASET fashion_mnist ...\n",
      "\n",
      "(48000, 16, 16, 1) train samples\n",
      "(12000, 16, 16, 1) validation samples\n",
      "(10000, 16, 16, 1) test samples\n",
      "Number of individuals eliminated by age: 0\n",
      "Genetic algorithm params\n",
      "Number of generations: 30\n",
      "Population size: 20\n",
      "Folder to save: ../../exp_resize/fashion_mnist/genetic/0_2020-03-06-19:18/GA_experiment\n",
      "num parents: 5\n",
      "offspring size: 15\n",
      "\n",
      "Population size level one: 20\n",
      "Population size level two: 8\n",
      "Number of parents level one: 5\n",
      "Number of parents level two: 4\n",
      "Offspring size level one: 15\n",
      "Offspring size level two: 4\n",
      "Grow V2 with image resizing to 16\n",
      "0\n",
      "30\n",
      "Creating Initial population\n",
      "\n",
      "Start evolution process...\n",
      "\n",
      "Initial population initialization...20\n",
      "\n",
      "0) Ranking level 1... Models to train: 20 ...OK (in 30.51 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "1) Ranking level 1... Models to train: 15 ...OK (in 24.89 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "1) Ranking level 2... Models to train: 8 ...OK (in 62.07 minutes)\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "2) Ranking level 1... Models to train: 15 ...OK (in 30.26 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "3) Ranking level 1... Models to train: 15 ...OK (in 26.15 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "3) Ranking level 2... Models to train: 4 ...OK (in 46.70 minutes)\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "Generation (3) in 220.59 minutes.\n",
      "Best first level fitness: 0.07093\n",
      "Best second level fitness: 0.07250\n",
      "\n",
      "4) Ranking level 1... Models to train: 15 ...OK (in 25.87 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "5) Ranking level 1... Models to train: 15 ...OK (in 36.48 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "6) Ranking level 1... Models to train: 15 ...OK (in 37.61 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "6) Ranking level 2... Models to train: 4 ...OK (in 41.99 minutes)\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "Generation (6) in 362.54 minutes.\n",
      "Best first level fitness: 0.06999\n",
      "Best second level fitness: 0.07099\n",
      "\n",
      "7) Ranking level 1... Models to train: 15 ...OK (in 42.99 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "8) Ranking level 1... Models to train: 15 ...OK (in 35.51 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "9) Ranking level 1... Models to train: 15 ...OK (in 45.02 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "9) Ranking level 2... Models to train: 4 ...OK (in 28.59 minutes)\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "Generation (9) in 514.65 minutes.\n",
      "Best first level fitness: 0.06999\n",
      "Best second level fitness: 0.07099\n",
      "\n",
      "10) Ranking level 1... Models to train: 15 ...OK (in 34.59 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "11) Ranking level 1... Models to train: 15 ...OK (in 27.89 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "12) Ranking level 1... Models to train: 15 ...OK (in 34.18 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "12) Ranking level 2... Models to train: 3 ...OK (in 30.68 minutes)\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "Generation (12) in 641.99 minutes.\n",
      "Best first level fitness: 0.06999\n",
      "Best second level fitness: 0.07099\n",
      "\n",
      "13) Ranking level 1... Models to train: 15 ...OK (in 37.24 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "14) Ranking level 1... Models to train: 14 ...OK (in 31.91 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "15) Ranking level 1... Models to train: 15 ...OK (in 29.93 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "15) Ranking level 2... Models to train: 3 ...OK (in 40.65 minutes)\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "Generation (15) in 781.72 minutes.\n",
      "Best first level fitness: 0.06999\n",
      "Best second level fitness: 0.07099\n",
      "\n",
      "16) Ranking level 1... Models to train: 15 ...OK (in 28.38 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "17) Ranking level 1... Models to train: 15 ...OK (in 29.07 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "18) Ranking level 1... Models to train: 15 ...OK (in 34.03 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "18) Ranking level 2... Models to train: 2 ...OK (in 23.73 minutes)\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "Generation (18) in 896.93 minutes.\n",
      "Best first level fitness: 0.06999\n",
      "Best second level fitness: 0.07099\n",
      "\n",
      "19) Ranking level 1... Models to train: 15 ...OK (in 38.37 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "20) Ranking level 1... Models to train: 15 ...OK (in 31.75 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "21) Ranking level 1... Models to train: 15 ...OK (in 47.24 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "21) Ranking level 2... Models to train: 3 ...OK (in 29.59 minutes)\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "Generation (21) in 1043.89 minutes.\n",
      "Best first level fitness: 0.06999\n",
      "Best second level fitness: 0.06973\n",
      "\n",
      "22) Ranking level 1... Models to train: 15 ...OK (in 40.20 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "23) Ranking level 1... Models to train: 15 ...OK (in 31.21 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "24) Ranking level 1... Models to train: 15 ...OK (in 40.45 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "24) Ranking level 2... Models to train: 3 ...OK (in 30.79 minutes)\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "Generation (24) in 1186.54 minutes.\n",
      "Best first level fitness: 0.06999\n",
      "Best second level fitness: 0.06973\n",
      "\n",
      "25) Ranking level 1... Models to train: 15 ...OK (in 72.14 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "26) Ranking level 1... Models to train: 15 ...OK (in 42.48 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "27) Ranking level 1... Models to train: 15 ...OK (in 32.86 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "27) Ranking level 2... Models to train: 3 ...OK (in 21.53 minutes)\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "Generation (27) in 1355.56 minutes.\n",
      "Best first level fitness: 0.06999\n",
      "Best second level fitness: 0.06973\n",
      "\n",
      "28) Ranking level 1... Models to train: 15 ...OK (in 32.17 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "29) Ranking level 1... Models to train: 15 ...OK (in 32.66 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "29) Ranking level 2... Models to train: 0 ...OK (in 0.00 minutes)\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "Generation (29) in 1420.40 minutes.\n",
      "Best first level fitness: 0.06999\n",
      "Best second level fitness: 0.06973\n",
      "Best Gen -> \n",
      "||Identity||woCAT||1||\n",
      "||Identity||woCAT||01||\n",
      "||CNN|F:0.8|K:3|A:prelu|D:0.40||woCAT||001||\n",
      "||CNN|F:1.1|K:3|A:elu|D:0.25||CAT||0101||\n",
      "HP->|GR:4.42|CELL:2|BLOCK:2|STEM:32|LR:0.0149|WU:0.4\n",
      "\n",
      "With Fitness (val): 0.0697 and (test): 0.0657\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 87087.362\n"
     ]
    }
   ],
   "source": [
    "fitness_cnn = FitnessGrow()    \n",
    "c = ChromosomeGrow.random_individual()   \n",
    "experiments_folder = '../../exp_resize' \n",
    "description = \"Grow V2 with image resizing to 16\"\n",
    "\n",
    "experiments_folder = experiments_folder\n",
    "os.makedirs(experiments_folder, exist_ok=True)\n",
    "for dataset in datasets:\n",
    "    print(\"\\nEVOLVING IN DATASET %s ...\\n\" % dataset)\n",
    "    exp_folder = os.path.join(experiments_folder, dataset)\n",
    "    folder = os.path.join(exp_folder, 'genetic')\n",
    "    fitness_folder = exp_folder\n",
    "    fitness_file = os.path.join(fitness_folder, 'fitness_example')   \n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        generational = TwoLevelGA.load_genetic_algorithm(folder=folder)\n",
    "        # Load data\n",
    "        num_clases = 100 if dataset == 'cifar100' else 10\n",
    "        dm = DataManager(dataset, clases=classes, folder_var_mnist=data_folder, num_clases=num_clases) #, max_examples=8000)\n",
    "        data = dm.load_data()\n",
    "        fitness_cnn.set_params(data=data, verbose=verbose, batch_size=batch_size, reduce_plateau=redu_plat,\n",
    "                       epochs=epochs, cosine_decay=cosine_dec, early_stop=early_stop, \n",
    "                       warm_epochs=warm_up_epochs, base_lr=base_lr, smooth_label=smooth, find_lr=lr_find,\n",
    "                       precise_epochs=precise_eps, include_time=include_time, test_eps=test_eps, augment=augment)\n",
    "\n",
    "        fitness_cnn.save(fitness_file)\n",
    "    except:\n",
    "        # Load data\n",
    "        num_clases = 100 if dataset == 'cifar100' else 10\n",
    "        dm = DataManager(dataset, clases=classes, folder_var_mnist=data_folder, num_clases=num_clases, resize=resize) #, max_examples=8000)\n",
    "        data = dm.load_data()\n",
    "        fitness_cnn.set_params(data=data, verbose=verbose, batch_size=batch_size, reduce_plateau=redu_plat,\n",
    "                       epochs=epochs, cosine_decay=cosine_dec, early_stop=early_stop, \n",
    "                       warm_epochs=warm_up_epochs, base_lr=base_lr, smooth_label=smooth, find_lr=lr_find,\n",
    "                       precise_epochs=precise_eps, include_time=include_time, test_eps=test_eps,  augment=augment)\n",
    "\n",
    "        fitness_cnn.save(fitness_file)\n",
    "\n",
    "        del dm, data\n",
    "\n",
    "        fitness = FitnessCNNParallel()\n",
    "        fitness.set_params(chrom_files_folder=fitness_folder, fitness_file=fitness_file, max_gpus=gpus,\n",
    "                       fp=32, main_line=command)\n",
    "        generational = TwoLevelGA(chromosome=c,\n",
    "                                  fitness=fitness,\n",
    "                                  generations=generations,\n",
    "                                  population_first_level=population_first_level,\n",
    "                                  population_second_level=population_second_level,\n",
    "                                  training_hours=training_hours,\n",
    "                                  save_progress=save_progress,\n",
    "                                  maximize_fitness=maximize_fitness,\n",
    "                                  statistical_validation=statistical_validation,\n",
    "                                  folder=folder,\n",
    "                                  start_level2=start_level2,\n",
    "                                  frequency_second_level=frequency_second_level)\n",
    "        generational.print_genetic(description)\n",
    "\n",
    "\n",
    "    ti_all = time()\n",
    "    print(generational.generation)\n",
    "    print(generational.num_generations)\n",
    "    if generational.generation < generational.num_generations:\n",
    "        winner, best_fit, ranking = generational.evolve()\n",
    "    print(\"Total elapsed time: %0.3f\" % (time() - ti_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 28, 28, 1) train samples\n",
      "(12000, 28, 28, 1) validation samples\n",
      "(10000, 28, 28, 1) test samples\n",
      "Training... Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 28, 28, 32)   320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 28, 28, 32)   128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 28, 28, 32)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 28, 28, 46)   13294       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 28, 28, 46)   36064       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 28, 28, 78)   0           conv2d_1[0][0]                   \n",
      "                                                                 p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 28, 28, 78)   312         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 28, 28, 78)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 28, 28, 71)   49913       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 28, 28, 71)   284         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 28, 28, 71)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 28, 28, 96)   61440       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_2 (PReLU)               (None, 28, 28, 96)   75264       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 28, 28, 167)  0           conv2d_3[0][0]                   \n",
      "                                                                 p_re_lu_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 28, 28, 167)  668         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 28, 28, 167)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 28, 28, 149)  224096      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 14, 14, 149)  0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 14, 14, 149)  596         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 14, 14, 149)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 14, 14, 203)  272426      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_3 (PReLU)               (None, 14, 14, 203)  39788       conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 14, 14, 352)  0           max_pooling2d_1[0][0]            \n",
      "                                                                 p_re_lu_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 14, 14, 352)  1408        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 14, 14, 352)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 14, 14, 315)  998235      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 14, 14, 315)  1260        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 14, 14, 315)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 14, 14, 426)  1208136     dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_4 (PReLU)               (None, 14, 14, 426)  83496       conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 14, 14, 741)  0           conv2d_7[0][0]                   \n",
      "                                                                 p_re_lu_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 14, 14, 741)  2964        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 14, 14, 741)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 14, 14, 663)  4422210     dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 663)          0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           6640        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 7,498,942\n",
      "Trainable params: 7,495,132\n",
      "Non-trainable params: 3,810\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 67s 1ms/step - loss: 1.1970 - accuracy: 0.6916 - val_loss: 4.4039 - val_accuracy: 0.1937\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 58s 959us/step - loss: 0.8801 - accuracy: 0.8432 - val_loss: 2.1918 - val_accuracy: 0.5860\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 58s 973us/step - loss: 0.8088 - accuracy: 0.8759 - val_loss: 0.6713 - val_accuracy: 0.8000\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 59s 978us/step - loss: 0.7612 - accuracy: 0.8957 - val_loss: 0.2987 - val_accuracy: 0.9000\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 60s 995us/step - loss: 0.7239 - accuracy: 0.9134 - val_loss: 0.2843 - val_accuracy: 0.9163\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 59s 976us/step - loss: 0.6984 - accuracy: 0.9238 - val_loss: 0.2978 - val_accuracy: 0.9067\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 59s 989us/step - loss: 0.6791 - accuracy: 0.9315 - val_loss: 0.2676 - val_accuracy: 0.9277\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 60s 1000us/step - loss: 0.6663 - accuracy: 0.9366 - val_loss: 0.2947 - val_accuracy: 0.9167\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 60s 992us/step - loss: 0.6570 - accuracy: 0.9402 - val_loss: 0.2595 - val_accuracy: 0.9280\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.6482 - accuracy: 0.9446 - val_loss: 0.2780 - val_accuracy: 0.9227\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 60s 995us/step - loss: 0.6399 - accuracy: 0.9486 - val_loss: 0.2720 - val_accuracy: 0.9300\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 60s 996us/step - loss: 0.6363 - accuracy: 0.9513 - val_loss: 0.2465 - val_accuracy: 0.9353\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 60s 997us/step - loss: 0.6264 - accuracy: 0.9558 - val_loss: 0.3159 - val_accuracy: 0.9170\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 60s 998us/step - loss: 0.6224 - accuracy: 0.9574 - val_loss: 0.2612 - val_accuracy: 0.9367\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 60s 997us/step - loss: 0.6132 - accuracy: 0.9628 - val_loss: 0.2774 - val_accuracy: 0.9333\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 60s 998us/step - loss: 0.6065 - accuracy: 0.9658 - val_loss: 0.2734 - val_accuracy: 0.9320\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 60s 997us/step - loss: 0.6044 - accuracy: 0.9671 - val_loss: 0.2505 - val_accuracy: 0.9403\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 60s 999us/step - loss: 0.5972 - accuracy: 0.9708 - val_loss: 0.2633 - val_accuracy: 0.9383\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 60s 999us/step - loss: 0.5962 - accuracy: 0.9711 - val_loss: 0.2931 - val_accuracy: 0.9237\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 60s 998us/step - loss: 0.5904 - accuracy: 0.9739 - val_loss: 0.2711 - val_accuracy: 0.9373\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 60s 999us/step - loss: 0.5860 - accuracy: 0.9758 - val_loss: 0.2556 - val_accuracy: 0.9350\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 60s 999us/step - loss: 0.5832 - accuracy: 0.9779 - val_loss: 0.2485 - val_accuracy: 0.9360\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 60s 998us/step - loss: 0.5785 - accuracy: 0.9797 - val_loss: 0.2931 - val_accuracy: 0.9297\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 60s 998us/step - loss: 0.5744 - accuracy: 0.9819 - val_loss: 0.2467 - val_accuracy: 0.9387\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 60s 998us/step - loss: 0.5693 - accuracy: 0.9835 - val_loss: 0.2606 - val_accuracy: 0.9353\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 60s 998us/step - loss: 0.5692 - accuracy: 0.9847 - val_loss: 0.2736 - val_accuracy: 0.9357\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 60s 998us/step - loss: 0.5656 - accuracy: 0.9853 - val_loss: 0.2669 - val_accuracy: 0.9377\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 60s 998us/step - loss: 0.5643 - accuracy: 0.9863 - val_loss: 0.2661 - val_accuracy: 0.9333\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 60s 998us/step - loss: 0.5618 - accuracy: 0.9876 - val_loss: 0.2448 - val_accuracy: 0.9423\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 60s 998us/step - loss: 0.5606 - accuracy: 0.9883 - val_loss: 0.2604 - val_accuracy: 0.9327\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 60s 998us/step - loss: 0.5559 - accuracy: 0.9901 - val_loss: 0.2542 - val_accuracy: 0.9303\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 60s 997us/step - loss: 0.5524 - accuracy: 0.9919 - val_loss: 0.2805 - val_accuracy: 0.9340\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 60s 998us/step - loss: 0.5522 - accuracy: 0.9916 - val_loss: 0.2577 - val_accuracy: 0.9383\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 60s 998us/step - loss: 0.5506 - accuracy: 0.9919 - val_loss: 0.2547 - val_accuracy: 0.9363\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 60s 997us/step - loss: 0.5539 - accuracy: 0.9907 - val_loss: 0.2454 - val_accuracy: 0.9400\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 60s 997us/step - loss: 0.5528 - accuracy: 0.9917 - val_loss: 0.2442 - val_accuracy: 0.9380\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 60s 997us/step - loss: 0.5497 - accuracy: 0.9930 - val_loss: 0.2673 - val_accuracy: 0.9327\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 60s 998us/step - loss: 0.5479 - accuracy: 0.9934 - val_loss: 0.2659 - val_accuracy: 0.9380\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 60s 996us/step - loss: 6.7795 - accuracy: 0.7188 - val_loss: 1.3160 - val_accuracy: 0.6330\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 1.0486 - accuracy: 0.7757 - val_loss: 0.5137 - val_accuracy: 0.8300\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.9366 - accuracy: 0.8252 - val_loss: 0.5412 - val_accuracy: 0.8247\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.8651 - accuracy: 0.8515 - val_loss: 0.6103 - val_accuracy: 0.8103\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 60s 992us/step - loss: 0.8290 - accuracy: 0.8683 - val_loss: 0.5356 - val_accuracy: 0.8527\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.7965 - accuracy: 0.8834 - val_loss: 0.3873 - val_accuracy: 0.8827\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.7750 - accuracy: 0.8937 - val_loss: 0.3612 - val_accuracy: 0.8933\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.7622 - accuracy: 0.9000 - val_loss: 0.3729 - val_accuracy: 0.8900\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.7557 - accuracy: 0.9044 - val_loss: 0.3892 - val_accuracy: 0.8903\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.7397 - accuracy: 0.9111 - val_loss: 0.3316 - val_accuracy: 0.9083\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.7325 - accuracy: 0.9148 - val_loss: 0.3193 - val_accuracy: 0.9127\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.7227 - accuracy: 0.9192 - val_loss: 0.3941 - val_accuracy: 0.8797\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.7237 - accuracy: 0.9201 - val_loss: 0.3631 - val_accuracy: 0.9000\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.7028 - accuracy: 0.9270 - val_loss: 0.3180 - val_accuracy: 0.9020\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.7002 - accuracy: 0.9288 - val_loss: 0.2952 - val_accuracy: 0.9230\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.6885 - accuracy: 0.9336 - val_loss: 0.3794 - val_accuracy: 0.8953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.6782 - accuracy: 0.9380 - val_loss: 0.2819 - val_accuracy: 0.9267\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.6833 - accuracy: 0.9384 - val_loss: 0.2861 - val_accuracy: 0.9207\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.6659 - accuracy: 0.9436 - val_loss: 0.3021 - val_accuracy: 0.9197\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.6621 - accuracy: 0.9459 - val_loss: 0.3221 - val_accuracy: 0.9127\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.6524 - accuracy: 0.9502 - val_loss: 0.3657 - val_accuracy: 0.9040\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.6575 - accuracy: 0.9493 - val_loss: 0.3162 - val_accuracy: 0.9150\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.6544 - accuracy: 0.9509 - val_loss: 0.2837 - val_accuracy: 0.9267\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.6350 - accuracy: 0.9588 - val_loss: 0.3134 - val_accuracy: 0.9220\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.6307 - accuracy: 0.9596 - val_loss: 0.2624 - val_accuracy: 0.9337\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.6184 - accuracy: 0.9645 - val_loss: 0.3167 - val_accuracy: 0.9203\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.6275 - accuracy: 0.9621 - val_loss: 0.2643 - val_accuracy: 0.9303\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.6114 - accuracy: 0.9677 - val_loss: 0.2925 - val_accuracy: 0.9253\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.6115 - accuracy: 0.9687 - val_loss: 0.3103 - val_accuracy: 0.9140\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.6030 - accuracy: 0.9726 - val_loss: 0.2747 - val_accuracy: 0.9307\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.5960 - accuracy: 0.9748 - val_loss: 0.2893 - val_accuracy: 0.9290\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 60s 995us/step - loss: 0.5960 - accuracy: 0.9753 - val_loss: 0.2940 - val_accuracy: 0.9267\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.5863 - accuracy: 0.9789 - val_loss: 0.2660 - val_accuracy: 0.9347\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.5822 - accuracy: 0.9804 - val_loss: 0.2609 - val_accuracy: 0.9310\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.5769 - accuracy: 0.9825 - val_loss: 0.2585 - val_accuracy: 0.9347\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.5777 - accuracy: 0.9823 - val_loss: 0.2906 - val_accuracy: 0.9237\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.5714 - accuracy: 0.9851 - val_loss: 0.2676 - val_accuracy: 0.9313\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.5721 - accuracy: 0.9855 - val_loss: 0.2880 - val_accuracy: 0.9313\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.5696 - accuracy: 0.9866 - val_loss: 0.2738 - val_accuracy: 0.9297\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.5653 - accuracy: 0.9876 - val_loss: 0.2766 - val_accuracy: 0.9320\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.5596 - accuracy: 0.9898 - val_loss: 0.2814 - val_accuracy: 0.9297\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.5598 - accuracy: 0.9898 - val_loss: 0.2652 - val_accuracy: 0.9330\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.5567 - accuracy: 0.9906 - val_loss: 0.2587 - val_accuracy: 0.9370\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.5563 - accuracy: 0.9908 - val_loss: 0.2833 - val_accuracy: 0.9360\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.5518 - accuracy: 0.9924 - val_loss: 0.2617 - val_accuracy: 0.9380\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.5492 - accuracy: 0.9929 - val_loss: 0.2566 - val_accuracy: 0.9367\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.5483 - accuracy: 0.9933 - val_loss: 0.2584 - val_accuracy: 0.9420\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.5447 - accuracy: 0.9942 - val_loss: 0.2598 - val_accuracy: 0.9377\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.5429 - accuracy: 0.9949 - val_loss: 0.2650 - val_accuracy: 0.9363\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.5427 - accuracy: 0.9949 - val_loss: 0.2701 - val_accuracy: 0.9403\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.5405 - accuracy: 0.9955 - val_loss: 0.2679 - val_accuracy: 0.9373\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.5392 - accuracy: 0.9955 - val_loss: 0.2719 - val_accuracy: 0.9350\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.5386 - accuracy: 0.9954 - val_loss: 0.2668 - val_accuracy: 0.9373\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.5361 - accuracy: 0.9963 - val_loss: 0.2671 - val_accuracy: 0.9390\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.5344 - accuracy: 0.9969 - val_loss: 0.2642 - val_accuracy: 0.9380\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.5333 - accuracy: 0.9973 - val_loss: 0.2641 - val_accuracy: 0.9363\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 60s 993us/step - loss: 0.5322 - accuracy: 0.9978 - val_loss: 0.2696 - val_accuracy: 0.9370\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 60s 995us/step - loss: 0.5322 - accuracy: 0.9976 - val_loss: 0.2668 - val_accuracy: 0.9367\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.5308 - accuracy: 0.9973 - val_loss: 0.2647 - val_accuracy: 0.9373\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.5305 - accuracy: 0.9976 - val_loss: 0.2657 - val_accuracy: 0.9390\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 60s 994us/step - loss: 0.5294 - accuracy: 0.9979 - val_loss: 0.2655 - val_accuracy: 0.9387\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 60s 995us/step - loss: 0.5284 - accuracy: 0.9980 - val_loss: 0.2656 - val_accuracy: 0.9390\n",
      "Acc -> Val acc: 0.0577,Test (best_acc) acc: 0.0583\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0583 in 99.8 min\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dm = DataManager(dataset, clases=classes, folder_var_mnist=data_folder, num_clases=num_clases, resize=False) #, max_examples=8000)\n",
    "data = dm.load_data()\n",
    "fitness_cnn.set_params(data=data, verbose=verbose, batch_size=batch_size, reduce_plateau=redu_plat,\n",
    "               epochs=epochs, cosine_decay=cosine_dec, early_stop=early_stop, \n",
    "               warm_epochs=warm_up_epochs, base_lr=base_lr, smooth_label=smooth, find_lr=lr_find,\n",
    "               precise_epochs=precise_eps, include_time=include_time, test_eps=test_eps,  augment=augment)\n",
    "fitness_cnn.verb = True\n",
    "score = fitness_cnn.calc(winner, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EVOLVING IN DATASET fashion_mnist ...\n",
      "\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 6us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 3s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 2s 0us/step\n",
      "(48000, 28, 28, 1) train samples\n",
      "(12000, 28, 28, 1) validation samples\n",
      "(10000, 28, 28, 1) test samples\n",
      "Number of individuals eliminated by age: 0\n",
      "Genetic algorithm params\n",
      "Number of generations: 30\n",
      "Population size: 20\n",
      "Folder to save: ../../exp_finals_pool/fashion_mnist/genetic/0_2020-02-06-17:48/GA_experiment\n",
      "num parents: 5\n",
      "offspring size: 15\n",
      "\n",
      "Population size level one: 20\n",
      "Population size level two: 8\n",
      "Number of parents level one: 5\n",
      "Number of parents level two: 4\n",
      "Offspring size level one: 15\n",
      "Offspring size level two: 4\n",
      "Grow V2 Maxpool and AvgPool\n",
      "0\n",
      "30\n",
      "Creating Initial population\n",
      "\n",
      "Start evolution process...\n",
      "\n",
      "Initial population initialization...20\n",
      "\n",
      "0) Ranking level 1... Models to train: 20 ...OK (in 5.04 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "1) Ranking level 1... Models to train: 15 ..."
     ]
    }
   ],
   "source": [
    "for evolve_maxpool in [True, False]:\n",
    "    if evolve_maxpool:\n",
    "        OperationBlock._operations = [CNNGrow, IdentityGrow, MaxPooling, AvPooling]\n",
    "    else:\n",
    "        OperationBlock._operations = [CNNGrow, IdentityGrow]\n",
    "        \n",
    "    fitness_cnn = FitnessGrow()    \n",
    "    c = ChromosomeGrow.random_individual()   \n",
    "    experiments_folder = '../../exp_finals_pool' if evolve_maxpool else '../../exp_finals'\n",
    "    description = \"Grow V2 Maxpool and AvgPool\" if evolve_maxpool else \"Grow V2\"\n",
    "    \n",
    "    experiments_folder = experiments_folder\n",
    "    os.makedirs(experiments_folder, exist_ok=True)\n",
    "    for dataset in datasets:\n",
    "        if dataset == 'cifar10':\n",
    "            test_eps = 200\n",
    "            augment = 'cutout'\n",
    "        print(\"\\nEVOLVING IN DATASET %s ...\\n\" % dataset)\n",
    "        exp_folder = os.path.join(experiments_folder, dataset)\n",
    "        folder = os.path.join(exp_folder, 'genetic')\n",
    "        fitness_folder = exp_folder\n",
    "        fitness_file = os.path.join(fitness_folder, 'fitness_example')   \n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            generational = TwoLevelGA.load_genetic_algorithm(folder=folder)\n",
    "            # Load data\n",
    "            num_clases = 100 if dataset == 'cifar100' else 10\n",
    "            dm = DataManager(dataset, clases=classes, folder_var_mnist=data_folder, num_clases=num_clases) #, max_examples=8000)\n",
    "            data = dm.load_data()\n",
    "            fitness_cnn.set_params(data=data, verbose=verbose, batch_size=batch_size, reduce_plateau=redu_plat,\n",
    "                           epochs=epochs, cosine_decay=cosine_dec, early_stop=early_stop, \n",
    "                           warm_epochs=warm_up_epochs, base_lr=base_lr, smooth_label=smooth, find_lr=lr_find,\n",
    "                           precise_epochs=precise_eps, include_time=include_time, test_eps=test_eps, augment=augment)\n",
    "\n",
    "            fitness_cnn.save(fitness_file)\n",
    "        except:\n",
    "            # Load data\n",
    "            num_clases = 100 if dataset == 'cifar100' else 10\n",
    "            dm = DataManager(dataset, clases=classes, folder_var_mnist=data_folder, num_clases=num_clases) #, max_examples=8000)\n",
    "            data = dm.load_data()\n",
    "            fitness_cnn.set_params(data=data, verbose=verbose, batch_size=batch_size, reduce_plateau=redu_plat,\n",
    "                           epochs=epochs, cosine_decay=cosine_dec, early_stop=early_stop, \n",
    "                           warm_epochs=warm_up_epochs, base_lr=base_lr, smooth_label=smooth, find_lr=lr_find,\n",
    "                           precise_epochs=precise_eps, include_time=include_time, test_eps=test_eps,  augment=augment)\n",
    "\n",
    "            fitness_cnn.save(fitness_file)\n",
    "\n",
    "            del dm, data\n",
    "\n",
    "            fitness = FitnessCNNParallel()\n",
    "            fitness.set_params(chrom_files_folder=fitness_folder, fitness_file=fitness_file, max_gpus=gpus,\n",
    "                           fp=32, main_line=command)\n",
    "            generational = TwoLevelGA(chromosome=c,\n",
    "                                      fitness=fitness,\n",
    "                                      generations=generations,\n",
    "                                      population_first_level=population_first_level,\n",
    "                                      population_second_level=population_second_level,\n",
    "                                      training_hours=training_hours,\n",
    "                                      save_progress=save_progress,\n",
    "                                      maximize_fitness=maximize_fitness,\n",
    "                                      statistical_validation=statistical_validation,\n",
    "                                      folder=folder,\n",
    "                                      start_level2=start_level2,\n",
    "                                      frequency_second_level=frequency_second_level)\n",
    "            generational.print_genetic(description)\n",
    "\n",
    "\n",
    "        ti_all = time()\n",
    "        print(generational.generation)\n",
    "        print(generational.num_generations)\n",
    "        if generational.generation < generational.num_generations:\n",
    "            winner, best_fit, ranking = generational.evolve()\n",
    "        print(\"Total elapsed time: %0.3f\" % (time() - ti_all))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9600, 28, 28, 1) train samples\n",
      "(2400, 28, 28, 1) validation samples\n",
      "(50000, 28, 28, 1) test samples\n",
      "Training with learning rate: 0.07855550518176016\n",
      "\n",
      "Epochs: 15\n",
      "Warmup epochs: 1\n",
      "Training... Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 28, 28, 32)   320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 28, 28, 32)   128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 28, 28, 32)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 28, 28, 8)    6408        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 28, 28, 8)    32          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 28, 28, 32)   128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 28, 28, 50)   450         batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 28, 28, 26)   858         batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 28, 28, 58)   0           conv2d_2[0][0]                   \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 28, 28, 58)   0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 28, 28, 58)   0           concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 28, 28, 58)   232         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 28, 28, 58)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 28, 28, 31)   16213       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 28, 28, 32)   128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 28, 28, 8)    32          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 28, 28, 31)   124         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 28, 28, 39)   1287        batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 28, 28, 63)   567         batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 28, 28, 40)   1280        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 28, 28, 71)   0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 28, 28, 71)   0           conv2d_2[0][0]                   \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 28, 28, 71)   0           conv2d_5[0][0]                   \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 28, 28, 71)   0           concatenate_3[0][0]              \n",
      "                                                                 concatenate_4[0][0]              \n",
      "                                                                 concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 28, 28, 71)   284         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 28, 28, 71)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 28, 28, 39)   69264       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 28, 28, 39)   156         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 28, 28, 39)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 28, 28, 19)   18544       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 28, 28, 19)   76          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 28, 28, 39)   156         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 28, 28, 111)  2220        batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 28, 28, 91)   3640        batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 28, 28, 130)  0           conv2d_10[0][0]                  \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 28, 28, 130)  0           conv2d_9[0][0]                   \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 28, 28, 130)  0           concatenate_6[0][0]              \n",
      "                                                                 concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 28, 28, 130)  520         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 28, 28, 130)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 28, 28, 70)   81970       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 28, 28, 39)   156         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 28, 28, 19)   76          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 28, 28, 70)   280         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 28, 28, 120)  4800        batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 28, 28, 140)  2800        batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 28, 28, 89)   6319        batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 28, 28, 159)  0           conv2d_9[0][0]                   \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 28, 28, 159)  0           conv2d_10[0][0]                  \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 28, 28, 159)  0           conv2d_13[0][0]                  \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 28, 28, 159)  0           concatenate_8[0][0]              \n",
      "                                                                 concatenate_9[0][0]              \n",
      "                                                                 concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 28, 28, 159)  636         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 28, 28, 159)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 28, 28, 87)   345912      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 14, 14, 87)   0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 14, 14, 87)   348         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 14, 14, 87)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 14, 14, 43)   93568       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 14, 14, 43)   172         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 14, 14, 87)   348         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 14, 14, 246)  10824       batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 14, 14, 202)  17776       batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 14, 14, 289)  0           conv2d_18[0][0]                  \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 14, 14, 289)  0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 14, 14, 289)  0           concatenate_11[0][0]             \n",
      "                                                                 concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 14, 14, 289)  1156        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 14, 14, 289)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 14, 14, 157)  408514      dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 14, 14, 87)   348         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 14, 14, 43)   172         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 14, 14, 157)  628         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 14, 14, 266)  23408       batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 14, 14, 310)  13640       batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 14, 14, 196)  30968       batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 14, 14, 353)  0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 14, 14, 353)  0           conv2d_18[0][0]                  \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 14, 14, 353)  0           conv2d_21[0][0]                  \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 14, 14, 353)  0           concatenate_13[0][0]             \n",
      "                                                                 concatenate_14[0][0]             \n",
      "                                                                 concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 14, 14, 353)  1412        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 14, 14, 353)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 14, 14, 194)  1712244     dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 14, 14, 194)  776         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 14, 14, 194)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 14, 14, 96)   465696      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 14, 14, 96)   384         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 14, 14, 194)  776         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 14, 14, 548)  53156       batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 14, 14, 450)  87750       batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 14, 14, 644)  0           conv2d_26[0][0]                  \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 14, 14, 644)  0           conv2d_25[0][0]                  \n",
      "                                                                 conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 14, 14, 644)  0           concatenate_16[0][0]             \n",
      "                                                                 concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 14, 14, 644)  2576        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 14, 14, 644)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 14, 14, 349)  2023153     dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 14, 14, 194)  776         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 14, 14, 96)   384         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 14, 14, 349)  1396        conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 14, 14, 593)  115635      batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 14, 14, 691)  67027       batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 14, 14, 438)  153300      batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 14, 14, 787)  0           conv2d_25[0][0]                  \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 14, 14, 787)  0           conv2d_26[0][0]                  \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 14, 14, 787)  0           conv2d_29[0][0]                  \n",
      "                                                                 conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 14, 14, 787)  0           concatenate_18[0][0]             \n",
      "                                                                 concatenate_19[0][0]             \n",
      "                                                                 concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 14, 14, 787)  3148        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 14, 14, 787)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 14, 14, 432)  8500032     dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 432)          0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           4330        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 14,361,817\n",
      "Trainable params: 14,352,845\n",
      "Non-trainable params: 8,972\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9600 samples, validate on 2400 samples\n",
      "Epoch 1/15\n",
      "\n",
      "Batch 00001: setting learning rate to 2e-05.\n",
      " 128/9600 [..............................] - ETA: 15:35 - loss: 2.4506 - accuracy: 0.1328\n",
      "Batch 00002: setting learning rate to 0.0010671400690901354.\n",
      " 256/9600 [..............................] - ETA: 7:50 - loss: 2.4812 - accuracy: 0.1250 \n",
      "Batch 00003: setting learning rate to 0.0021142801381802707.\n",
      " 384/9600 [>.............................] - ETA: 5:14 - loss: 3.2472 - accuracy: 0.1224\n",
      "Batch 00004: setting learning rate to 0.003161420207270406.\n",
      " 512/9600 [>.............................] - ETA: 3:56 - loss: 3.8623 - accuracy: 0.0996\n",
      "Batch 00005: setting learning rate to 0.004208560276360541.\n",
      " 640/9600 [=>............................] - ETA: 3:10 - loss: 6.4704 - accuracy: 0.0922\n",
      "Batch 00006: setting learning rate to 0.005255700345450677.\n",
      " 768/9600 [=>............................] - ETA: 2:38 - loss: 10.6788 - accuracy: 0.0859\n",
      "Batch 00007: setting learning rate to 0.006302840414540812.\n",
      " 896/9600 [=>............................] - ETA: 2:16 - loss: 13.0253 - accuracy: 0.0938\n",
      "Batch 00008: setting learning rate to 0.0073499804836309485.\n",
      "1024/9600 [==>...........................] - ETA: 1:59 - loss: 16.8907 - accuracy: 0.0928\n",
      "Batch 00009: setting learning rate to 0.008397120552721082.\n",
      "1152/9600 [==>...........................] - ETA: 1:46 - loss: 21.6798 - accuracy: 0.1059\n",
      "Batch 00010: setting learning rate to 0.009444260621811217.\n",
      "1280/9600 [===>..........................] - ETA: 1:35 - loss: 23.2855 - accuracy: 0.1094\n",
      "Batch 00011: setting learning rate to 0.010491400690901353.\n",
      "1408/9600 [===>..........................] - ETA: 1:27 - loss: 24.9217 - accuracy: 0.1186\n",
      "Batch 00012: setting learning rate to 0.011538540759991488.\n",
      "1536/9600 [===>..........................] - ETA: 1:19 - loss: 27.3302 - accuracy: 0.1335\n",
      "Batch 00013: setting learning rate to 0.012585680829081623.\n",
      "1664/9600 [====>.........................] - ETA: 1:13 - loss: 30.0255 - accuracy: 0.1394\n",
      "Batch 00014: setting learning rate to 0.01363282089817176.\n",
      "1792/9600 [====>.........................] - ETA: 1:08 - loss: 32.3622 - accuracy: 0.1501\n",
      "Batch 00015: setting learning rate to 0.014679960967261896.\n",
      "1920/9600 [=====>........................] - ETA: 1:03 - loss: 35.1688 - accuracy: 0.1536\n",
      "Batch 00016: setting learning rate to 0.01572710103635203.\n",
      "2048/9600 [=====>........................] - ETA: 59s - loss: 38.3233 - accuracy: 0.1587 \n",
      "Batch 00017: setting learning rate to 0.016774241105442164.\n",
      "2176/9600 [=====>........................] - ETA: 55s - loss: 40.6841 - accuracy: 0.1618\n",
      "Batch 00018: setting learning rate to 0.017821381174532303.\n",
      "2304/9600 [======>.......................] - ETA: 52s - loss: 41.9173 - accuracy: 0.1697\n",
      "Batch 00019: setting learning rate to 0.018868521243622434.\n",
      "2432/9600 [======>.......................] - ETA: 49s - loss: 43.3692 - accuracy: 0.1719\n",
      "Batch 00020: setting learning rate to 0.019915661312712572.\n",
      "2560/9600 [=======>......................] - ETA: 47s - loss: 45.3562 - accuracy: 0.1730\n",
      "Batch 00021: setting learning rate to 0.020962801381802707.\n",
      "2688/9600 [=======>......................] - ETA: 44s - loss: 47.5646 - accuracy: 0.1756\n",
      "Batch 00022: setting learning rate to 0.022009941450892842.\n",
      "2816/9600 [=======>......................] - ETA: 42s - loss: 47.9102 - accuracy: 0.1818\n",
      "Batch 00023: setting learning rate to 0.023057081519982977.\n",
      "2944/9600 [========>.....................] - ETA: 40s - loss: 47.6492 - accuracy: 0.1895\n",
      "Batch 00024: setting learning rate to 0.024104221589073115.\n",
      "3072/9600 [========>.....................] - ETA: 38s - loss: 47.8895 - accuracy: 0.1930\n",
      "Batch 00025: setting learning rate to 0.025151361658163247.\n",
      "3200/9600 [=========>....................] - ETA: 36s - loss: 47.6438 - accuracy: 0.1953\n",
      "Batch 00026: setting learning rate to 0.026198501727253385.\n",
      "3328/9600 [=========>....................] - ETA: 34s - loss: 47.5446 - accuracy: 0.1950\n",
      "Batch 00027: setting learning rate to 0.02724564179634352.\n",
      "3456/9600 [=========>....................] - ETA: 33s - loss: 47.2764 - accuracy: 0.1947\n",
      "Batch 00028: setting learning rate to 0.02829278186543365.\n",
      "3584/9600 [==========>...................] - ETA: 31s - loss: 47.2231 - accuracy: 0.1956\n",
      "Batch 00029: setting learning rate to 0.029339921934523793.\n",
      "3712/9600 [==========>...................] - ETA: 30s - loss: 47.0684 - accuracy: 0.1994\n",
      "Batch 00030: setting learning rate to 0.030387062003613925.\n",
      "3840/9600 [===========>..................] - ETA: 29s - loss: 47.1159 - accuracy: 0.2008\n",
      "Batch 00031: setting learning rate to 0.03143420207270406.\n",
      "3968/9600 [===========>..................] - ETA: 28s - loss: 47.1392 - accuracy: 0.2016\n",
      "Batch 00032: setting learning rate to 0.0324813421417942.\n",
      "4096/9600 [===========>..................] - ETA: 26s - loss: 47.0333 - accuracy: 0.2029\n",
      "Batch 00033: setting learning rate to 0.03352848221088433.\n",
      "4224/9600 [============>.................] - ETA: 25s - loss: 46.4626 - accuracy: 0.2069\n",
      "Batch 00034: setting learning rate to 0.03457562227997447.\n",
      "4352/9600 [============>.................] - ETA: 24s - loss: 45.6658 - accuracy: 0.2105\n",
      "Batch 00035: setting learning rate to 0.035622762349064606.\n",
      "4480/9600 [=============>................] - ETA: 23s - loss: 44.7842 - accuracy: 0.2129\n",
      "Batch 00036: setting learning rate to 0.03666990241815474.\n",
      "4608/9600 [=============>................] - ETA: 22s - loss: 43.7235 - accuracy: 0.2196\n",
      "Batch 00037: setting learning rate to 0.03771704248724487.\n",
      "4736/9600 [=============>................] - ETA: 21s - loss: 42.6646 - accuracy: 0.2242\n",
      "Batch 00038: setting learning rate to 0.03876418255633501.\n",
      "4864/9600 [==============>...............] - ETA: 20s - loss: 41.6417 - accuracy: 0.2284\n",
      "Batch 00039: setting learning rate to 0.039811322625425145.\n",
      "4992/9600 [==============>...............] - ETA: 19s - loss: 40.6587 - accuracy: 0.2318\n",
      "Batch 00040: setting learning rate to 0.04085846269451528.\n",
      "5120/9600 [===============>..............] - ETA: 19s - loss: 39.6972 - accuracy: 0.2377\n",
      "Batch 00041: setting learning rate to 0.041905602763605415.\n",
      "5248/9600 [===============>..............] - ETA: 18s - loss: 38.7741 - accuracy: 0.2439\n",
      "Batch 00042: setting learning rate to 0.04295274283269555.\n",
      "5376/9600 [===============>..............] - ETA: 17s - loss: 37.8962 - accuracy: 0.2480\n",
      "Batch 00043: setting learning rate to 0.043999882901785685.\n",
      "5504/9600 [================>.............] - ETA: 16s - loss: 37.0615 - accuracy: 0.2498\n",
      "Batch 00044: setting learning rate to 0.04504702297087582.\n",
      "5632/9600 [================>.............] - ETA: 16s - loss: 36.2598 - accuracy: 0.2537\n",
      "Batch 00045: setting learning rate to 0.046094163039965955.\n",
      "5760/9600 [=================>............] - ETA: 15s - loss: 35.4964 - accuracy: 0.2556\n",
      "Batch 00046: setting learning rate to 0.047141303109056086.\n",
      "5888/9600 [=================>............] - ETA: 14s - loss: 34.7673 - accuracy: 0.2590\n",
      "Batch 00047: setting learning rate to 0.04818844317814623.\n",
      "6016/9600 [=================>............] - ETA: 14s - loss: 34.0691 - accuracy: 0.2625\n",
      "Batch 00048: setting learning rate to 0.04923558324723636.\n",
      "6144/9600 [==================>...........] - ETA: 13s - loss: 33.3993 - accuracy: 0.2669\n",
      "Batch 00049: setting learning rate to 0.050282723316326494.\n",
      "6272/9600 [==================>...........] - ETA: 12s - loss: 32.7551 - accuracy: 0.2717\n",
      "Batch 00050: setting learning rate to 0.05132986338541663.\n",
      "6400/9600 [===================>..........] - ETA: 12s - loss: 32.1356 - accuracy: 0.2761\n",
      "Batch 00051: setting learning rate to 0.05237700345450677.\n",
      "6528/9600 [===================>..........] - ETA: 11s - loss: 31.5422 - accuracy: 0.2785\n",
      "Batch 00052: setting learning rate to 0.05342414352359691.\n",
      "6656/9600 [===================>..........] - ETA: 10s - loss: 30.9710 - accuracy: 0.2800\n",
      "Batch 00053: setting learning rate to 0.05447128359268704.\n",
      "6784/9600 [====================>.........] - ETA: 10s - loss: 30.4227 - accuracy: 0.2817\n",
      "Batch 00054: setting learning rate to 0.05551842366177717.\n",
      "6912/9600 [====================>.........] - ETA: 9s - loss: 29.8906 - accuracy: 0.2867 \n",
      "Batch 00055: setting learning rate to 0.0565655637308673.\n",
      "7040/9600 [=====================>........] - ETA: 9s - loss: 29.3757 - accuracy: 0.2912\n",
      "Batch 00056: setting learning rate to 0.05761270379995744.\n",
      "7168/9600 [=====================>........] - ETA: 8s - loss: 28.8802 - accuracy: 0.2955\n",
      "Batch 00057: setting learning rate to 0.05865984386904759.\n",
      "7296/9600 [=====================>........] - ETA: 8s - loss: 28.4003 - accuracy: 0.3007\n",
      "Batch 00058: setting learning rate to 0.05970698393813772.\n",
      "7424/9600 [======================>.......] - ETA: 7s - loss: 27.9373 - accuracy: 0.3048\n",
      "Batch 00059: setting learning rate to 0.06075412400722785.\n",
      "7552/9600 [======================>.......] - ETA: 7s - loss: 27.4897 - accuracy: 0.3089\n",
      "Batch 00060: setting learning rate to 0.06180126407631799.\n",
      "7680/9600 [=======================>......] - ETA: 6s - loss: 27.0582 - accuracy: 0.3121\n",
      "Batch 00061: setting learning rate to 0.06284840414540813.\n",
      "7808/9600 [=======================>......] - ETA: 6s - loss: 26.6380 - accuracy: 0.3172\n",
      "Batch 00062: setting learning rate to 0.06389554421449826.\n",
      "7936/9600 [=======================>......] - ETA: 5s - loss: 26.2304 - accuracy: 0.3237\n",
      "Batch 00063: setting learning rate to 0.0649426842835884.\n",
      "8064/9600 [========================>.....] - ETA: 5s - loss: 25.8351 - accuracy: 0.3292\n",
      "Batch 00064: setting learning rate to 0.06598982435267854.\n",
      "8192/9600 [========================>.....] - ETA: 4s - loss: 25.4539 - accuracy: 0.3345\n",
      "Batch 00065: setting learning rate to 0.06703696442176867.\n",
      "8320/9600 [=========================>....] - ETA: 4s - loss: 25.0834 - accuracy: 0.3395\n",
      "Batch 00066: setting learning rate to 0.0680841044908588.\n",
      "8448/9600 [=========================>....] - ETA: 3s - loss: 24.7245 - accuracy: 0.3452\n",
      "Batch 00067: setting learning rate to 0.06913124455994894.\n",
      "8576/9600 [=========================>....] - ETA: 3s - loss: 24.3737 - accuracy: 0.3509\n",
      "Batch 00068: setting learning rate to 0.07017838462903908.\n",
      "8704/9600 [==========================>...] - ETA: 2s - loss: 24.0343 - accuracy: 0.3558\n",
      "Batch 00069: setting learning rate to 0.07122552469812922.\n",
      "8832/9600 [==========================>...] - ETA: 2s - loss: 23.7028 - accuracy: 0.3611\n",
      "Batch 00070: setting learning rate to 0.07227266476721934.\n",
      "8960/9600 [===========================>..] - ETA: 2s - loss: 23.3840 - accuracy: 0.3651\n",
      "Batch 00071: setting learning rate to 0.07331980483630948.\n",
      "9088/9600 [===========================>..] - ETA: 1s - loss: 23.0739 - accuracy: 0.3684\n",
      "Batch 00072: setting learning rate to 0.07436694490539962.\n",
      "9216/9600 [===========================>..] - ETA: 1s - loss: 22.7703 - accuracy: 0.3738\n",
      "Batch 00073: setting learning rate to 0.07541408497448975.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 22.4742 - accuracy: 0.3792\n",
      "Batch 00074: setting learning rate to 0.0764612250435799.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 22.1870 - accuracy: 0.3842\n",
      "Batch 00075: setting learning rate to 0.07750836511267002.\n",
      "9600/9600 [==============================] - 33s 3ms/step - loss: 21.9075 - accuracy: 0.3884 - val_loss: 3714.7026 - val_accuracy: 0.1096\n",
      "Epoch 2/15\n",
      "\n",
      "Batch 00076: setting learning rate to 0.07855550518176017.\n",
      " 128/9600 [..............................] - ETA: 17s - loss: 1.2556 - accuracy: 0.7031\n",
      "Batch 00077: setting learning rate to 0.07848070946253945.\n",
      " 256/9600 [..............................] - ETA: 17s - loss: 1.2045 - accuracy: 0.7227\n",
      "Batch 00078: setting learning rate to 0.07840591374331872.\n",
      " 384/9600 [>.............................] - ETA: 17s - loss: 1.1984 - accuracy: 0.7318\n",
      "Batch 00079: setting learning rate to 0.07833111802409799.\n",
      " 512/9600 [>.............................] - ETA: 17s - loss: 1.1893 - accuracy: 0.7285\n",
      "Batch 00080: setting learning rate to 0.07825632230487727.\n",
      " 640/9600 [=>............................] - ETA: 17s - loss: 1.1812 - accuracy: 0.7375\n",
      "Batch 00081: setting learning rate to 0.07818152658565654.\n",
      " 768/9600 [=>............................] - ETA: 17s - loss: 1.1753 - accuracy: 0.7422\n",
      "Batch 00082: setting learning rate to 0.07810673086643581.\n",
      " 896/9600 [=>............................] - ETA: 16s - loss: 1.1727 - accuracy: 0.7422\n",
      "Batch 00083: setting learning rate to 0.07803193514721508.\n",
      "1024/9600 [==>...........................] - ETA: 16s - loss: 1.1785 - accuracy: 0.7344\n",
      "Batch 00084: setting learning rate to 0.07795713942799437.\n",
      "1152/9600 [==>...........................] - ETA: 16s - loss: 1.1844 - accuracy: 0.7292\n",
      "Batch 00085: setting learning rate to 0.07788234370877366.\n",
      "1280/9600 [===>..........................] - ETA: 16s - loss: 1.1776 - accuracy: 0.7367\n",
      "Batch 00086: setting learning rate to 0.07780754798955293.\n",
      "1408/9600 [===>..........................] - ETA: 15s - loss: 1.1629 - accuracy: 0.7443\n",
      "Batch 00087: setting learning rate to 0.0777327522703322.\n",
      "1536/9600 [===>..........................] - ETA: 15s - loss: 1.1630 - accuracy: 0.7428\n",
      "Batch 00088: setting learning rate to 0.07765795655111148.\n",
      "1664/9600 [====>.........................] - ETA: 15s - loss: 1.1629 - accuracy: 0.7428\n",
      "Batch 00089: setting learning rate to 0.07758316083189075.\n",
      "1792/9600 [====>.........................] - ETA: 15s - loss: 1.1674 - accuracy: 0.7405\n",
      "Batch 00090: setting learning rate to 0.07750836511267002.\n",
      "1920/9600 [=====>........................] - ETA: 14s - loss: 1.1666 - accuracy: 0.7391\n",
      "Batch 00091: setting learning rate to 0.0774335693934493.\n",
      "2048/9600 [=====>........................] - ETA: 14s - loss: 1.1604 - accuracy: 0.7422\n",
      "Batch 00092: setting learning rate to 0.07735877367422857.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 1.1635 - accuracy: 0.7436\n",
      "Batch 00093: setting learning rate to 0.07728397795500785.\n",
      "2304/9600 [======>.......................] - ETA: 14s - loss: 1.1587 - accuracy: 0.7426\n",
      "Batch 00094: setting learning rate to 0.07720918223578714.\n",
      "2432/9600 [======>.......................] - ETA: 13s - loss: 1.1563 - accuracy: 0.7405\n",
      "Batch 00095: setting learning rate to 0.07713438651656641.\n",
      "2560/9600 [=======>......................] - ETA: 13s - loss: 1.1560 - accuracy: 0.7414\n",
      "Batch 00096: setting learning rate to 0.07705959079734569.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 1.1493 - accuracy: 0.7429\n",
      "Batch 00097: setting learning rate to 0.07698479507812496.\n",
      "2816/9600 [=======>......................] - ETA: 13s - loss: 1.1450 - accuracy: 0.7461\n",
      "Batch 00098: setting learning rate to 0.07690999935890423.\n",
      "2944/9600 [========>.....................] - ETA: 12s - loss: 1.1353 - accuracy: 0.7524\n",
      "Batch 00099: setting learning rate to 0.0768352036396835.\n",
      "3072/9600 [========>.....................] - ETA: 12s - loss: 1.1291 - accuracy: 0.7555\n",
      "Batch 00100: setting learning rate to 0.07676040792046278.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 1.1280 - accuracy: 0.7528\n",
      "Batch 00101: setting learning rate to 0.07668561220124205.\n",
      "3328/9600 [=========>....................] - ETA: 12s - loss: 1.1240 - accuracy: 0.7542\n",
      "Batch 00102: setting learning rate to 0.07661081648202135.\n",
      "3456/9600 [=========>....................] - ETA: 11s - loss: 1.1220 - accuracy: 0.7526\n",
      "Batch 00103: setting learning rate to 0.07653602076280062.\n",
      "3584/9600 [==========>...................] - ETA: 11s - loss: 1.1185 - accuracy: 0.7550\n",
      "Batch 00104: setting learning rate to 0.0764612250435799.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 1.1204 - accuracy: 0.7557\n",
      "Batch 00105: setting learning rate to 0.07638642932435917.\n",
      "3840/9600 [===========>..................] - ETA: 11s - loss: 1.1145 - accuracy: 0.7591\n",
      "Batch 00106: setting learning rate to 0.07631163360513844.\n",
      "3968/9600 [===========>..................] - ETA: 10s - loss: 1.1100 - accuracy: 0.7616\n",
      "Batch 00107: setting learning rate to 0.07623683788591772.\n",
      "4096/9600 [===========>..................] - ETA: 10s - loss: 1.1081 - accuracy: 0.7634\n",
      "Batch 00108: setting learning rate to 0.07616204216669699.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 1.1031 - accuracy: 0.7661\n",
      "Batch 00109: setting learning rate to 0.07608724644747626.\n",
      "4352/9600 [============>.................] - ETA: 10s - loss: 1.1008 - accuracy: 0.7684\n",
      "Batch 00110: setting learning rate to 0.07601245072825555.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4480/9600 [=============>................] - ETA: 9s - loss: 1.1013 - accuracy: 0.7683 \n",
      "Batch 00111: setting learning rate to 0.07593765500903484.\n",
      "4608/9600 [=============>................] - ETA: 9s - loss: 1.1002 - accuracy: 0.7689\n",
      "Batch 00112: setting learning rate to 0.07586285928981411.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 1.0981 - accuracy: 0.7690\n",
      "Batch 00113: setting learning rate to 0.07578806357059338.\n",
      "4864/9600 [==============>...............] - ETA: 9s - loss: 1.0959 - accuracy: 0.7699\n",
      "Batch 00114: setting learning rate to 0.07571326785137265.\n",
      "4992/9600 [==============>...............] - ETA: 8s - loss: 1.0929 - accuracy: 0.7716\n",
      "Batch 00115: setting learning rate to 0.07563847213215193.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 1.0895 - accuracy: 0.7736\n",
      "Batch 00116: setting learning rate to 0.0755636764129312.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 1.0862 - accuracy: 0.7761\n",
      "Batch 00117: setting learning rate to 0.07548888069371047.\n",
      "5376/9600 [===============>..............] - ETA: 8s - loss: 1.0825 - accuracy: 0.7775\n",
      "Batch 00118: setting learning rate to 0.07541408497448975.\n",
      "5504/9600 [================>.............] - ETA: 7s - loss: 1.0809 - accuracy: 0.7783\n",
      "Batch 00119: setting learning rate to 0.07533928925526905.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 1.0771 - accuracy: 0.7800\n",
      "Batch 00120: setting learning rate to 0.07526449353604832.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 1.0760 - accuracy: 0.7806\n",
      "Batch 00121: setting learning rate to 0.07518969781682759.\n",
      "5888/9600 [=================>............] - ETA: 7s - loss: 1.0736 - accuracy: 0.7824\n",
      "Batch 00122: setting learning rate to 0.07511490209760686.\n",
      "6016/9600 [=================>............] - ETA: 6s - loss: 1.0693 - accuracy: 0.7847\n",
      "Batch 00123: setting learning rate to 0.07504010637838614.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 1.0659 - accuracy: 0.7868\n",
      "Batch 00124: setting learning rate to 0.07496531065916541.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 1.0646 - accuracy: 0.7878\n",
      "Batch 00125: setting learning rate to 0.07489051493994468.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 1.0610 - accuracy: 0.7889\n",
      "Batch 00126: setting learning rate to 0.07481571922072396.\n",
      "6528/9600 [===================>..........] - ETA: 5s - loss: 1.0569 - accuracy: 0.7906\n",
      "Batch 00127: setting learning rate to 0.07474092350150323.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 1.0540 - accuracy: 0.7924\n",
      "Batch 00128: setting learning rate to 0.07466612778228253.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 1.0512 - accuracy: 0.7935\n",
      "Batch 00129: setting learning rate to 0.0745913320630618.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 1.0496 - accuracy: 0.7946\n",
      "Batch 00130: setting learning rate to 0.07451653634384107.\n",
      "7040/9600 [=====================>........] - ETA: 4s - loss: 1.0475 - accuracy: 0.7957\n",
      "Batch 00131: setting learning rate to 0.07444174062462035.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 1.0466 - accuracy: 0.7960\n",
      "Batch 00132: setting learning rate to 0.07436694490539962.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 1.0439 - accuracy: 0.7971\n",
      "Batch 00133: setting learning rate to 0.0742921491861789.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 1.0424 - accuracy: 0.7982\n",
      "Batch 00134: setting learning rate to 0.07421735346695817.\n",
      "7552/9600 [======================>.......] - ETA: 3s - loss: 1.0406 - accuracy: 0.7994\n",
      "Batch 00135: setting learning rate to 0.07414255774773744.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 1.0382 - accuracy: 0.8005\n",
      "Batch 00136: setting learning rate to 0.07406776202851671.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 1.0360 - accuracy: 0.8014\n",
      "Batch 00137: setting learning rate to 0.07399296630929601.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 1.0357 - accuracy: 0.8015\n",
      "Batch 00138: setting learning rate to 0.07391817059007529.\n",
      "8064/9600 [========================>.....] - ETA: 2s - loss: 1.0335 - accuracy: 0.8030\n",
      "Batch 00139: setting learning rate to 0.07384337487085456.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 1.0308 - accuracy: 0.8042\n",
      "Batch 00140: setting learning rate to 0.07376857915163383.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 1.0296 - accuracy: 0.8044\n",
      "Batch 00141: setting learning rate to 0.0736937834324131.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 1.0270 - accuracy: 0.8054\n",
      "Batch 00142: setting learning rate to 0.07361898771319238.\n",
      "8576/9600 [=========================>....] - ETA: 1s - loss: 1.0258 - accuracy: 0.8060\n",
      "Batch 00143: setting learning rate to 0.07354419199397165.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 1.0228 - accuracy: 0.8077\n",
      "Batch 00144: setting learning rate to 0.07346939627475092.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 1.0202 - accuracy: 0.8088\n",
      "Batch 00145: setting learning rate to 0.07339460055553021.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 1.0186 - accuracy: 0.8094\n",
      "Batch 00146: setting learning rate to 0.0733198048363095.\n",
      "9088/9600 [===========================>..] - ETA: 0s - loss: 1.0169 - accuracy: 0.8100\n",
      "Batch 00147: setting learning rate to 0.07324500911708877.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 1.0155 - accuracy: 0.8112\n",
      "Batch 00148: setting learning rate to 0.07317021339786804.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 1.0127 - accuracy: 0.8126\n",
      "Batch 00149: setting learning rate to 0.07309541767864731.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 1.0113 - accuracy: 0.8133\n",
      "Batch 00150: setting learning rate to 0.07302062195942659.\n",
      "9600/9600 [==============================] - 20s 2ms/step - loss: 1.0105 - accuracy: 0.8140 - val_loss: 19.9549 - val_accuracy: 0.2512\n",
      "Epoch 3/15\n",
      "\n",
      "Batch 00151: setting learning rate to 0.07294582624020586.\n",
      " 128/9600 [..............................] - ETA: 18s - loss: 0.8663 - accuracy: 0.9219\n",
      "Batch 00152: setting learning rate to 0.07287103052098513.\n",
      " 256/9600 [..............................] - ETA: 17s - loss: 0.8997 - accuracy: 0.9023\n",
      "Batch 00153: setting learning rate to 0.0727962348017644.\n",
      " 384/9600 [>.............................] - ETA: 17s - loss: 0.8858 - accuracy: 0.9036\n",
      "Batch 00154: setting learning rate to 0.07272143908254369.\n",
      " 512/9600 [>.............................] - ETA: 16s - loss: 0.8641 - accuracy: 0.9043\n",
      "Batch 00155: setting learning rate to 0.07264664336332298.\n",
      " 640/9600 [=>............................] - ETA: 16s - loss: 0.8641 - accuracy: 0.8953\n",
      "Batch 00156: setting learning rate to 0.07257184764410225.\n",
      " 768/9600 [=>............................] - ETA: 16s - loss: 0.8629 - accuracy: 0.8906\n",
      "Batch 00157: setting learning rate to 0.07249705192488153.\n",
      " 896/9600 [=>............................] - ETA: 15s - loss: 0.8638 - accuracy: 0.8940\n",
      "Batch 00158: setting learning rate to 0.0724222562056608.\n",
      "1024/9600 [==>...........................] - ETA: 15s - loss: 0.8559 - accuracy: 0.8955\n",
      "Batch 00159: setting learning rate to 0.07234746048644007.\n",
      "1152/9600 [==>...........................] - ETA: 15s - loss: 0.8576 - accuracy: 0.8967\n",
      "Batch 00160: setting learning rate to 0.07227266476721934.\n",
      "1280/9600 [===>..........................] - ETA: 15s - loss: 0.8587 - accuracy: 0.8977\n",
      "Batch 00161: setting learning rate to 0.07219786904799862.\n",
      "1408/9600 [===>..........................] - ETA: 14s - loss: 0.8676 - accuracy: 0.8942\n",
      "Batch 00162: setting learning rate to 0.07212307332877789.\n",
      "1536/9600 [===>..........................] - ETA: 14s - loss: 0.8677 - accuracy: 0.8913\n",
      "Batch 00163: setting learning rate to 0.07204827760955718.\n",
      "1664/9600 [====>.........................] - ETA: 14s - loss: 0.8649 - accuracy: 0.8894\n",
      "Batch 00164: setting learning rate to 0.07197348189033646.\n",
      "1792/9600 [====>.........................] - ETA: 14s - loss: 0.8622 - accuracy: 0.8917\n",
      "Batch 00165: setting learning rate to 0.07189868617111574.\n",
      "1920/9600 [=====>........................] - ETA: 14s - loss: 0.8631 - accuracy: 0.8911\n",
      "Batch 00166: setting learning rate to 0.07182389045189501.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048/9600 [=====>........................] - ETA: 13s - loss: 0.8668 - accuracy: 0.8906\n",
      "Batch 00167: setting learning rate to 0.07174909473267428.\n",
      "2176/9600 [=====>........................] - ETA: 13s - loss: 0.8638 - accuracy: 0.8920\n",
      "Batch 00168: setting learning rate to 0.07167429901345355.\n",
      "2304/9600 [======>.......................] - ETA: 13s - loss: 0.8646 - accuracy: 0.8915\n",
      "Batch 00169: setting learning rate to 0.07159950329423283.\n",
      "2432/9600 [======>.......................] - ETA: 13s - loss: 0.8669 - accuracy: 0.8882\n",
      "Batch 00170: setting learning rate to 0.0715247075750121.\n",
      "2560/9600 [=======>......................] - ETA: 13s - loss: 0.8634 - accuracy: 0.8891\n",
      "Batch 00171: setting learning rate to 0.07144991185579139.\n",
      "2688/9600 [=======>......................] - ETA: 12s - loss: 0.8621 - accuracy: 0.8906\n",
      "Batch 00172: setting learning rate to 0.07137511613657066.\n",
      "2816/9600 [=======>......................] - ETA: 12s - loss: 0.8645 - accuracy: 0.8896\n",
      "Batch 00173: setting learning rate to 0.07130032041734995.\n",
      "2944/9600 [========>.....................] - ETA: 12s - loss: 0.8616 - accuracy: 0.8913\n",
      "Batch 00174: setting learning rate to 0.07122552469812922.\n",
      "3072/9600 [========>.....................] - ETA: 12s - loss: 0.8612 - accuracy: 0.8910\n",
      "Batch 00175: setting learning rate to 0.07115072897890849.\n",
      "3200/9600 [=========>....................] - ETA: 11s - loss: 0.8617 - accuracy: 0.8909\n",
      "Batch 00176: setting learning rate to 0.07107593325968777.\n",
      "3328/9600 [=========>....................] - ETA: 11s - loss: 0.8606 - accuracy: 0.8909\n",
      "Batch 00177: setting learning rate to 0.07100113754046704.\n",
      "3456/9600 [=========>....................] - ETA: 11s - loss: 0.8588 - accuracy: 0.8909\n",
      "Batch 00178: setting learning rate to 0.07092634182124631.\n",
      "3584/9600 [==========>...................] - ETA: 11s - loss: 0.8586 - accuracy: 0.8895\n",
      "Batch 00179: setting learning rate to 0.07085154610202558.\n",
      "3712/9600 [==========>...................] - ETA: 10s - loss: 0.8582 - accuracy: 0.8895\n",
      "Batch 00180: setting learning rate to 0.07077675038280488.\n",
      "3840/9600 [===========>..................] - ETA: 10s - loss: 0.8618 - accuracy: 0.8878\n",
      "Batch 00181: setting learning rate to 0.07070195466358416.\n",
      "3968/9600 [===========>..................] - ETA: 10s - loss: 0.8602 - accuracy: 0.8889\n",
      "Batch 00182: setting learning rate to 0.07062715894436343.\n",
      "4096/9600 [===========>..................] - ETA: 10s - loss: 0.8580 - accuracy: 0.8896\n",
      "Batch 00183: setting learning rate to 0.0705523632251427.\n",
      "4224/9600 [============>.................] - ETA: 9s - loss: 0.8587 - accuracy: 0.8885 \n",
      "Batch 00184: setting learning rate to 0.07047756750592198.\n",
      "4352/9600 [============>.................] - ETA: 9s - loss: 0.8573 - accuracy: 0.8886\n",
      "Batch 00185: setting learning rate to 0.07040277178670125.\n",
      "4480/9600 [=============>................] - ETA: 9s - loss: 0.8577 - accuracy: 0.8877\n",
      "Batch 00186: setting learning rate to 0.07032797606748052.\n",
      "4608/9600 [=============>................] - ETA: 9s - loss: 0.8551 - accuracy: 0.8882\n",
      "Batch 00187: setting learning rate to 0.0702531803482598.\n",
      "4736/9600 [=============>................] - ETA: 8s - loss: 0.8547 - accuracy: 0.8889\n",
      "Batch 00188: setting learning rate to 0.07017838462903907.\n",
      "4864/9600 [==============>...............] - ETA: 8s - loss: 0.8531 - accuracy: 0.8894\n",
      "Batch 00189: setting learning rate to 0.07010358890981837.\n",
      "4992/9600 [==============>...............] - ETA: 8s - loss: 0.8514 - accuracy: 0.8900\n",
      "Batch 00190: setting learning rate to 0.07002879319059764.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 0.8525 - accuracy: 0.8904\n",
      "Batch 00191: setting learning rate to 0.06995399747137691.\n",
      "5248/9600 [===============>..............] - ETA: 7s - loss: 0.8509 - accuracy: 0.8912\n",
      "Batch 00192: setting learning rate to 0.06987920175215619.\n",
      "5376/9600 [===============>..............] - ETA: 7s - loss: 0.8497 - accuracy: 0.8914\n",
      "Batch 00193: setting learning rate to 0.06980440603293546.\n",
      "5504/9600 [================>.............] - ETA: 7s - loss: 0.8516 - accuracy: 0.8908\n",
      "Batch 00194: setting learning rate to 0.06972961031371473.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 0.8523 - accuracy: 0.8908\n",
      "Batch 00195: setting learning rate to 0.069654814594494.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.8536 - accuracy: 0.8903\n",
      "Batch 00196: setting learning rate to 0.06958001887527328.\n",
      "5888/9600 [=================>............] - ETA: 6s - loss: 0.8522 - accuracy: 0.8905\n",
      "Batch 00197: setting learning rate to 0.06950522315605256.\n",
      "6016/9600 [=================>............] - ETA: 6s - loss: 0.8512 - accuracy: 0.8908\n",
      "Batch 00198: setting learning rate to 0.06943042743683185.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.8521 - accuracy: 0.8905\n",
      "Batch 00199: setting learning rate to 0.06935563171761112.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.8514 - accuracy: 0.8913\n",
      "Batch 00200: setting learning rate to 0.0692808359983904.\n",
      "6400/9600 [===================>..........] - ETA: 5s - loss: 0.8506 - accuracy: 0.8919\n",
      "Batch 00201: setting learning rate to 0.06920604027916967.\n",
      "6528/9600 [===================>..........] - ETA: 5s - loss: 0.8490 - accuracy: 0.8925\n",
      "Batch 00202: setting learning rate to 0.06913124455994894.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.8481 - accuracy: 0.8929\n",
      "Batch 00203: setting learning rate to 0.06905644884072822.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.8479 - accuracy: 0.8927\n",
      "Batch 00204: setting learning rate to 0.06898165312150749.\n",
      "6912/9600 [====================>.........] - ETA: 4s - loss: 0.8476 - accuracy: 0.8929\n",
      "Batch 00205: setting learning rate to 0.06890685740228676.\n",
      "7040/9600 [=====================>........] - ETA: 4s - loss: 0.8462 - accuracy: 0.8936\n",
      "Batch 00206: setting learning rate to 0.06883206168306605.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.8462 - accuracy: 0.8934\n",
      "Batch 00207: setting learning rate to 0.06875726596384533.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.8445 - accuracy: 0.8941\n",
      "Batch 00208: setting learning rate to 0.06868247024462461.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.8438 - accuracy: 0.8944\n",
      "Batch 00209: setting learning rate to 0.06860767452540388.\n",
      "7552/9600 [======================>.......] - ETA: 3s - loss: 0.8436 - accuracy: 0.8942\n",
      "Batch 00210: setting learning rate to 0.06853287880618315.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.8417 - accuracy: 0.8949\n",
      "Batch 00211: setting learning rate to 0.06845808308696243.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.8411 - accuracy: 0.8951\n",
      "Batch 00212: setting learning rate to 0.0683832873677417.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.8408 - accuracy: 0.8953\n",
      "Batch 00213: setting learning rate to 0.06830849164852097.\n",
      "8064/9600 [========================>.....] - ETA: 2s - loss: 0.8399 - accuracy: 0.8957\n",
      "Batch 00214: setting learning rate to 0.06823369592930024.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.8397 - accuracy: 0.8954\n",
      "Batch 00215: setting learning rate to 0.06815890021007953.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.8389 - accuracy: 0.8954\n",
      "Batch 00216: setting learning rate to 0.06808410449085882.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.8386 - accuracy: 0.8955\n",
      "Batch 00217: setting learning rate to 0.06800930877163809.\n",
      "8576/9600 [=========================>....] - ETA: 1s - loss: 0.8390 - accuracy: 0.8953\n",
      "Batch 00218: setting learning rate to 0.06793451305241736.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.8377 - accuracy: 0.8959\n",
      "Batch 00219: setting learning rate to 0.06785971733319664.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.8370 - accuracy: 0.8963\n",
      "Batch 00220: setting learning rate to 0.06778492161397591.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.8363 - accuracy: 0.8969\n",
      "Batch 00221: setting learning rate to 0.06771012589475518.\n",
      "9088/9600 [===========================>..] - ETA: 0s - loss: 0.8351 - accuracy: 0.8977\n",
      "Batch 00222: setting learning rate to 0.06763533017553446.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.8347 - accuracy: 0.8976\n",
      "Batch 00223: setting learning rate to 0.06756053445631374.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.8357 - accuracy: 0.8970\n",
      "Batch 00224: setting learning rate to 0.06748573873709302.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.8367 - accuracy: 0.8969\n",
      "Batch 00225: setting learning rate to 0.0674109430178723.\n",
      "9600/9600 [==============================] - 19s 2ms/step - loss: 0.8354 - accuracy: 0.8974 - val_loss: 2.3439 - val_accuracy: 0.4867\n",
      "Epoch 4/15\n",
      "\n",
      "Batch 00226: setting learning rate to 0.06733614729865157.\n",
      " 128/9600 [..............................] - ETA: 19s - loss: 0.7844 - accuracy: 0.9141\n",
      "Batch 00227: setting learning rate to 0.06726135157943085.\n",
      " 256/9600 [..............................] - ETA: 18s - loss: 0.7971 - accuracy: 0.9102\n",
      "Batch 00228: setting learning rate to 0.06718655586021012.\n",
      " 384/9600 [>.............................] - ETA: 18s - loss: 0.7927 - accuracy: 0.9089\n",
      "Batch 00229: setting learning rate to 0.0671117601409894.\n",
      " 512/9600 [>.............................] - ETA: 17s - loss: 0.7852 - accuracy: 0.9141\n",
      "Batch 00230: setting learning rate to 0.06703696442176867.\n",
      " 640/9600 [=>............................] - ETA: 17s - loss: 0.7949 - accuracy: 0.9141\n",
      "Batch 00231: setting learning rate to 0.06696216870254794.\n",
      " 768/9600 [=>............................] - ETA: 17s - loss: 0.8077 - accuracy: 0.9102\n",
      "Batch 00232: setting learning rate to 0.06688737298332724.\n",
      " 896/9600 [=>............................] - ETA: 17s - loss: 0.8038 - accuracy: 0.9118\n",
      "Batch 00233: setting learning rate to 0.0668125772641065.\n",
      "1024/9600 [==>...........................] - ETA: 16s - loss: 0.8078 - accuracy: 0.9092\n",
      "Batch 00234: setting learning rate to 0.06673778154488579.\n",
      "1152/9600 [==>...........................] - ETA: 16s - loss: 0.8132 - accuracy: 0.9054\n",
      "Batch 00235: setting learning rate to 0.06666298582566506.\n",
      "1280/9600 [===>..........................] - ETA: 16s - loss: 0.8100 - accuracy: 0.9055\n",
      "Batch 00236: setting learning rate to 0.06658819010644433.\n",
      "1408/9600 [===>..........................] - ETA: 16s - loss: 0.8094 - accuracy: 0.9055\n",
      "Batch 00237: setting learning rate to 0.0665133943872236.\n",
      "1536/9600 [===>..........................] - ETA: 15s - loss: 0.8106 - accuracy: 0.9056\n",
      "Batch 00238: setting learning rate to 0.06643859866800288.\n",
      "1664/9600 [====>.........................] - ETA: 15s - loss: 0.8090 - accuracy: 0.9050\n",
      "Batch 00239: setting learning rate to 0.06636380294878215.\n",
      "1792/9600 [====>.........................] - ETA: 15s - loss: 0.8076 - accuracy: 0.9040\n",
      "Batch 00240: setting learning rate to 0.06628900722956142.\n",
      "1920/9600 [=====>........................] - ETA: 15s - loss: 0.8086 - accuracy: 0.9031\n",
      "Batch 00241: setting learning rate to 0.06621421151034072.\n",
      "2048/9600 [=====>........................] - ETA: 14s - loss: 0.8137 - accuracy: 0.8989\n",
      "Batch 00242: setting learning rate to 0.06613941579111998.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 0.8108 - accuracy: 0.9003\n",
      "Batch 00243: setting learning rate to 0.06606462007189927.\n",
      "2304/9600 [======>.......................] - ETA: 14s - loss: 0.8106 - accuracy: 0.9015\n",
      "Batch 00244: setting learning rate to 0.06598982435267854.\n",
      "2432/9600 [======>.......................] - ETA: 14s - loss: 0.8088 - accuracy: 0.9025\n",
      "Batch 00245: setting learning rate to 0.06591502863345781.\n",
      "2560/9600 [=======>......................] - ETA: 13s - loss: 0.8063 - accuracy: 0.9035\n",
      "Batch 00246: setting learning rate to 0.06584023291423709.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 0.8050 - accuracy: 0.9040\n",
      "Batch 00247: setting learning rate to 0.06576543719501636.\n",
      "2816/9600 [=======>......................] - ETA: 13s - loss: 0.8010 - accuracy: 0.9059\n",
      "Batch 00248: setting learning rate to 0.06569064147579563.\n",
      "2944/9600 [========>.....................] - ETA: 13s - loss: 0.7997 - accuracy: 0.9066\n",
      "Batch 00249: setting learning rate to 0.06561584575657492.\n",
      "3072/9600 [========>.....................] - ETA: 12s - loss: 0.7994 - accuracy: 0.9059\n",
      "Batch 00250: setting learning rate to 0.0655410500373542.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 0.7945 - accuracy: 0.9078\n",
      "Batch 00251: setting learning rate to 0.06546625431813347.\n",
      "3328/9600 [=========>....................] - ETA: 12s - loss: 0.7934 - accuracy: 0.9087\n",
      "Batch 00252: setting learning rate to 0.06539145859891275.\n",
      "3456/9600 [=========>....................] - ETA: 12s - loss: 0.7917 - accuracy: 0.9094\n",
      "Batch 00253: setting learning rate to 0.06531666287969203.\n",
      "3584/9600 [==========>...................] - ETA: 11s - loss: 0.7912 - accuracy: 0.9099\n",
      "Batch 00254: setting learning rate to 0.0652418671604713.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 0.7899 - accuracy: 0.9106\n",
      "Batch 00255: setting learning rate to 0.06516707144125057.\n",
      "3840/9600 [===========>..................] - ETA: 11s - loss: 0.7898 - accuracy: 0.9107\n",
      "Batch 00256: setting learning rate to 0.06509227572202984.\n",
      "3968/9600 [===========>..................] - ETA: 11s - loss: 0.7887 - accuracy: 0.9118\n",
      "Batch 00257: setting learning rate to 0.06501748000280912.\n",
      "4096/9600 [===========>..................] - ETA: 10s - loss: 0.7866 - accuracy: 0.9126\n",
      "Batch 00258: setting learning rate to 0.0649426842835884.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 0.7862 - accuracy: 0.9134\n",
      "Batch 00259: setting learning rate to 0.06486788856436769.\n",
      "4352/9600 [============>.................] - ETA: 10s - loss: 0.7849 - accuracy: 0.9138\n",
      "Batch 00260: setting learning rate to 0.06479309284514695.\n",
      "4480/9600 [=============>................] - ETA: 10s - loss: 0.7834 - accuracy: 0.9143\n",
      "Batch 00261: setting learning rate to 0.06471829712592624.\n",
      "4608/9600 [=============>................] - ETA: 9s - loss: 0.7812 - accuracy: 0.9149 \n",
      "Batch 00262: setting learning rate to 0.06464350140670551.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.7781 - accuracy: 0.9160\n",
      "Batch 00263: setting learning rate to 0.06456870568748478.\n",
      "4864/9600 [==============>...............] - ETA: 9s - loss: 0.7775 - accuracy: 0.9157\n",
      "Batch 00264: setting learning rate to 0.06449390996826405.\n",
      "4992/9600 [==============>...............] - ETA: 9s - loss: 0.7751 - accuracy: 0.9169\n",
      "Batch 00265: setting learning rate to 0.06441911424904333.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 0.7744 - accuracy: 0.9172\n",
      "Batch 00266: setting learning rate to 0.0643443185298226.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.7745 - accuracy: 0.9173\n",
      "Batch 00267: setting learning rate to 0.06426952281060189.\n",
      "5376/9600 [===============>..............] - ETA: 8s - loss: 0.7737 - accuracy: 0.9174\n",
      "Batch 00268: setting learning rate to 0.06419472709138117.\n",
      "5504/9600 [================>.............] - ETA: 8s - loss: 0.7720 - accuracy: 0.9179\n",
      "Batch 00269: setting learning rate to 0.06411993137216043.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 0.7709 - accuracy: 0.9183\n",
      "Batch 00270: setting learning rate to 0.06404513565293972.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.7717 - accuracy: 0.9181\n",
      "Batch 00271: setting learning rate to 0.06397033993371899.\n",
      "5888/9600 [=================>............] - ETA: 7s - loss: 0.7699 - accuracy: 0.9192\n",
      "Batch 00272: setting learning rate to 0.06389554421449826.\n",
      "6016/9600 [=================>............] - ETA: 6s - loss: 0.7700 - accuracy: 0.9192\n",
      "Batch 00273: setting learning rate to 0.06382074849527754.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.7670 - accuracy: 0.9206\n",
      "Batch 00274: setting learning rate to 0.06374595277605681.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.7658 - accuracy: 0.9212\n",
      "Batch 00275: setting learning rate to 0.0636711570568361.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 0.7663 - accuracy: 0.9209\n",
      "Batch 00276: setting learning rate to 0.06359636133761537.\n",
      "6528/9600 [===================>..........] - ETA: 5s - loss: 0.7670 - accuracy: 0.9202\n",
      "Batch 00277: setting learning rate to 0.06352156561839466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.7670 - accuracy: 0.9205\n",
      "Batch 00278: setting learning rate to 0.06344676989917393.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.7673 - accuracy: 0.9207\n",
      "Batch 00279: setting learning rate to 0.0633719741799532.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.7660 - accuracy: 0.9213\n",
      "Batch 00280: setting learning rate to 0.06329717846073248.\n",
      "7040/9600 [=====================>........] - ETA: 4s - loss: 0.7656 - accuracy: 0.9213\n",
      "Batch 00281: setting learning rate to 0.06322238274151175.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.7647 - accuracy: 0.9220\n",
      "Batch 00282: setting learning rate to 0.06314758702229102.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.7643 - accuracy: 0.9219\n",
      "Batch 00283: setting learning rate to 0.0630727913030703.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.7637 - accuracy: 0.9219\n",
      "Batch 00284: setting learning rate to 0.06299799558384958.\n",
      "7552/9600 [======================>.......] - ETA: 3s - loss: 0.7626 - accuracy: 0.9224\n",
      "Batch 00285: setting learning rate to 0.06292319986462885.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.7614 - accuracy: 0.9229\n",
      "Batch 00286: setting learning rate to 0.06284840414540814.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.7608 - accuracy: 0.9233\n",
      "Batch 00287: setting learning rate to 0.06277360842618741.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.7601 - accuracy: 0.9236\n",
      "Batch 00288: setting learning rate to 0.06269881270696669.\n",
      "8064/9600 [========================>.....] - ETA: 3s - loss: 0.7595 - accuracy: 0.9242\n",
      "Batch 00289: setting learning rate to 0.06262401698774596.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.7594 - accuracy: 0.9246\n",
      "Batch 00290: setting learning rate to 0.06254922126852523.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.7601 - accuracy: 0.9243\n",
      "Batch 00291: setting learning rate to 0.06247442554930451.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.7609 - accuracy: 0.9238\n",
      "Batch 00292: setting learning rate to 0.062399629830083785.\n",
      "8576/9600 [=========================>....] - ETA: 2s - loss: 0.7614 - accuracy: 0.9236\n",
      "Batch 00293: setting learning rate to 0.06232483411086306.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.7612 - accuracy: 0.9239\n",
      "Batch 00294: setting learning rate to 0.06225003839164233.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.7612 - accuracy: 0.9240\n",
      "Batch 00295: setting learning rate to 0.062175242672421624.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.7606 - accuracy: 0.9242\n",
      "Batch 00296: setting learning rate to 0.0621004469532009.\n",
      "9088/9600 [===========================>..] - ETA: 1s - loss: 0.7602 - accuracy: 0.9245\n",
      "Batch 00297: setting learning rate to 0.06202565123398017.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.7604 - accuracy: 0.9245\n",
      "Batch 00298: setting learning rate to 0.06195085551475944.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.7601 - accuracy: 0.9244\n",
      "Batch 00299: setting learning rate to 0.06187605979553872.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.7595 - accuracy: 0.9247\n",
      "Batch 00300: setting learning rate to 0.061801264076317995.\n",
      "9600/9600 [==============================] - 20s 2ms/step - loss: 0.7605 - accuracy: 0.9246 - val_loss: 1.7092 - val_accuracy: 0.4729\n",
      "Epoch 5/15\n",
      "\n",
      "Batch 00301: setting learning rate to 0.06172646835709727.\n",
      " 128/9600 [..............................] - ETA: 19s - loss: 0.6514 - accuracy: 0.9766\n",
      "Batch 00302: setting learning rate to 0.061651672637876555.\n",
      " 256/9600 [..............................] - ETA: 18s - loss: 0.7308 - accuracy: 0.9492\n",
      "Batch 00303: setting learning rate to 0.061576876918655814.\n",
      " 384/9600 [>.............................] - ETA: 18s - loss: 0.7289 - accuracy: 0.9505\n",
      "Batch 00304: setting learning rate to 0.06150208119943511.\n",
      " 512/9600 [>.............................] - ETA: 17s - loss: 0.7380 - accuracy: 0.9414\n",
      "Batch 00305: setting learning rate to 0.06142728548021438.\n",
      " 640/9600 [=>............................] - ETA: 17s - loss: 0.7509 - accuracy: 0.9406\n",
      "Batch 00306: setting learning rate to 0.06135248976099365.\n",
      " 768/9600 [=>............................] - ETA: 17s - loss: 0.7360 - accuracy: 0.9479\n",
      "Batch 00307: setting learning rate to 0.061277694041772926.\n",
      " 896/9600 [=>............................] - ETA: 17s - loss: 0.7286 - accuracy: 0.9487\n",
      "Batch 00308: setting learning rate to 0.061202898322552206.\n",
      "1024/9600 [==>...........................] - ETA: 16s - loss: 0.7299 - accuracy: 0.9463\n",
      "Batch 00309: setting learning rate to 0.06112810260333148.\n",
      "1152/9600 [==>...........................] - ETA: 16s - loss: 0.7343 - accuracy: 0.9418\n",
      "Batch 00310: setting learning rate to 0.06105330688411075.\n",
      "1280/9600 [===>..........................] - ETA: 16s - loss: 0.7304 - accuracy: 0.9430\n",
      "Batch 00311: setting learning rate to 0.06097851116489004.\n",
      "1408/9600 [===>..........................] - ETA: 16s - loss: 0.7272 - accuracy: 0.9439\n",
      "Batch 00312: setting learning rate to 0.06090371544566931.\n",
      "1536/9600 [===>..........................] - ETA: 15s - loss: 0.7305 - accuracy: 0.9414\n",
      "Batch 00313: setting learning rate to 0.06082891972644859.\n",
      "1664/9600 [====>.........................] - ETA: 15s - loss: 0.7283 - accuracy: 0.9423\n",
      "Batch 00314: setting learning rate to 0.060754124007227864.\n",
      "1792/9600 [====>.........................] - ETA: 15s - loss: 0.7274 - accuracy: 0.9425\n",
      "Batch 00315: setting learning rate to 0.06067932828800714.\n",
      "1920/9600 [=====>........................] - ETA: 15s - loss: 0.7215 - accuracy: 0.9443\n",
      "Batch 00316: setting learning rate to 0.06060453256878641.\n",
      "2048/9600 [=====>........................] - ETA: 14s - loss: 0.7185 - accuracy: 0.9458\n",
      "Batch 00317: setting learning rate to 0.06052973684956569.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 0.7229 - accuracy: 0.9430\n",
      "Batch 00318: setting learning rate to 0.06045494113034496.\n",
      "2304/9600 [======>.......................] - ETA: 14s - loss: 0.7218 - accuracy: 0.9440\n",
      "Batch 00319: setting learning rate to 0.060380145411124235.\n",
      "2432/9600 [======>.......................] - ETA: 14s - loss: 0.7197 - accuracy: 0.9449\n",
      "Batch 00320: setting learning rate to 0.060305349691903515.\n",
      "2560/9600 [=======>......................] - ETA: 13s - loss: 0.7199 - accuracy: 0.9441\n",
      "Batch 00321: setting learning rate to 0.060230553972682795.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 0.7200 - accuracy: 0.9435\n",
      "Batch 00322: setting learning rate to 0.060155758253462074.\n",
      "2816/9600 [=======>......................] - ETA: 13s - loss: 0.7197 - accuracy: 0.9442\n",
      "Batch 00323: setting learning rate to 0.06008096253424135.\n",
      "2944/9600 [========>.....................] - ETA: 12s - loss: 0.7178 - accuracy: 0.9453\n",
      "Batch 00324: setting learning rate to 0.06000616681502062.\n",
      "3072/9600 [========>.....................] - ETA: 12s - loss: 0.7184 - accuracy: 0.9443\n",
      "Batch 00325: setting learning rate to 0.0599313710957999.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 0.7188 - accuracy: 0.9438\n",
      "Batch 00326: setting learning rate to 0.05985657537657917.\n",
      "3328/9600 [=========>....................] - ETA: 12s - loss: 0.7168 - accuracy: 0.9444\n",
      "Batch 00327: setting learning rate to 0.059781779657358446.\n",
      "3456/9600 [=========>....................] - ETA: 11s - loss: 0.7177 - accuracy: 0.9442\n",
      "Batch 00328: setting learning rate to 0.05970698393813772.\n",
      "3584/9600 [==========>...................] - ETA: 11s - loss: 0.7158 - accuracy: 0.9445\n",
      "Batch 00329: setting learning rate to 0.059632188218917.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 0.7137 - accuracy: 0.9445\n",
      "Batch 00330: setting learning rate to 0.05955739249969628.\n",
      "3840/9600 [===========>..................] - ETA: 11s - loss: 0.7142 - accuracy: 0.9440\n",
      "Batch 00331: setting learning rate to 0.05948259678047556.\n",
      "3968/9600 [===========>..................] - ETA: 10s - loss: 0.7146 - accuracy: 0.9438\n",
      "Batch 00332: setting learning rate to 0.05940780106125483.\n",
      "4096/9600 [===========>..................] - ETA: 10s - loss: 0.7139 - accuracy: 0.9436\n",
      "Batch 00333: setting learning rate to 0.059333005342034104.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 0.7130 - accuracy: 0.9439\n",
      "Batch 00334: setting learning rate to 0.05925820962281338.\n",
      "4352/9600 [============>.................] - ETA: 10s - loss: 0.7115 - accuracy: 0.9444\n",
      "Batch 00335: setting learning rate to 0.059183413903592656.\n",
      "4480/9600 [=============>................] - ETA: 10s - loss: 0.7119 - accuracy: 0.9435\n",
      "Batch 00336: setting learning rate to 0.05910861818437193.\n",
      "4608/9600 [=============>................] - ETA: 9s - loss: 0.7129 - accuracy: 0.9427 \n",
      "Batch 00337: setting learning rate to 0.05903382246515121.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.7124 - accuracy: 0.9428\n",
      "Batch 00338: setting learning rate to 0.05895902674593049.\n",
      "4864/9600 [==============>...............] - ETA: 9s - loss: 0.7112 - accuracy: 0.9435\n",
      "Batch 00339: setting learning rate to 0.05888423102670976.\n",
      "4992/9600 [==============>...............] - ETA: 8s - loss: 0.7108 - accuracy: 0.9439\n",
      "Batch 00340: setting learning rate to 0.05880943530748904.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 0.7098 - accuracy: 0.9449\n",
      "Batch 00341: setting learning rate to 0.058734639588268314.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.7097 - accuracy: 0.9446\n",
      "Batch 00342: setting learning rate to 0.05865984386904759.\n",
      "5376/9600 [===============>..............] - ETA: 8s - loss: 0.7097 - accuracy: 0.9446\n",
      "Batch 00343: setting learning rate to 0.05858504814982687.\n",
      "5504/9600 [================>.............] - ETA: 8s - loss: 0.7090 - accuracy: 0.9451\n",
      "Batch 00344: setting learning rate to 0.05851025243060614.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 0.7080 - accuracy: 0.9455\n",
      "Batch 00345: setting learning rate to 0.05843545671138541.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.7091 - accuracy: 0.9444\n",
      "Batch 00346: setting learning rate to 0.05836066099216469.\n",
      "5888/9600 [=================>............] - ETA: 7s - loss: 0.7090 - accuracy: 0.9440\n",
      "Batch 00347: setting learning rate to 0.05828586527294397.\n",
      "6016/9600 [=================>............] - ETA: 7s - loss: 0.7100 - accuracy: 0.9430\n",
      "Batch 00348: setting learning rate to 0.05821106955372325.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.7104 - accuracy: 0.9425\n",
      "Batch 00349: setting learning rate to 0.058136273834502525.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.7096 - accuracy: 0.9428\n",
      "Batch 00350: setting learning rate to 0.0580614781152818.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 0.7082 - accuracy: 0.9434\n",
      "Batch 00351: setting learning rate to 0.05798668239606108.\n",
      "6528/9600 [===================>..........] - ETA: 6s - loss: 0.7091 - accuracy: 0.9430\n",
      "Batch 00352: setting learning rate to 0.05791188667684035.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.7087 - accuracy: 0.9432\n",
      "Batch 00353: setting learning rate to 0.05783709095761962.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.7075 - accuracy: 0.9440\n",
      "Batch 00354: setting learning rate to 0.057762295238398896.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.7081 - accuracy: 0.9439\n",
      "Batch 00355: setting learning rate to 0.057687499519178176.\n",
      "7040/9600 [=====================>........] - ETA: 5s - loss: 0.7079 - accuracy: 0.9440\n",
      "Batch 00356: setting learning rate to 0.057612703799957456.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.7079 - accuracy: 0.9436\n",
      "Batch 00357: setting learning rate to 0.057537908080736735.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.7079 - accuracy: 0.9433\n",
      "Batch 00358: setting learning rate to 0.05746311236151601.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.7068 - accuracy: 0.9437\n",
      "Batch 00359: setting learning rate to 0.05738831664229528.\n",
      "7552/9600 [======================>.......] - ETA: 4s - loss: 0.7059 - accuracy: 0.9439\n",
      "Batch 00360: setting learning rate to 0.05731352092307456.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.7064 - accuracy: 0.9438\n",
      "Batch 00361: setting learning rate to 0.057238725203853834.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.7053 - accuracy: 0.9444\n",
      "Batch 00362: setting learning rate to 0.05716392948463311.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.7053 - accuracy: 0.9443\n",
      "Batch 00363: setting learning rate to 0.05708913376541238.\n",
      "8064/9600 [========================>.....] - ETA: 3s - loss: 0.7051 - accuracy: 0.9441\n",
      "Batch 00364: setting learning rate to 0.057014338046191666.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.7049 - accuracy: 0.9442\n",
      "Batch 00365: setting learning rate to 0.05693954232697094.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.7049 - accuracy: 0.9441\n",
      "Batch 00366: setting learning rate to 0.05686474660775022.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.7057 - accuracy: 0.9441\n",
      "Batch 00367: setting learning rate to 0.05678995088852949.\n",
      "8576/9600 [=========================>....] - ETA: 2s - loss: 0.7065 - accuracy: 0.9439\n",
      "Batch 00368: setting learning rate to 0.056715155169308765.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.7061 - accuracy: 0.9439\n",
      "Batch 00369: setting learning rate to 0.056640359450088044.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.7051 - accuracy: 0.9444\n",
      "Batch 00370: setting learning rate to 0.05656556373086732.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.7047 - accuracy: 0.9444\n",
      "Batch 00371: setting learning rate to 0.05649076801164659.\n",
      "9088/9600 [===========================>..] - ETA: 1s - loss: 0.7051 - accuracy: 0.9442\n",
      "Batch 00372: setting learning rate to 0.05641597229242587.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.7052 - accuracy: 0.9440\n",
      "Batch 00373: setting learning rate to 0.05634117657320515.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.7048 - accuracy: 0.9439\n",
      "Batch 00374: setting learning rate to 0.05626638085398442.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.7057 - accuracy: 0.9437\n",
      "Batch 00375: setting learning rate to 0.0561915851347637.\n",
      "9600/9600 [==============================] - 20s 2ms/step - loss: 0.7047 - accuracy: 0.9442 - val_loss: 1.0540 - val_accuracy: 0.6517\n",
      "Epoch 6/15\n",
      "\n",
      "Batch 00376: setting learning rate to 0.056116789415542975.\n",
      " 128/9600 [..............................] - ETA: 18s - loss: 0.6416 - accuracy: 0.9844\n",
      "Batch 00377: setting learning rate to 0.056041993696322255.\n",
      " 256/9600 [..............................] - ETA: 18s - loss: 0.6725 - accuracy: 0.9609\n",
      "Batch 00378: setting learning rate to 0.05596719797710153.\n",
      " 384/9600 [>.............................] - ETA: 17s - loss: 0.6787 - accuracy: 0.9583\n",
      "Batch 00379: setting learning rate to 0.0558924022578808.\n",
      " 512/9600 [>.............................] - ETA: 17s - loss: 0.7019 - accuracy: 0.9492\n",
      "Batch 00380: setting learning rate to 0.055817606538660074.\n",
      " 640/9600 [=>............................] - ETA: 17s - loss: 0.6915 - accuracy: 0.9516\n",
      "Batch 00381: setting learning rate to 0.05574281081943935.\n",
      " 768/9600 [=>............................] - ETA: 16s - loss: 0.6980 - accuracy: 0.9479\n",
      "Batch 00382: setting learning rate to 0.05566801510021863.\n",
      " 896/9600 [=>............................] - ETA: 16s - loss: 0.6969 - accuracy: 0.9464\n",
      "Batch 00383: setting learning rate to 0.05559321938099791.\n",
      "1024/9600 [==>...........................] - ETA: 16s - loss: 0.6882 - accuracy: 0.9492\n",
      "Batch 00384: setting learning rate to 0.055518423661777186.\n",
      "1152/9600 [==>...........................] - ETA: 16s - loss: 0.6887 - accuracy: 0.9505\n",
      "Batch 00385: setting learning rate to 0.05544362794255646.\n",
      "1280/9600 [===>..........................] - ETA: 16s - loss: 0.6850 - accuracy: 0.9516\n",
      "Batch 00386: setting learning rate to 0.05536883222333574.\n",
      "1408/9600 [===>..........................] - ETA: 15s - loss: 0.6852 - accuracy: 0.9524\n",
      "Batch 00387: setting learning rate to 0.05529403650411501.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536/9600 [===>..........................] - ETA: 15s - loss: 0.6840 - accuracy: 0.9518\n",
      "Batch 00388: setting learning rate to 0.055219240784894284.\n",
      "1664/9600 [====>.........................] - ETA: 15s - loss: 0.6800 - accuracy: 0.9531\n",
      "Batch 00389: setting learning rate to 0.05514444506567356.\n",
      "1792/9600 [====>.........................] - ETA: 15s - loss: 0.6814 - accuracy: 0.9515\n",
      "Batch 00390: setting learning rate to 0.055069649346452844.\n",
      "1920/9600 [=====>........................] - ETA: 14s - loss: 0.6830 - accuracy: 0.9521\n",
      "Batch 00391: setting learning rate to 0.05499485362723212.\n",
      "2048/9600 [=====>........................] - ETA: 14s - loss: 0.6841 - accuracy: 0.9526\n",
      "Batch 00392: setting learning rate to 0.054920057908011397.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 0.6854 - accuracy: 0.9522\n",
      "Batch 00393: setting learning rate to 0.05484526218879067.\n",
      "2304/9600 [======>.......................] - ETA: 14s - loss: 0.6833 - accuracy: 0.9536\n",
      "Batch 00394: setting learning rate to 0.05477046646956994.\n",
      "2432/9600 [======>.......................] - ETA: 13s - loss: 0.6844 - accuracy: 0.9527\n",
      "Batch 00395: setting learning rate to 0.05469567075034922.\n",
      "2560/9600 [=======>......................] - ETA: 13s - loss: 0.6814 - accuracy: 0.9539\n",
      "Batch 00396: setting learning rate to 0.054620875031128495.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 0.6820 - accuracy: 0.9539\n",
      "Batch 00397: setting learning rate to 0.05454607931190777.\n",
      "2816/9600 [=======>......................] - ETA: 12s - loss: 0.6814 - accuracy: 0.9542\n",
      "Batch 00398: setting learning rate to 0.05447128359268704.\n",
      "2944/9600 [========>.....................] - ETA: 12s - loss: 0.6786 - accuracy: 0.9552\n",
      "Batch 00399: setting learning rate to 0.05439648787346633.\n",
      "3072/9600 [========>.....................] - ETA: 12s - loss: 0.6796 - accuracy: 0.9554\n",
      "Batch 00400: setting learning rate to 0.0543216921542456.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 0.6781 - accuracy: 0.9559\n",
      "Batch 00401: setting learning rate to 0.05424689643502488.\n",
      "3328/9600 [=========>....................] - ETA: 11s - loss: 0.6749 - accuracy: 0.9573\n",
      "Batch 00402: setting learning rate to 0.05417210071580415.\n",
      "3456/9600 [=========>....................] - ETA: 11s - loss: 0.6743 - accuracy: 0.9572\n",
      "Batch 00403: setting learning rate to 0.05409730499658343.\n",
      "3584/9600 [==========>...................] - ETA: 11s - loss: 0.6738 - accuracy: 0.9568\n",
      "Batch 00404: setting learning rate to 0.054022509277362706.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 0.6724 - accuracy: 0.9572\n",
      "Batch 00405: setting learning rate to 0.05394771355814198.\n",
      "3840/9600 [===========>..................] - ETA: 11s - loss: 0.6703 - accuracy: 0.9583\n",
      "Batch 00406: setting learning rate to 0.05387291783892125.\n",
      "3968/9600 [===========>..................] - ETA: 10s - loss: 0.6725 - accuracy: 0.9577\n",
      "Batch 00407: setting learning rate to 0.05379812211970053.\n",
      "4096/9600 [===========>..................] - ETA: 10s - loss: 0.6709 - accuracy: 0.9587\n",
      "Batch 00408: setting learning rate to 0.05372332640047981.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 0.6714 - accuracy: 0.9579\n",
      "Batch 00409: setting learning rate to 0.053648530681259084.\n",
      "4352/9600 [============>.................] - ETA: 10s - loss: 0.6713 - accuracy: 0.9580\n",
      "Batch 00410: setting learning rate to 0.05357373496203836.\n",
      "4480/9600 [=============>................] - ETA: 9s - loss: 0.6715 - accuracy: 0.9578 \n",
      "Batch 00411: setting learning rate to 0.053498939242817636.\n",
      "4608/9600 [=============>................] - ETA: 9s - loss: 0.6704 - accuracy: 0.9579\n",
      "Batch 00412: setting learning rate to 0.053424143523596916.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.6717 - accuracy: 0.9567\n",
      "Batch 00413: setting learning rate to 0.05334934780437619.\n",
      "4864/9600 [==============>...............] - ETA: 9s - loss: 0.6710 - accuracy: 0.9568\n",
      "Batch 00414: setting learning rate to 0.05327455208515546.\n",
      "4992/9600 [==============>...............] - ETA: 8s - loss: 0.6717 - accuracy: 0.9563\n",
      "Batch 00415: setting learning rate to 0.05319975636593474.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 0.6714 - accuracy: 0.9563\n",
      "Batch 00416: setting learning rate to 0.05312496064671402.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.6702 - accuracy: 0.9569\n",
      "Batch 00417: setting learning rate to 0.053050164927493294.\n",
      "5376/9600 [===============>..............] - ETA: 8s - loss: 0.6710 - accuracy: 0.9565\n",
      "Batch 00418: setting learning rate to 0.05297536920827257.\n",
      "5504/9600 [================>.............] - ETA: 7s - loss: 0.6723 - accuracy: 0.9564\n",
      "Batch 00419: setting learning rate to 0.05290057348905185.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 0.6740 - accuracy: 0.9554\n",
      "Batch 00420: setting learning rate to 0.05282577776983112.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.6745 - accuracy: 0.9552\n",
      "Batch 00421: setting learning rate to 0.0527509820506104.\n",
      "5888/9600 [=================>............] - ETA: 7s - loss: 0.6735 - accuracy: 0.9557\n",
      "Batch 00422: setting learning rate to 0.05267618633138967.\n",
      "6016/9600 [=================>............] - ETA: 6s - loss: 0.6728 - accuracy: 0.9555\n",
      "Batch 00423: setting learning rate to 0.052601390612168945.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.6727 - accuracy: 0.9554\n",
      "Batch 00424: setting learning rate to 0.052526594892948225.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.6720 - accuracy: 0.9555\n",
      "Batch 00425: setting learning rate to 0.052451799173727505.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 0.6720 - accuracy: 0.9555\n",
      "Batch 00426: setting learning rate to 0.05237700345450678.\n",
      "6528/9600 [===================>..........] - ETA: 5s - loss: 0.6721 - accuracy: 0.9554\n",
      "Batch 00427: setting learning rate to 0.05230220773528605.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.6723 - accuracy: 0.9554\n",
      "Batch 00428: setting learning rate to 0.05222741201606533.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.6716 - accuracy: 0.9556\n",
      "Batch 00429: setting learning rate to 0.05215261629684461.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.6707 - accuracy: 0.9560\n",
      "Batch 00430: setting learning rate to 0.05207782057762388.\n",
      "7040/9600 [=====================>........] - ETA: 4s - loss: 0.6700 - accuracy: 0.9560\n",
      "Batch 00431: setting learning rate to 0.052003024858403156.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.6703 - accuracy: 0.9559\n",
      "Batch 00432: setting learning rate to 0.05192822913918243.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.6699 - accuracy: 0.9561\n",
      "Batch 00433: setting learning rate to 0.05185343341996171.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.6702 - accuracy: 0.9562\n",
      "Batch 00434: setting learning rate to 0.05177863770074099.\n",
      "7552/9600 [======================>.......] - ETA: 3s - loss: 0.6697 - accuracy: 0.9563\n",
      "Batch 00435: setting learning rate to 0.05170384198152026.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.6701 - accuracy: 0.9563\n",
      "Batch 00436: setting learning rate to 0.051629046262299534.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.6695 - accuracy: 0.9565\n",
      "Batch 00437: setting learning rate to 0.051554250543078814.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.6699 - accuracy: 0.9563\n",
      "Batch 00438: setting learning rate to 0.051479454823858094.\n",
      "8064/9600 [========================>.....] - ETA: 2s - loss: 0.6705 - accuracy: 0.9557\n",
      "Batch 00439: setting learning rate to 0.05140465910463737.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.6709 - accuracy: 0.9554\n",
      "Batch 00440: setting learning rate to 0.05132986338541664.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.6706 - accuracy: 0.9555\n",
      "Batch 00441: setting learning rate to 0.05125506766619592.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.6713 - accuracy: 0.9555\n",
      "Batch 00442: setting learning rate to 0.0511802719469752.\n",
      "8576/9600 [=========================>....] - ETA: 1s - loss: 0.6718 - accuracy: 0.9552\n",
      "Batch 00443: setting learning rate to 0.05110547622775447.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.6723 - accuracy: 0.9552\n",
      "Batch 00444: setting learning rate to 0.051030680508533745.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.6716 - accuracy: 0.9555\n",
      "Batch 00445: setting learning rate to 0.05095588478931302.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.6710 - accuracy: 0.9556\n",
      "Batch 00446: setting learning rate to 0.0508810890700923.\n",
      "9088/9600 [===========================>..] - ETA: 1s - loss: 0.6719 - accuracy: 0.9552\n",
      "Batch 00447: setting learning rate to 0.05080629335087158.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.6719 - accuracy: 0.9551\n",
      "Batch 00448: setting learning rate to 0.05073149763165085.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.6711 - accuracy: 0.9554\n",
      "Batch 00449: setting learning rate to 0.05065670191243012.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.6704 - accuracy: 0.9557\n",
      "Batch 00450: setting learning rate to 0.0505819061932094.\n",
      "9600/9600 [==============================] - 20s 2ms/step - loss: 0.6698 - accuracy: 0.9560 - val_loss: 0.4852 - val_accuracy: 0.9108\n",
      "Epoch 7/15\n",
      "\n",
      "Batch 00451: setting learning rate to 0.05050711047398868.\n",
      " 128/9600 [..............................] - ETA: 19s - loss: 0.7234 - accuracy: 0.9219\n",
      "Batch 00452: setting learning rate to 0.050432314754767955.\n",
      " 256/9600 [..............................] - ETA: 18s - loss: 0.6675 - accuracy: 0.9570\n",
      "Batch 00453: setting learning rate to 0.05035751903554723.\n",
      " 384/9600 [>.............................] - ETA: 18s - loss: 0.6793 - accuracy: 0.9505\n",
      "Batch 00454: setting learning rate to 0.05028272331632651.\n",
      " 512/9600 [>.............................] - ETA: 17s - loss: 0.6821 - accuracy: 0.9453\n",
      "Batch 00455: setting learning rate to 0.05020792759710579.\n",
      " 640/9600 [=>............................] - ETA: 17s - loss: 0.6793 - accuracy: 0.9453\n",
      "Batch 00456: setting learning rate to 0.05013313187788506.\n",
      " 768/9600 [=>............................] - ETA: 17s - loss: 0.6719 - accuracy: 0.9479\n",
      "Batch 00457: setting learning rate to 0.050058336158664334.\n",
      " 896/9600 [=>............................] - ETA: 17s - loss: 0.6669 - accuracy: 0.9487\n",
      "Batch 00458: setting learning rate to 0.049983540439443606.\n",
      "1024/9600 [==>...........................] - ETA: 16s - loss: 0.6641 - accuracy: 0.9492\n",
      "Batch 00459: setting learning rate to 0.049908744720222886.\n",
      "1152/9600 [==>...........................] - ETA: 16s - loss: 0.6616 - accuracy: 0.9514\n",
      "Batch 00460: setting learning rate to 0.049833949001002166.\n",
      "1280/9600 [===>..........................] - ETA: 16s - loss: 0.6599 - accuracy: 0.9523\n",
      "Batch 00461: setting learning rate to 0.04975915328178144.\n",
      "1408/9600 [===>..........................] - ETA: 16s - loss: 0.6618 - accuracy: 0.9524\n",
      "Batch 00462: setting learning rate to 0.04968435756256071.\n",
      "1536/9600 [===>..........................] - ETA: 15s - loss: 0.6611 - accuracy: 0.9525\n",
      "Batch 00463: setting learning rate to 0.04960956184333999.\n",
      "1664/9600 [====>.........................] - ETA: 15s - loss: 0.6642 - accuracy: 0.9513\n",
      "Batch 00464: setting learning rate to 0.04953476612411927.\n",
      "1792/9600 [====>.........................] - ETA: 15s - loss: 0.6650 - accuracy: 0.9515\n",
      "Batch 00465: setting learning rate to 0.049459970404898544.\n",
      "1920/9600 [=====>........................] - ETA: 15s - loss: 0.6638 - accuracy: 0.9521\n",
      "Batch 00466: setting learning rate to 0.04938517468567782.\n",
      "2048/9600 [=====>........................] - ETA: 14s - loss: 0.6650 - accuracy: 0.9507\n",
      "Batch 00467: setting learning rate to 0.04931037896645709.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 0.6637 - accuracy: 0.9517\n",
      "Batch 00468: setting learning rate to 0.04923558324723638.\n",
      "2304/9600 [======>.......................] - ETA: 14s - loss: 0.6636 - accuracy: 0.9510\n",
      "Batch 00469: setting learning rate to 0.04916078752801565.\n",
      "2432/9600 [======>.......................] - ETA: 14s - loss: 0.6602 - accuracy: 0.9535\n",
      "Batch 00470: setting learning rate to 0.04908599180879492.\n",
      "2560/9600 [=======>......................] - ETA: 13s - loss: 0.6596 - accuracy: 0.9543\n",
      "Batch 00471: setting learning rate to 0.049011196089574195.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 0.6585 - accuracy: 0.9546\n",
      "Batch 00472: setting learning rate to 0.048936400370353475.\n",
      "2816/9600 [=======>......................] - ETA: 13s - loss: 0.6589 - accuracy: 0.9538\n",
      "Batch 00473: setting learning rate to 0.048861604651132755.\n",
      "2944/9600 [========>.....................] - ETA: 13s - loss: 0.6587 - accuracy: 0.9548\n",
      "Batch 00474: setting learning rate to 0.04878680893191203.\n",
      "3072/9600 [========>.....................] - ETA: 12s - loss: 0.6605 - accuracy: 0.9541\n",
      "Batch 00475: setting learning rate to 0.0487120132126913.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 0.6588 - accuracy: 0.9547\n",
      "Batch 00476: setting learning rate to 0.04863721749347058.\n",
      "3328/9600 [=========>....................] - ETA: 12s - loss: 0.6606 - accuracy: 0.9537\n",
      "Batch 00477: setting learning rate to 0.04856242177424986.\n",
      "3456/9600 [=========>....................] - ETA: 12s - loss: 0.6593 - accuracy: 0.9540\n",
      "Batch 00478: setting learning rate to 0.04848762605502913.\n",
      "3584/9600 [==========>...................] - ETA: 11s - loss: 0.6563 - accuracy: 0.9554\n",
      "Batch 00479: setting learning rate to 0.048412830335808406.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 0.6557 - accuracy: 0.9553\n",
      "Batch 00480: setting learning rate to 0.04833803461658768.\n",
      "3840/9600 [===========>..................] - ETA: 11s - loss: 0.6551 - accuracy: 0.9555\n",
      "Batch 00481: setting learning rate to 0.04826323889736696.\n",
      "3968/9600 [===========>..................] - ETA: 11s - loss: 0.6544 - accuracy: 0.9556\n",
      "Batch 00482: setting learning rate to 0.04818844317814624.\n",
      "4096/9600 [===========>..................] - ETA: 10s - loss: 0.6550 - accuracy: 0.9556\n",
      "Batch 00483: setting learning rate to 0.04811364745892551.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 0.6545 - accuracy: 0.9557\n",
      "Batch 00484: setting learning rate to 0.048038851739704784.\n",
      "4352/9600 [============>.................] - ETA: 10s - loss: 0.6524 - accuracy: 0.9570\n",
      "Batch 00485: setting learning rate to 0.047964056020484064.\n",
      "4480/9600 [=============>................] - ETA: 10s - loss: 0.6531 - accuracy: 0.9567\n",
      "Batch 00486: setting learning rate to 0.047889260301263344.\n",
      "4608/9600 [=============>................] - ETA: 9s - loss: 0.6532 - accuracy: 0.9572 \n",
      "Batch 00487: setting learning rate to 0.047814464582042616.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.6525 - accuracy: 0.9576\n",
      "Batch 00488: setting learning rate to 0.04773966886282189.\n",
      "4864/9600 [==============>...............] - ETA: 9s - loss: 0.6510 - accuracy: 0.9585\n",
      "Batch 00489: setting learning rate to 0.04766487314360117.\n",
      "4992/9600 [==============>...............] - ETA: 9s - loss: 0.6507 - accuracy: 0.9583\n",
      "Batch 00490: setting learning rate to 0.04759007742438045.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 0.6508 - accuracy: 0.9586\n",
      "Batch 00491: setting learning rate to 0.04751528170515972.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.6518 - accuracy: 0.9588\n",
      "Batch 00492: setting learning rate to 0.047440485985938995.\n",
      "5376/9600 [===============>..............] - ETA: 8s - loss: 0.6519 - accuracy: 0.9591\n",
      "Batch 00493: setting learning rate to 0.04736569026671827.\n",
      "5504/9600 [================>.............] - ETA: 8s - loss: 0.6512 - accuracy: 0.9595\n",
      "Batch 00494: setting learning rate to 0.04729089454749755.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 0.6505 - accuracy: 0.9599\n",
      "Batch 00495: setting learning rate to 0.04721609882827683.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.6505 - accuracy: 0.9597\n",
      "Batch 00496: setting learning rate to 0.0471413031090561.\n",
      "5888/9600 [=================>............] - ETA: 7s - loss: 0.6525 - accuracy: 0.9591\n",
      "Batch 00497: setting learning rate to 0.04706650738983537.\n",
      "6016/9600 [=================>............] - ETA: 7s - loss: 0.6537 - accuracy: 0.9586\n",
      "Batch 00498: setting learning rate to 0.04699171167061465.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.6527 - accuracy: 0.9590\n",
      "Batch 00499: setting learning rate to 0.04691691595139393.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.6523 - accuracy: 0.9593\n",
      "Batch 00500: setting learning rate to 0.046842120232173205.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 0.6520 - accuracy: 0.9597\n",
      "Batch 00501: setting learning rate to 0.04676732451295248.\n",
      "6528/9600 [===================>..........] - ETA: 6s - loss: 0.6513 - accuracy: 0.9600\n",
      "Batch 00502: setting learning rate to 0.04669252879373175.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.6506 - accuracy: 0.9603\n",
      "Batch 00503: setting learning rate to 0.04661773307451104.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.6511 - accuracy: 0.9602\n",
      "Batch 00504: setting learning rate to 0.04654293735529031.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.6505 - accuracy: 0.9606\n",
      "Batch 00505: setting learning rate to 0.04646814163606958.\n",
      "7040/9600 [=====================>........] - ETA: 5s - loss: 0.6512 - accuracy: 0.9602\n",
      "Batch 00506: setting learning rate to 0.046393345916848856.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.6518 - accuracy: 0.9600\n",
      "Batch 00507: setting learning rate to 0.046318550197628136.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.6514 - accuracy: 0.9603\n",
      "Batch 00508: setting learning rate to 0.046243754478407416.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.6510 - accuracy: 0.9607\n",
      "Batch 00509: setting learning rate to 0.04616895875918669.\n",
      "7552/9600 [======================>.......] - ETA: 4s - loss: 0.6507 - accuracy: 0.9608\n",
      "Batch 00510: setting learning rate to 0.04609416303996596.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.6499 - accuracy: 0.9611\n",
      "Batch 00511: setting learning rate to 0.04601936732074524.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.6503 - accuracy: 0.9611\n",
      "Batch 00512: setting learning rate to 0.04594457160152452.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.6508 - accuracy: 0.9609\n",
      "Batch 00513: setting learning rate to 0.045869775882303794.\n",
      "8064/9600 [========================>.....] - ETA: 3s - loss: 0.6513 - accuracy: 0.9611\n",
      "Batch 00514: setting learning rate to 0.04579498016308307.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.6509 - accuracy: 0.9609\n",
      "Batch 00515: setting learning rate to 0.04572018444386234.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.6509 - accuracy: 0.9607\n",
      "Batch 00516: setting learning rate to 0.045645388724641626.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.6506 - accuracy: 0.9611\n",
      "Batch 00517: setting learning rate to 0.0455705930054209.\n",
      "8576/9600 [=========================>....] - ETA: 2s - loss: 0.6515 - accuracy: 0.9606\n",
      "Batch 00518: setting learning rate to 0.04549579728620017.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.6511 - accuracy: 0.9607\n",
      "Batch 00519: setting learning rate to 0.045421001566979445.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.6505 - accuracy: 0.9609\n",
      "Batch 00520: setting learning rate to 0.045346205847758725.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.6509 - accuracy: 0.9607\n",
      "Batch 00521: setting learning rate to 0.045271410128538005.\n",
      "9088/9600 [===========================>..] - ETA: 1s - loss: 0.6519 - accuracy: 0.9606\n",
      "Batch 00522: setting learning rate to 0.04519661440931728.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.6518 - accuracy: 0.9606\n",
      "Batch 00523: setting learning rate to 0.04512181869009655.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.6517 - accuracy: 0.9607\n",
      "Batch 00524: setting learning rate to 0.04504702297087583.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.6520 - accuracy: 0.9605\n",
      "Batch 00525: setting learning rate to 0.04497222725165511.\n",
      "9600/9600 [==============================] - 20s 2ms/step - loss: 0.6523 - accuracy: 0.9602 - val_loss: 0.3871 - val_accuracy: 0.9442\n",
      "Epoch 8/15\n",
      "\n",
      "Batch 00526: setting learning rate to 0.04489743153243438.\n",
      " 128/9600 [..............................] - ETA: 20s - loss: 0.6316 - accuracy: 0.9375\n",
      "Batch 00527: setting learning rate to 0.044822635813213656.\n",
      " 256/9600 [..............................] - ETA: 19s - loss: 0.6381 - accuracy: 0.9453\n",
      "Batch 00528: setting learning rate to 0.04474784009399293.\n",
      " 384/9600 [>.............................] - ETA: 18s - loss: 0.6181 - accuracy: 0.9635\n",
      "Batch 00529: setting learning rate to 0.044673044374772215.\n",
      " 512/9600 [>.............................] - ETA: 18s - loss: 0.6296 - accuracy: 0.9609\n",
      "Batch 00530: setting learning rate to 0.04459824865555149.\n",
      " 640/9600 [=>............................] - ETA: 18s - loss: 0.6234 - accuracy: 0.9641\n",
      "Batch 00531: setting learning rate to 0.04452345293633076.\n",
      " 768/9600 [=>............................] - ETA: 17s - loss: 0.6211 - accuracy: 0.9688\n",
      "Batch 00532: setting learning rate to 0.044448657217110034.\n",
      " 896/9600 [=>............................] - ETA: 17s - loss: 0.6298 - accuracy: 0.9643\n",
      "Batch 00533: setting learning rate to 0.044373861497889314.\n",
      "1024/9600 [==>...........................] - ETA: 17s - loss: 0.6305 - accuracy: 0.9629\n",
      "Batch 00534: setting learning rate to 0.04429906577866859.\n",
      "1152/9600 [==>...........................] - ETA: 16s - loss: 0.6335 - accuracy: 0.9635\n",
      "Batch 00535: setting learning rate to 0.044224270059447866.\n",
      "1280/9600 [===>..........................] - ETA: 16s - loss: 0.6310 - accuracy: 0.9656\n",
      "Batch 00536: setting learning rate to 0.04414947434022714.\n",
      "1408/9600 [===>..........................] - ETA: 16s - loss: 0.6311 - accuracy: 0.9659\n",
      "Batch 00537: setting learning rate to 0.04407467862100641.\n",
      "1536/9600 [===>..........................] - ETA: 15s - loss: 0.6360 - accuracy: 0.9655\n",
      "Batch 00538: setting learning rate to 0.0439998829017857.\n",
      "1664/9600 [====>.........................] - ETA: 15s - loss: 0.6390 - accuracy: 0.9651\n",
      "Batch 00539: setting learning rate to 0.04392508718256497.\n",
      "1792/9600 [====>.........................] - ETA: 15s - loss: 0.6367 - accuracy: 0.9671\n",
      "Batch 00540: setting learning rate to 0.043850291463344244.\n",
      "1920/9600 [=====>........................] - ETA: 15s - loss: 0.6356 - accuracy: 0.9667\n",
      "Batch 00541: setting learning rate to 0.04377549574412352.\n",
      "2048/9600 [=====>........................] - ETA: 14s - loss: 0.6362 - accuracy: 0.9663\n",
      "Batch 00542: setting learning rate to 0.043700700024902804.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 0.6363 - accuracy: 0.9660\n",
      "Batch 00543: setting learning rate to 0.04362590430568208.\n",
      "2304/9600 [======>.......................] - ETA: 14s - loss: 0.6365 - accuracy: 0.9657\n",
      "Batch 00544: setting learning rate to 0.04355110858646135.\n",
      "2432/9600 [======>.......................] - ETA: 13s - loss: 0.6357 - accuracy: 0.9655\n",
      "Batch 00545: setting learning rate to 0.04347631286724062.\n",
      "2560/9600 [=======>......................] - ETA: 13s - loss: 0.6382 - accuracy: 0.9648\n",
      "Batch 00546: setting learning rate to 0.0434015171480199.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 0.6369 - accuracy: 0.9658\n",
      "Batch 00547: setting learning rate to 0.04332672142879918.\n",
      "2816/9600 [=======>......................] - ETA: 13s - loss: 0.6375 - accuracy: 0.9652\n",
      "Batch 00548: setting learning rate to 0.043251925709578455.\n",
      "2944/9600 [========>.....................] - ETA: 12s - loss: 0.6366 - accuracy: 0.9657\n",
      "Batch 00549: setting learning rate to 0.04317712999035773.\n",
      "3072/9600 [========>.....................] - ETA: 12s - loss: 0.6367 - accuracy: 0.9658\n",
      "Batch 00550: setting learning rate to 0.043102334271137.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 0.6353 - accuracy: 0.9663\n",
      "Batch 00551: setting learning rate to 0.04302753855191629.\n",
      "3328/9600 [=========>....................] - ETA: 11s - loss: 0.6335 - accuracy: 0.9669\n",
      "Batch 00552: setting learning rate to 0.04295274283269556.\n",
      "3456/9600 [=========>....................] - ETA: 11s - loss: 0.6335 - accuracy: 0.9667\n",
      "Batch 00553: setting learning rate to 0.04287794711347483.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3584/9600 [==========>...................] - ETA: 11s - loss: 0.6358 - accuracy: 0.9657\n",
      "Batch 00554: setting learning rate to 0.042803151394254106.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 0.6368 - accuracy: 0.9652\n",
      "Batch 00555: setting learning rate to 0.04272835567503339.\n",
      "3840/9600 [===========>..................] - ETA: 10s - loss: 0.6361 - accuracy: 0.9651\n",
      "Batch 00556: setting learning rate to 0.042653559955812666.\n",
      "3968/9600 [===========>..................] - ETA: 10s - loss: 0.6360 - accuracy: 0.9652\n",
      "Batch 00557: setting learning rate to 0.04257876423659194.\n",
      "4096/9600 [===========>..................] - ETA: 10s - loss: 0.6365 - accuracy: 0.9646\n",
      "Batch 00558: setting learning rate to 0.04250396851737121.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 0.6364 - accuracy: 0.9652\n",
      "Batch 00559: setting learning rate to 0.04242917279815049.\n",
      "4352/9600 [============>.................] - ETA: 9s - loss: 0.6359 - accuracy: 0.9653 \n",
      "Batch 00560: setting learning rate to 0.04235437707892977.\n",
      "4480/9600 [=============>................] - ETA: 9s - loss: 0.6363 - accuracy: 0.9647\n",
      "Batch 00561: setting learning rate to 0.042279581359709044.\n",
      "4608/9600 [=============>................] - ETA: 9s - loss: 0.6363 - accuracy: 0.9642\n",
      "Batch 00562: setting learning rate to 0.04220478564048832.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.6350 - accuracy: 0.9649\n",
      "Batch 00563: setting learning rate to 0.04212998992126759.\n",
      "4864/9600 [==============>...............] - ETA: 8s - loss: 0.6350 - accuracy: 0.9646\n",
      "Batch 00564: setting learning rate to 0.042055194202046876.\n",
      "4992/9600 [==============>...............] - ETA: 8s - loss: 0.6338 - accuracy: 0.9651\n",
      "Batch 00565: setting learning rate to 0.04198039848282615.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 0.6328 - accuracy: 0.9660\n",
      "Batch 00566: setting learning rate to 0.04190560276360542.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.6339 - accuracy: 0.9657\n",
      "Batch 00567: setting learning rate to 0.041830807044384695.\n",
      "5376/9600 [===============>..............] - ETA: 7s - loss: 0.6330 - accuracy: 0.9663\n",
      "Batch 00568: setting learning rate to 0.04175601132516398.\n",
      "5504/9600 [================>.............] - ETA: 7s - loss: 0.6317 - accuracy: 0.9669\n",
      "Batch 00569: setting learning rate to 0.041681215605943255.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 0.6321 - accuracy: 0.9670\n",
      "Batch 00570: setting learning rate to 0.04160641988672253.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.6322 - accuracy: 0.9668\n",
      "Batch 00571: setting learning rate to 0.0415316241675018.\n",
      "5888/9600 [=================>............] - ETA: 6s - loss: 0.6320 - accuracy: 0.9671\n",
      "Batch 00572: setting learning rate to 0.04145682844828107.\n",
      "6016/9600 [=================>............] - ETA: 6s - loss: 0.6315 - accuracy: 0.9669\n",
      "Batch 00573: setting learning rate to 0.04138203272906036.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.6309 - accuracy: 0.9670\n",
      "Batch 00574: setting learning rate to 0.04130723700983963.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.6303 - accuracy: 0.9670\n",
      "Batch 00575: setting learning rate to 0.041232441290618906.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 0.6301 - accuracy: 0.9672\n",
      "Batch 00576: setting learning rate to 0.04115764557139818.\n",
      "6528/9600 [===================>..........] - ETA: 5s - loss: 0.6297 - accuracy: 0.9671\n",
      "Batch 00577: setting learning rate to 0.041082849852177465.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.6293 - accuracy: 0.9672\n",
      "Batch 00578: setting learning rate to 0.04100805413295674.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.6293 - accuracy: 0.9671\n",
      "Batch 00579: setting learning rate to 0.04093325841373601.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.6292 - accuracy: 0.9670\n",
      "Batch 00580: setting learning rate to 0.040858462694515284.\n",
      "7040/9600 [=====================>........] - ETA: 4s - loss: 0.6294 - accuracy: 0.9670\n",
      "Batch 00581: setting learning rate to 0.04078366697529457.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.6294 - accuracy: 0.9671\n",
      "Batch 00582: setting learning rate to 0.04070887125607384.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.6303 - accuracy: 0.9664\n",
      "Batch 00583: setting learning rate to 0.040634075536853116.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.6298 - accuracy: 0.9666\n",
      "Batch 00584: setting learning rate to 0.04055927981763239.\n",
      "7552/9600 [======================>.......] - ETA: 3s - loss: 0.6303 - accuracy: 0.9662\n",
      "Batch 00585: setting learning rate to 0.04048448409841166.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.6305 - accuracy: 0.9663\n",
      "Batch 00586: setting learning rate to 0.04040968837919095.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.6301 - accuracy: 0.9662\n",
      "Batch 00587: setting learning rate to 0.04033489265997022.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.6299 - accuracy: 0.9662\n",
      "Batch 00588: setting learning rate to 0.040260096940749494.\n",
      "8064/9600 [========================>.....] - ETA: 2s - loss: 0.6301 - accuracy: 0.9661\n",
      "Batch 00589: setting learning rate to 0.04018530122152877.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.6301 - accuracy: 0.9661\n",
      "Batch 00590: setting learning rate to 0.040110505502308054.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.6301 - accuracy: 0.9659\n",
      "Batch 00591: setting learning rate to 0.04003570978308733.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.6301 - accuracy: 0.9658\n",
      "Batch 00592: setting learning rate to 0.0399609140638666.\n",
      "8576/9600 [=========================>....] - ETA: 1s - loss: 0.6297 - accuracy: 0.9658\n",
      "Batch 00593: setting learning rate to 0.03988611834464587.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.6293 - accuracy: 0.9661\n",
      "Batch 00594: setting learning rate to 0.03981132262542516.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.6298 - accuracy: 0.9658\n",
      "Batch 00595: setting learning rate to 0.03973652690620443.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.6301 - accuracy: 0.9656\n",
      "Batch 00596: setting learning rate to 0.039661731186983705.\n",
      "9088/9600 [===========================>..] - ETA: 0s - loss: 0.6298 - accuracy: 0.9658\n",
      "Batch 00597: setting learning rate to 0.03958693546776298.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.6302 - accuracy: 0.9655\n",
      "Batch 00598: setting learning rate to 0.03951213974854225.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.6300 - accuracy: 0.9658\n",
      "Batch 00599: setting learning rate to 0.03943734402932154.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.6303 - accuracy: 0.9657\n",
      "Batch 00600: setting learning rate to 0.03936254831010081.\n",
      "9600/9600 [==============================] - 19s 2ms/step - loss: 0.6304 - accuracy: 0.9656 - val_loss: 0.2672 - val_accuracy: 0.9592\n",
      "Epoch 9/15\n",
      "\n",
      "Batch 00601: setting learning rate to 0.03928775259088008.\n",
      " 128/9600 [..............................] - ETA: 19s - loss: 0.5994 - accuracy: 0.9766\n",
      "Batch 00602: setting learning rate to 0.039212956871659356.\n",
      " 256/9600 [..............................] - ETA: 17s - loss: 0.6376 - accuracy: 0.9648\n",
      "Batch 00603: setting learning rate to 0.03913816115243864.\n",
      " 384/9600 [>.............................] - ETA: 17s - loss: 0.6357 - accuracy: 0.9661\n",
      "Batch 00604: setting learning rate to 0.039063365433217916.\n",
      " 512/9600 [>.............................] - ETA: 17s - loss: 0.6290 - accuracy: 0.9727\n",
      "Batch 00605: setting learning rate to 0.03898856971399719.\n",
      " 640/9600 [=>............................] - ETA: 16s - loss: 0.6340 - accuracy: 0.9719\n",
      "Batch 00606: setting learning rate to 0.03891377399477646.\n",
      " 768/9600 [=>............................] - ETA: 16s - loss: 0.6402 - accuracy: 0.9701\n",
      "Batch 00607: setting learning rate to 0.03883897827555574.\n",
      " 896/9600 [=>............................] - ETA: 16s - loss: 0.6358 - accuracy: 0.9699\n",
      "Batch 00608: setting learning rate to 0.03876418255633502.\n",
      "1024/9600 [==>...........................] - ETA: 15s - loss: 0.6302 - accuracy: 0.9727\n",
      "Batch 00609: setting learning rate to 0.038689386837114294.\n",
      "1152/9600 [==>...........................] - ETA: 15s - loss: 0.6292 - accuracy: 0.9731\n",
      "Batch 00610: setting learning rate to 0.03861459111789357.\n",
      "1280/9600 [===>..........................] - ETA: 15s - loss: 0.6298 - accuracy: 0.9719\n",
      "Batch 00611: setting learning rate to 0.03853979539867284.\n",
      "1408/9600 [===>..........................] - ETA: 15s - loss: 0.6307 - accuracy: 0.9723\n",
      "Batch 00612: setting learning rate to 0.038464999679452126.\n",
      "1536/9600 [===>..........................] - ETA: 14s - loss: 0.6302 - accuracy: 0.9727\n",
      "Batch 00613: setting learning rate to 0.0383902039602314.\n",
      "1664/9600 [====>.........................] - ETA: 14s - loss: 0.6307 - accuracy: 0.9724\n",
      "Batch 00614: setting learning rate to 0.03831540824101067.\n",
      "1792/9600 [====>.........................] - ETA: 14s - loss: 0.6281 - accuracy: 0.9732\n",
      "Batch 00615: setting learning rate to 0.038240612521789945.\n",
      "1920/9600 [=====>........................] - ETA: 14s - loss: 0.6276 - accuracy: 0.9734\n",
      "Batch 00616: setting learning rate to 0.03816581680256923.\n",
      "2048/9600 [=====>........................] - ETA: 13s - loss: 0.6295 - accuracy: 0.9736\n",
      "Batch 00617: setting learning rate to 0.038091021083348504.\n",
      "2176/9600 [=====>........................] - ETA: 13s - loss: 0.6294 - accuracy: 0.9733\n",
      "Batch 00618: setting learning rate to 0.03801622536412778.\n",
      "2304/9600 [======>.......................] - ETA: 13s - loss: 0.6328 - accuracy: 0.9705\n",
      "Batch 00619: setting learning rate to 0.03794142964490705.\n",
      "2432/9600 [======>.......................] - ETA: 13s - loss: 0.6307 - accuracy: 0.9708\n",
      "Batch 00620: setting learning rate to 0.03786663392568633.\n",
      "2560/9600 [=======>......................] - ETA: 12s - loss: 0.6293 - accuracy: 0.9707\n",
      "Batch 00621: setting learning rate to 0.03779183820646561.\n",
      "2688/9600 [=======>......................] - ETA: 12s - loss: 0.6315 - accuracy: 0.9699\n",
      "Batch 00622: setting learning rate to 0.03771704248724488.\n",
      "2816/9600 [=======>......................] - ETA: 12s - loss: 0.6323 - accuracy: 0.9695\n",
      "Batch 00623: setting learning rate to 0.037642246768024155.\n",
      "2944/9600 [========>.....................] - ETA: 12s - loss: 0.6305 - accuracy: 0.9701\n",
      "Batch 00624: setting learning rate to 0.03756745104880343.\n",
      "3072/9600 [========>.....................] - ETA: 12s - loss: 0.6301 - accuracy: 0.9697\n",
      "Batch 00625: setting learning rate to 0.037492655329582715.\n",
      "3200/9600 [=========>....................] - ETA: 11s - loss: 0.6293 - accuracy: 0.9706\n",
      "Batch 00626: setting learning rate to 0.03741785961036199.\n",
      "3328/9600 [=========>....................] - ETA: 11s - loss: 0.6297 - accuracy: 0.9703\n",
      "Batch 00627: setting learning rate to 0.03734306389114126.\n",
      "3456/9600 [=========>....................] - ETA: 11s - loss: 0.6282 - accuracy: 0.9711\n",
      "Batch 00628: setting learning rate to 0.037268268171920534.\n",
      "3584/9600 [==========>...................] - ETA: 11s - loss: 0.6297 - accuracy: 0.9699\n",
      "Batch 00629: setting learning rate to 0.03719347245269982.\n",
      "3712/9600 [==========>...................] - ETA: 10s - loss: 0.6302 - accuracy: 0.9690\n",
      "Batch 00630: setting learning rate to 0.03711867673347909.\n",
      "3840/9600 [===========>..................] - ETA: 10s - loss: 0.6283 - accuracy: 0.9698\n",
      "Batch 00631: setting learning rate to 0.037043881014258366.\n",
      "3968/9600 [===========>..................] - ETA: 10s - loss: 0.6283 - accuracy: 0.9693\n",
      "Batch 00632: setting learning rate to 0.03696908529503764.\n",
      "4096/9600 [===========>..................] - ETA: 10s - loss: 0.6282 - accuracy: 0.9690\n",
      "Batch 00633: setting learning rate to 0.03689428957581692.\n",
      "4224/9600 [============>.................] - ETA: 9s - loss: 0.6268 - accuracy: 0.9697 \n",
      "Batch 00634: setting learning rate to 0.0368194938565962.\n",
      "4352/9600 [============>.................] - ETA: 9s - loss: 0.6260 - accuracy: 0.9701\n",
      "Batch 00635: setting learning rate to 0.03674469813737547.\n",
      "4480/9600 [=============>................] - ETA: 9s - loss: 0.6250 - accuracy: 0.9705\n",
      "Batch 00636: setting learning rate to 0.036669902418154744.\n",
      "4608/9600 [=============>................] - ETA: 9s - loss: 0.6246 - accuracy: 0.9707\n",
      "Batch 00637: setting learning rate to 0.03659510669893402.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.6230 - accuracy: 0.9713\n",
      "Batch 00638: setting learning rate to 0.036520310979713304.\n",
      "4864/9600 [==============>...............] - ETA: 8s - loss: 0.6231 - accuracy: 0.9712\n",
      "Batch 00639: setting learning rate to 0.03644551526049258.\n",
      "4992/9600 [==============>...............] - ETA: 8s - loss: 0.6238 - accuracy: 0.9706\n",
      "Batch 00640: setting learning rate to 0.03637071954127185.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 0.6232 - accuracy: 0.9707\n",
      "Batch 00641: setting learning rate to 0.03629592382205112.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.6222 - accuracy: 0.9714\n",
      "Batch 00642: setting learning rate to 0.0362211281028304.\n",
      "5376/9600 [===============>..............] - ETA: 7s - loss: 0.6221 - accuracy: 0.9715\n",
      "Batch 00643: setting learning rate to 0.03614633238360968.\n",
      "5504/9600 [================>.............] - ETA: 7s - loss: 0.6218 - accuracy: 0.9717\n",
      "Batch 00644: setting learning rate to 0.036071536664388955.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 0.6218 - accuracy: 0.9718\n",
      "Batch 00645: setting learning rate to 0.03599674094516823.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.6216 - accuracy: 0.9715\n",
      "Batch 00646: setting learning rate to 0.03592194522594751.\n",
      "5888/9600 [=================>............] - ETA: 6s - loss: 0.6207 - accuracy: 0.9720\n",
      "Batch 00647: setting learning rate to 0.03584714950672679.\n",
      "6016/9600 [=================>............] - ETA: 6s - loss: 0.6202 - accuracy: 0.9722\n",
      "Batch 00648: setting learning rate to 0.03577235378750606.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.6194 - accuracy: 0.9725\n",
      "Batch 00649: setting learning rate to 0.03569755806828533.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.6182 - accuracy: 0.9729\n",
      "Batch 00650: setting learning rate to 0.035622762349064606.\n",
      "6400/9600 [===================>..........] - ETA: 5s - loss: 0.6187 - accuracy: 0.9727\n",
      "Batch 00651: setting learning rate to 0.03554796662984389.\n",
      "6528/9600 [===================>..........] - ETA: 5s - loss: 0.6181 - accuracy: 0.9727\n",
      "Batch 00652: setting learning rate to 0.035473170910623165.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.6180 - accuracy: 0.9725\n",
      "Batch 00653: setting learning rate to 0.03539837519140244.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.6173 - accuracy: 0.9726\n",
      "Batch 00654: setting learning rate to 0.03532357947218171.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.6176 - accuracy: 0.9722\n",
      "Batch 00655: setting learning rate to 0.03524878375296099.\n",
      "7040/9600 [=====================>........] - ETA: 4s - loss: 0.6186 - accuracy: 0.9716\n",
      "Batch 00656: setting learning rate to 0.03517398803374027.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.6187 - accuracy: 0.9717\n",
      "Batch 00657: setting learning rate to 0.035099192314519544.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.6182 - accuracy: 0.9718\n",
      "Batch 00658: setting learning rate to 0.035024396595298816.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.6187 - accuracy: 0.9714\n",
      "Batch 00659: setting learning rate to 0.034949600876078096.\n",
      "7552/9600 [======================>.......] - ETA: 3s - loss: 0.6186 - accuracy: 0.9715\n",
      "Batch 00660: setting learning rate to 0.034874805156857376.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.6187 - accuracy: 0.9715\n",
      "Batch 00661: setting learning rate to 0.03480000943763665.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.6184 - accuracy: 0.9714\n",
      "Batch 00662: setting learning rate to 0.03472521371841592.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.6185 - accuracy: 0.9710\n",
      "Batch 00663: setting learning rate to 0.034650417999195195.\n",
      "8064/9600 [========================>.....] - ETA: 2s - loss: 0.6188 - accuracy: 0.9711\n",
      "Batch 00664: setting learning rate to 0.03457562227997448.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.6197 - accuracy: 0.9709\n",
      "Batch 00665: setting learning rate to 0.034500826560753754.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.6197 - accuracy: 0.9709\n",
      "Batch 00666: setting learning rate to 0.03442603084153303.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.6199 - accuracy: 0.9711\n",
      "Batch 00667: setting learning rate to 0.0343512351223123.\n",
      "8576/9600 [=========================>....] - ETA: 1s - loss: 0.6201 - accuracy: 0.9711\n",
      "Batch 00668: setting learning rate to 0.03427643940309158.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.6202 - accuracy: 0.9710\n",
      "Batch 00669: setting learning rate to 0.03420164368387086.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.6202 - accuracy: 0.9711\n",
      "Batch 00670: setting learning rate to 0.03412684796465013.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.6207 - accuracy: 0.9708\n",
      "Batch 00671: setting learning rate to 0.034052052245429405.\n",
      "9088/9600 [===========================>..] - ETA: 0s - loss: 0.6203 - accuracy: 0.9708\n",
      "Batch 00672: setting learning rate to 0.033977256526208685.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.6203 - accuracy: 0.9709\n",
      "Batch 00673: setting learning rate to 0.033902460806987965.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.6198 - accuracy: 0.9712\n",
      "Batch 00674: setting learning rate to 0.03382766508776724.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.6198 - accuracy: 0.9714\n",
      "Batch 00675: setting learning rate to 0.03375286936854651.\n",
      "9600/9600 [==============================] - 19s 2ms/step - loss: 0.6200 - accuracy: 0.9712 - val_loss: 0.2303 - val_accuracy: 0.9625\n",
      "Epoch 10/15\n",
      "\n",
      "Batch 00676: setting learning rate to 0.03367807364932578.\n",
      " 128/9600 [..............................] - ETA: 18s - loss: 0.6041 - accuracy: 0.9844\n",
      "Batch 00677: setting learning rate to 0.03360327793010506.\n",
      " 256/9600 [..............................] - ETA: 18s - loss: 0.6079 - accuracy: 0.9766\n",
      "Batch 00678: setting learning rate to 0.03352848221088434.\n",
      " 384/9600 [>.............................] - ETA: 18s - loss: 0.6000 - accuracy: 0.9844\n",
      "Batch 00679: setting learning rate to 0.033453686491663616.\n",
      " 512/9600 [>.............................] - ETA: 18s - loss: 0.5985 - accuracy: 0.9824\n",
      "Batch 00680: setting learning rate to 0.03337889077244289.\n",
      " 640/9600 [=>............................] - ETA: 17s - loss: 0.5971 - accuracy: 0.9844\n",
      "Batch 00681: setting learning rate to 0.03330409505322217.\n",
      " 768/9600 [=>............................] - ETA: 17s - loss: 0.5950 - accuracy: 0.9844\n",
      "Batch 00682: setting learning rate to 0.03322929933400145.\n",
      " 896/9600 [=>............................] - ETA: 17s - loss: 0.5966 - accuracy: 0.9844\n",
      "Batch 00683: setting learning rate to 0.03315450361478072.\n",
      "1024/9600 [==>...........................] - ETA: 16s - loss: 0.5996 - accuracy: 0.9805\n",
      "Batch 00684: setting learning rate to 0.033079707895559994.\n",
      "1152/9600 [==>...........................] - ETA: 16s - loss: 0.5975 - accuracy: 0.9818\n",
      "Batch 00685: setting learning rate to 0.033004912176339274.\n",
      "1280/9600 [===>..........................] - ETA: 16s - loss: 0.5972 - accuracy: 0.9805\n",
      "Batch 00686: setting learning rate to 0.032930116457118554.\n",
      "1408/9600 [===>..........................] - ETA: 16s - loss: 0.6006 - accuracy: 0.9780\n",
      "Batch 00687: setting learning rate to 0.03285532073789783.\n",
      "1536/9600 [===>..........................] - ETA: 15s - loss: 0.5999 - accuracy: 0.9785\n",
      "Batch 00688: setting learning rate to 0.0327805250186771.\n",
      "1664/9600 [====>.........................] - ETA: 15s - loss: 0.6006 - accuracy: 0.9784\n",
      "Batch 00689: setting learning rate to 0.03270572929945637.\n",
      "1792/9600 [====>.........................] - ETA: 15s - loss: 0.6009 - accuracy: 0.9782\n",
      "Batch 00690: setting learning rate to 0.03263093358023565.\n",
      "1920/9600 [=====>........................] - ETA: 15s - loss: 0.6010 - accuracy: 0.9792\n",
      "Batch 00691: setting learning rate to 0.03255613786101493.\n",
      "2048/9600 [=====>........................] - ETA: 14s - loss: 0.6009 - accuracy: 0.9790\n",
      "Batch 00692: setting learning rate to 0.032481342141794205.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 0.5994 - accuracy: 0.9802\n",
      "Batch 00693: setting learning rate to 0.03240654642257348.\n",
      "2304/9600 [======>.......................] - ETA: 14s - loss: 0.6002 - accuracy: 0.9800\n",
      "Batch 00694: setting learning rate to 0.03233175070335276.\n",
      "2432/9600 [======>.......................] - ETA: 14s - loss: 0.6021 - accuracy: 0.9790\n",
      "Batch 00695: setting learning rate to 0.03225695498413204.\n",
      "2560/9600 [=======>......................] - ETA: 13s - loss: 0.6025 - accuracy: 0.9785\n",
      "Batch 00696: setting learning rate to 0.03218215926491131.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 0.6023 - accuracy: 0.9784\n",
      "Batch 00697: setting learning rate to 0.03210736354569058.\n",
      "2816/9600 [=======>......................] - ETA: 13s - loss: 0.6039 - accuracy: 0.9776\n",
      "Batch 00698: setting learning rate to 0.03203256782646986.\n",
      "2944/9600 [========>.....................] - ETA: 13s - loss: 0.6041 - accuracy: 0.9772\n",
      "Batch 00699: setting learning rate to 0.03195777210724914.\n",
      "3072/9600 [========>.....................] - ETA: 12s - loss: 0.6046 - accuracy: 0.9766\n",
      "Batch 00700: setting learning rate to 0.031882976388028415.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 0.6047 - accuracy: 0.9762\n",
      "Batch 00701: setting learning rate to 0.03180818066880769.\n",
      "3328/9600 [=========>....................] - ETA: 12s - loss: 0.6040 - accuracy: 0.9769\n",
      "Batch 00702: setting learning rate to 0.03173338494958696.\n",
      "3456/9600 [=========>....................] - ETA: 12s - loss: 0.6063 - accuracy: 0.9748\n",
      "Batch 00703: setting learning rate to 0.03165858923036624.\n",
      "3584/9600 [==========>...................] - ETA: 11s - loss: 0.6062 - accuracy: 0.9749\n",
      "Batch 00704: setting learning rate to 0.03158379351114552.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 0.6062 - accuracy: 0.9747\n",
      "Batch 00705: setting learning rate to 0.031508997791924793.\n",
      "3840/9600 [===========>..................] - ETA: 11s - loss: 0.6058 - accuracy: 0.9753\n",
      "Batch 00706: setting learning rate to 0.031434202072704066.\n",
      "3968/9600 [===========>..................] - ETA: 11s - loss: 0.6057 - accuracy: 0.9756\n",
      "Batch 00707: setting learning rate to 0.031359406353483346.\n",
      "4096/9600 [===========>..................] - ETA: 10s - loss: 0.6051 - accuracy: 0.9758\n",
      "Batch 00708: setting learning rate to 0.031284610634262626.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 0.6060 - accuracy: 0.9756\n",
      "Batch 00709: setting learning rate to 0.0312098149150419.\n",
      "4352/9600 [============>.................] - ETA: 10s - loss: 0.6062 - accuracy: 0.9756\n",
      "Batch 00710: setting learning rate to 0.031135019195821175.\n",
      "4480/9600 [=============>................] - ETA: 10s - loss: 0.6068 - accuracy: 0.9754\n",
      "Batch 00711: setting learning rate to 0.031060223476600448.\n",
      "4608/9600 [=============>................] - ETA: 9s - loss: 0.6057 - accuracy: 0.9761 \n",
      "Batch 00712: setting learning rate to 0.030985427757379724.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.6064 - accuracy: 0.9757\n",
      "Batch 00713: setting learning rate to 0.030910632038159004.\n",
      "4864/9600 [==============>...............] - ETA: 9s - loss: 0.6061 - accuracy: 0.9755\n",
      "Batch 00714: setting learning rate to 0.030835836318938277.\n",
      "4992/9600 [==============>...............] - ETA: 9s - loss: 0.6059 - accuracy: 0.9756\n",
      "Batch 00715: setting learning rate to 0.030761040599717553.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 0.6057 - accuracy: 0.9754\n",
      "Batch 00716: setting learning rate to 0.030686244880496826.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.6050 - accuracy: 0.9756\n",
      "Batch 00717: setting learning rate to 0.03061144916127611.\n",
      "5376/9600 [===============>..............] - ETA: 8s - loss: 0.6050 - accuracy: 0.9758\n",
      "Batch 00718: setting learning rate to 0.030536653442055382.\n",
      "5504/9600 [================>.............] - ETA: 8s - loss: 0.6057 - accuracy: 0.9758\n",
      "Batch 00719: setting learning rate to 0.03046185772283466.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 0.6053 - accuracy: 0.9760\n",
      "Batch 00720: setting learning rate to 0.03038706200361393.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.6053 - accuracy: 0.9759\n",
      "Batch 00721: setting learning rate to 0.030312266284393215.\n",
      "5888/9600 [=================>............] - ETA: 7s - loss: 0.6046 - accuracy: 0.9762\n",
      "Batch 00722: setting learning rate to 0.030237470565172488.\n",
      "6016/9600 [=================>............] - ETA: 7s - loss: 0.6045 - accuracy: 0.9762\n",
      "Batch 00723: setting learning rate to 0.030162674845951764.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.6054 - accuracy: 0.9759\n",
      "Batch 00724: setting learning rate to 0.030087879126731037.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.6053 - accuracy: 0.9759\n",
      "Batch 00725: setting learning rate to 0.030013083407510313.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 0.6050 - accuracy: 0.9758\n",
      "Batch 00726: setting learning rate to 0.029938287688289593.\n",
      "6528/9600 [===================>..........] - ETA: 6s - loss: 0.6049 - accuracy: 0.9759\n",
      "Batch 00727: setting learning rate to 0.029863491969068866.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.6059 - accuracy: 0.9754\n",
      "Batch 00728: setting learning rate to 0.029788696249848142.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.6061 - accuracy: 0.9751\n",
      "Batch 00729: setting learning rate to 0.029713900530627415.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.6061 - accuracy: 0.9753\n",
      "Batch 00730: setting learning rate to 0.029639104811406698.\n",
      "7040/9600 [=====================>........] - ETA: 5s - loss: 0.6057 - accuracy: 0.9756\n",
      "Batch 00731: setting learning rate to 0.02956430909218597.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.6059 - accuracy: 0.9753\n",
      "Batch 00732: setting learning rate to 0.029489513372965247.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.6068 - accuracy: 0.9751\n",
      "Batch 00733: setting learning rate to 0.02941471765374452.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.6072 - accuracy: 0.9749\n",
      "Batch 00734: setting learning rate to 0.029339921934523804.\n",
      "7552/9600 [======================>.......] - ETA: 4s - loss: 0.6067 - accuracy: 0.9751\n",
      "Batch 00735: setting learning rate to 0.029265126215303076.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.6063 - accuracy: 0.9753\n",
      "Batch 00736: setting learning rate to 0.029190330496082353.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.6071 - accuracy: 0.9750\n",
      "Batch 00737: setting learning rate to 0.029115534776861626.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.6072 - accuracy: 0.9751\n",
      "Batch 00738: setting learning rate to 0.029040739057640902.\n",
      "8064/9600 [========================>.....] - ETA: 3s - loss: 0.6083 - accuracy: 0.9745\n",
      "Batch 00739: setting learning rate to 0.02896594333842018.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.6085 - accuracy: 0.9744\n",
      "Batch 00740: setting learning rate to 0.028891147619199455.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.6085 - accuracy: 0.9743\n",
      "Batch 00741: setting learning rate to 0.02881635189997873.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.6087 - accuracy: 0.9740\n",
      "Batch 00742: setting learning rate to 0.028741556180758004.\n",
      "8576/9600 [=========================>....] - ETA: 2s - loss: 0.6087 - accuracy: 0.9738\n",
      "Batch 00743: setting learning rate to 0.028666760461537287.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.6086 - accuracy: 0.9738\n",
      "Batch 00744: setting learning rate to 0.02859196474231656.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.6089 - accuracy: 0.9736\n",
      "Batch 00745: setting learning rate to 0.028517169023095836.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.6089 - accuracy: 0.9738\n",
      "Batch 00746: setting learning rate to 0.02844237330387511.\n",
      "9088/9600 [===========================>..] - ETA: 1s - loss: 0.6092 - accuracy: 0.9736\n",
      "Batch 00747: setting learning rate to 0.028367577584654385.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.6093 - accuracy: 0.9734\n",
      "Batch 00748: setting learning rate to 0.028292781865433665.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.6098 - accuracy: 0.9732\n",
      "Batch 00749: setting learning rate to 0.02821798614621294.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.6099 - accuracy: 0.9730\n",
      "Batch 00750: setting learning rate to 0.028143190426992214.\n",
      "9600/9600 [==============================] - 20s 2ms/step - loss: 0.6100 - accuracy: 0.9729 - val_loss: 0.1963 - val_accuracy: 0.9717\n",
      "Epoch 11/15\n",
      "\n",
      "Batch 00751: setting learning rate to 0.02806839470777149.\n",
      " 128/9600 [..............................] - ETA: 20s - loss: 0.5996 - accuracy: 0.9844\n",
      "Batch 00752: setting learning rate to 0.02799359898855077.\n",
      " 256/9600 [..............................] - ETA: 19s - loss: 0.5828 - accuracy: 0.9883\n",
      "Batch 00753: setting learning rate to 0.027918803269330043.\n",
      " 384/9600 [>.............................] - ETA: 18s - loss: 0.5975 - accuracy: 0.9792\n",
      "Batch 00754: setting learning rate to 0.02784400755010932.\n",
      " 512/9600 [>.............................] - ETA: 18s - loss: 0.5989 - accuracy: 0.9785\n",
      "Batch 00755: setting learning rate to 0.027769211830888593.\n",
      " 640/9600 [=>............................] - ETA: 18s - loss: 0.5939 - accuracy: 0.9812\n",
      "Batch 00756: setting learning rate to 0.027694416111667876.\n",
      " 768/9600 [=>............................] - ETA: 17s - loss: 0.5972 - accuracy: 0.9792\n",
      "Batch 00757: setting learning rate to 0.02761962039244715.\n",
      " 896/9600 [=>............................] - ETA: 17s - loss: 0.5934 - accuracy: 0.9799\n",
      "Batch 00758: setting learning rate to 0.027544824673226425.\n",
      "1024/9600 [==>...........................] - ETA: 17s - loss: 0.5930 - accuracy: 0.9795\n",
      "Batch 00759: setting learning rate to 0.027470028954005698.\n",
      "1152/9600 [==>...........................] - ETA: 16s - loss: 0.5964 - accuracy: 0.9774\n",
      "Batch 00760: setting learning rate to 0.027395233234784974.\n",
      "1280/9600 [===>..........................] - ETA: 16s - loss: 0.5969 - accuracy: 0.9766\n",
      "Batch 00761: setting learning rate to 0.027320437515564254.\n",
      "1408/9600 [===>..........................] - ETA: 16s - loss: 0.5941 - accuracy: 0.9787\n",
      "Batch 00762: setting learning rate to 0.02724564179634353.\n",
      "1536/9600 [===>..........................] - ETA: 15s - loss: 0.5940 - accuracy: 0.9792\n",
      "Batch 00763: setting learning rate to 0.027170846077122803.\n",
      "1664/9600 [====>.........................] - ETA: 15s - loss: 0.5950 - accuracy: 0.9790\n",
      "Batch 00764: setting learning rate to 0.02709605035790208.\n",
      "1792/9600 [====>.........................] - ETA: 15s - loss: 0.5958 - accuracy: 0.9788\n",
      "Batch 00765: setting learning rate to 0.02702125463868136.\n",
      "1920/9600 [=====>........................] - ETA: 15s - loss: 0.5953 - accuracy: 0.9781\n",
      "Batch 00766: setting learning rate to 0.026946458919460632.\n",
      "2048/9600 [=====>........................] - ETA: 14s - loss: 0.5948 - accuracy: 0.9790\n",
      "Batch 00767: setting learning rate to 0.02687166320023991.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 0.5957 - accuracy: 0.9784\n",
      "Batch 00768: setting learning rate to 0.02679686748101918.\n",
      "2304/9600 [======>.......................] - ETA: 14s - loss: 0.5942 - accuracy: 0.9792\n",
      "Batch 00769: setting learning rate to 0.026722071761798465.\n",
      "2432/9600 [======>.......................] - ETA: 14s - loss: 0.5947 - accuracy: 0.9786\n",
      "Batch 00770: setting learning rate to 0.026647276042577737.\n",
      "2560/9600 [=======>......................] - ETA: 13s - loss: 0.5941 - accuracy: 0.9785\n",
      "Batch 00771: setting learning rate to 0.026572480323357014.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 0.5963 - accuracy: 0.9777\n",
      "Batch 00772: setting learning rate to 0.026497684604136287.\n",
      "2816/9600 [=======>......................] - ETA: 13s - loss: 0.5953 - accuracy: 0.9783\n",
      "Batch 00773: setting learning rate to 0.026422888884915563.\n",
      "2944/9600 [========>.....................] - ETA: 13s - loss: 0.5960 - accuracy: 0.9783\n",
      "Batch 00774: setting learning rate to 0.026348093165694843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072/9600 [========>.....................] - ETA: 12s - loss: 0.5972 - accuracy: 0.9775\n",
      "Batch 00775: setting learning rate to 0.02627329744647412.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 0.5968 - accuracy: 0.9781\n",
      "Batch 00776: setting learning rate to 0.026198501727253392.\n",
      "3328/9600 [=========>....................] - ETA: 12s - loss: 0.5956 - accuracy: 0.9787\n",
      "Batch 00777: setting learning rate to 0.02612370600803267.\n",
      "3456/9600 [=========>....................] - ETA: 11s - loss: 0.5971 - accuracy: 0.9780\n",
      "Batch 00778: setting learning rate to 0.026048910288811948.\n",
      "3584/9600 [==========>...................] - ETA: 11s - loss: 0.5986 - accuracy: 0.9774\n",
      "Batch 00779: setting learning rate to 0.02597411456959122.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 0.5979 - accuracy: 0.9779\n",
      "Batch 00780: setting learning rate to 0.025899318850370497.\n",
      "3840/9600 [===========>..................] - ETA: 11s - loss: 0.5976 - accuracy: 0.9781\n",
      "Batch 00781: setting learning rate to 0.02582452313114977.\n",
      "3968/9600 [===========>..................] - ETA: 10s - loss: 0.5967 - accuracy: 0.9783\n",
      "Batch 00782: setting learning rate to 0.025749727411929046.\n",
      "4096/9600 [===========>..................] - ETA: 10s - loss: 0.5966 - accuracy: 0.9780\n",
      "Batch 00783: setting learning rate to 0.025674931692708326.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 0.5971 - accuracy: 0.9775\n",
      "Batch 00784: setting learning rate to 0.025600135973487603.\n",
      "4352/9600 [============>.................] - ETA: 10s - loss: 0.5980 - accuracy: 0.9770\n",
      "Batch 00785: setting learning rate to 0.025525340254266875.\n",
      "4480/9600 [=============>................] - ETA: 9s - loss: 0.5976 - accuracy: 0.9775 \n",
      "Batch 00786: setting learning rate to 0.025450544535046152.\n",
      "4608/9600 [=============>................] - ETA: 9s - loss: 0.5999 - accuracy: 0.9763\n",
      "Batch 00787: setting learning rate to 0.02537574881582543.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.5998 - accuracy: 0.9761\n",
      "Batch 00788: setting learning rate to 0.025300953096604708.\n",
      "4864/9600 [==============>...............] - ETA: 9s - loss: 0.5998 - accuracy: 0.9762\n",
      "Batch 00789: setting learning rate to 0.02522615737738398.\n",
      "4992/9600 [==============>...............] - ETA: 9s - loss: 0.5994 - accuracy: 0.9764\n",
      "Batch 00790: setting learning rate to 0.025151361658163254.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 0.6001 - accuracy: 0.9760\n",
      "Batch 00791: setting learning rate to 0.025076565938942537.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.6001 - accuracy: 0.9760\n",
      "Batch 00792: setting learning rate to 0.02500177021972181.\n",
      "5376/9600 [===============>..............] - ETA: 8s - loss: 0.6012 - accuracy: 0.9754\n",
      "Batch 00793: setting learning rate to 0.024926974500501086.\n",
      "5504/9600 [================>.............] - ETA: 8s - loss: 0.6016 - accuracy: 0.9751\n",
      "Batch 00794: setting learning rate to 0.02485217878128036.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 0.6012 - accuracy: 0.9753\n",
      "Batch 00795: setting learning rate to 0.024777383062059635.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.6010 - accuracy: 0.9757\n",
      "Batch 00796: setting learning rate to 0.024702587342838915.\n",
      "5888/9600 [=================>............] - ETA: 7s - loss: 0.6013 - accuracy: 0.9759\n",
      "Batch 00797: setting learning rate to 0.02462779162361819.\n",
      "6016/9600 [=================>............] - ETA: 7s - loss: 0.6011 - accuracy: 0.9761\n",
      "Batch 00798: setting learning rate to 0.024552995904397464.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.6007 - accuracy: 0.9764\n",
      "Batch 00799: setting learning rate to 0.02447820018517674.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.6004 - accuracy: 0.9766\n",
      "Batch 00800: setting learning rate to 0.02440340446595602.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 0.6002 - accuracy: 0.9766\n",
      "Batch 00801: setting learning rate to 0.024328608746735297.\n",
      "6528/9600 [===================>..........] - ETA: 6s - loss: 0.5997 - accuracy: 0.9767\n",
      "Batch 00802: setting learning rate to 0.02425381302751457.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.5998 - accuracy: 0.9769\n",
      "Batch 00803: setting learning rate to 0.024179017308293842.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.5993 - accuracy: 0.9772\n",
      "Batch 00804: setting learning rate to 0.02410422158907312.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.5991 - accuracy: 0.9771\n",
      "Batch 00805: setting learning rate to 0.0240294258698524.\n",
      "7040/9600 [=====================>........] - ETA: 5s - loss: 0.5991 - accuracy: 0.9773\n",
      "Batch 00806: setting learning rate to 0.023954630150631675.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.5987 - accuracy: 0.9774\n",
      "Batch 00807: setting learning rate to 0.023879834431410948.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.5990 - accuracy: 0.9774\n",
      "Batch 00808: setting learning rate to 0.023805038712190224.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.5993 - accuracy: 0.9772\n",
      "Batch 00809: setting learning rate to 0.023730242992969504.\n",
      "7552/9600 [======================>.......] - ETA: 4s - loss: 0.5998 - accuracy: 0.9770\n",
      "Batch 00810: setting learning rate to 0.02365544727374878.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.5997 - accuracy: 0.9771\n",
      "Batch 00811: setting learning rate to 0.023580651554528053.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.5995 - accuracy: 0.9772\n",
      "Batch 00812: setting learning rate to 0.02350585583530733.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.5996 - accuracy: 0.9768\n",
      "Batch 00813: setting learning rate to 0.02343106011608661.\n",
      "8064/9600 [========================>.....] - ETA: 3s - loss: 0.6002 - accuracy: 0.9764\n",
      "Batch 00814: setting learning rate to 0.023356264396865885.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.6002 - accuracy: 0.9763\n",
      "Batch 00815: setting learning rate to 0.02328146867764516.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.6003 - accuracy: 0.9761\n",
      "Batch 00816: setting learning rate to 0.02320667295842443.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.6009 - accuracy: 0.9756\n",
      "Batch 00817: setting learning rate to 0.023131877239203708.\n",
      "8576/9600 [=========================>....] - ETA: 2s - loss: 0.6006 - accuracy: 0.9756\n",
      "Batch 00818: setting learning rate to 0.02305708151998298.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.6010 - accuracy: 0.9755\n",
      "Batch 00819: setting learning rate to 0.022982285800762257.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.6012 - accuracy: 0.9757\n",
      "Batch 00820: setting learning rate to 0.022907490081541543.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.6014 - accuracy: 0.9757\n",
      "Batch 00821: setting learning rate to 0.02283269436232082.\n",
      "9088/9600 [===========================>..] - ETA: 1s - loss: 0.6010 - accuracy: 0.9759\n",
      "Batch 00822: setting learning rate to 0.022757898643100093.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.6005 - accuracy: 0.9761\n",
      "Batch 00823: setting learning rate to 0.02268310292387937.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.6003 - accuracy: 0.9761\n",
      "Batch 00824: setting learning rate to 0.022608307204658642.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.6001 - accuracy: 0.9764\n",
      "Batch 00825: setting learning rate to 0.022533511485437918.\n",
      "9600/9600 [==============================] - 20s 2ms/step - loss: 0.5999 - accuracy: 0.9765 - val_loss: 0.2018 - val_accuracy: 0.9712\n",
      "Epoch 12/15\n",
      "\n",
      "Batch 00826: setting learning rate to 0.02245871576621719.\n",
      " 128/9600 [..............................] - ETA: 18s - loss: 0.5736 - accuracy: 0.9922\n",
      "Batch 00827: setting learning rate to 0.022383920046996467.\n",
      " 256/9600 [..............................] - ETA: 18s - loss: 0.6019 - accuracy: 0.9766\n",
      "Batch 00828: setting learning rate to 0.02230912432777574.\n",
      " 384/9600 [>.............................] - ETA: 17s - loss: 0.6177 - accuracy: 0.9688\n",
      "Batch 00829: setting learning rate to 0.022234328608555027.\n",
      " 512/9600 [>.............................] - ETA: 17s - loss: 0.6123 - accuracy: 0.9688\n",
      "Batch 00830: setting learning rate to 0.022159532889334303.\n",
      " 640/9600 [=>............................] - ETA: 17s - loss: 0.6134 - accuracy: 0.9703\n",
      "Batch 00831: setting learning rate to 0.022084737170113576.\n",
      " 768/9600 [=>............................] - ETA: 17s - loss: 0.6073 - accuracy: 0.9727\n",
      "Batch 00832: setting learning rate to 0.022009941450892852.\n",
      " 896/9600 [=>............................] - ETA: 16s - loss: 0.6062 - accuracy: 0.9721\n",
      "Batch 00833: setting learning rate to 0.021935145731672125.\n",
      "1024/9600 [==>...........................] - ETA: 16s - loss: 0.6053 - accuracy: 0.9727\n",
      "Batch 00834: setting learning rate to 0.0218603500124514.\n",
      "1152/9600 [==>...........................] - ETA: 16s - loss: 0.6036 - accuracy: 0.9748\n",
      "Batch 00835: setting learning rate to 0.021785554293230674.\n",
      "1280/9600 [===>..........................] - ETA: 15s - loss: 0.6012 - accuracy: 0.9766\n",
      "Batch 00836: setting learning rate to 0.02171075857400995.\n",
      "1408/9600 [===>..........................] - ETA: 15s - loss: 0.5992 - accuracy: 0.9766\n",
      "Batch 00837: setting learning rate to 0.021635962854789238.\n",
      "1536/9600 [===>..........................] - ETA: 15s - loss: 0.5999 - accuracy: 0.9753\n",
      "Batch 00838: setting learning rate to 0.021561167135568514.\n",
      "1664/9600 [====>.........................] - ETA: 15s - loss: 0.5979 - accuracy: 0.9766\n",
      "Batch 00839: setting learning rate to 0.021486371416347787.\n",
      "1792/9600 [====>.........................] - ETA: 14s - loss: 0.5981 - accuracy: 0.9777\n",
      "Batch 00840: setting learning rate to 0.021411575697127063.\n",
      "1920/9600 [=====>........................] - ETA: 14s - loss: 0.5987 - accuracy: 0.9776\n",
      "Batch 00841: setting learning rate to 0.021336779977906336.\n",
      "2048/9600 [=====>........................] - ETA: 14s - loss: 0.5977 - accuracy: 0.9771\n",
      "Batch 00842: setting learning rate to 0.02126198425868561.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 0.5985 - accuracy: 0.9775\n",
      "Batch 00843: setting learning rate to 0.021187188539464885.\n",
      "2304/9600 [======>.......................] - ETA: 13s - loss: 0.5973 - accuracy: 0.9779\n",
      "Batch 00844: setting learning rate to 0.021112392820244158.\n",
      "2432/9600 [======>.......................] - ETA: 13s - loss: 0.5979 - accuracy: 0.9774\n",
      "Batch 00845: setting learning rate to 0.021037597101023434.\n",
      "2560/9600 [=======>......................] - ETA: 13s - loss: 0.5976 - accuracy: 0.9777\n",
      "Batch 00846: setting learning rate to 0.02096280138180272.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 0.5967 - accuracy: 0.9784\n",
      "Batch 00847: setting learning rate to 0.020888005662581997.\n",
      "2816/9600 [=======>......................] - ETA: 13s - loss: 0.5959 - accuracy: 0.9787\n",
      "Batch 00848: setting learning rate to 0.02081320994336127.\n",
      "2944/9600 [========>.....................] - ETA: 12s - loss: 0.5951 - accuracy: 0.9789\n",
      "Batch 00849: setting learning rate to 0.020738414224140547.\n",
      "3072/9600 [========>.....................] - ETA: 12s - loss: 0.5968 - accuracy: 0.9782\n",
      "Batch 00850: setting learning rate to 0.02066361850491982.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 0.5969 - accuracy: 0.9772\n",
      "Batch 00851: setting learning rate to 0.020588822785699096.\n",
      "3328/9600 [=========>....................] - ETA: 12s - loss: 0.5963 - accuracy: 0.9775\n",
      "Batch 00852: setting learning rate to 0.02051402706647837.\n",
      "3456/9600 [=========>....................] - ETA: 11s - loss: 0.5957 - accuracy: 0.9780\n",
      "Batch 00853: setting learning rate to 0.020439231347257645.\n",
      "3584/9600 [==========>...................] - ETA: 11s - loss: 0.5947 - accuracy: 0.9788\n",
      "Batch 00854: setting learning rate to 0.020364435628036918.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 0.5956 - accuracy: 0.9790\n",
      "Batch 00855: setting learning rate to 0.020289639908816205.\n",
      "3840/9600 [===========>..................] - ETA: 11s - loss: 0.5945 - accuracy: 0.9794\n",
      "Batch 00856: setting learning rate to 0.02021484418959548.\n",
      "3968/9600 [===========>..................] - ETA: 10s - loss: 0.5954 - accuracy: 0.9791\n",
      "Batch 00857: setting learning rate to 0.020140048470374754.\n",
      "4096/9600 [===========>..................] - ETA: 10s - loss: 0.5951 - accuracy: 0.9790\n",
      "Batch 00858: setting learning rate to 0.02006525275115403.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 0.5951 - accuracy: 0.9792\n",
      "Batch 00859: setting learning rate to 0.019990457031933303.\n",
      "4352/9600 [============>.................] - ETA: 10s - loss: 0.5950 - accuracy: 0.9791\n",
      "Batch 00860: setting learning rate to 0.01991566131271258.\n",
      "4480/9600 [=============>................] - ETA: 10s - loss: 0.5945 - accuracy: 0.9795\n",
      "Batch 00861: setting learning rate to 0.019840865593491852.\n",
      "4608/9600 [=============>................] - ETA: 9s - loss: 0.5949 - accuracy: 0.9792 \n",
      "Batch 00862: setting learning rate to 0.01976606987427113.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.5950 - accuracy: 0.9791\n",
      "Batch 00863: setting learning rate to 0.0196912741550504.\n",
      "4864/9600 [==============>...............] - ETA: 9s - loss: 0.5961 - accuracy: 0.9786\n",
      "Batch 00864: setting learning rate to 0.01961647843582969.\n",
      "4992/9600 [==============>...............] - ETA: 9s - loss: 0.5970 - accuracy: 0.9784\n",
      "Batch 00865: setting learning rate to 0.019541682716608964.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 0.5960 - accuracy: 0.9787\n",
      "Batch 00866: setting learning rate to 0.01946688699738824.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.5956 - accuracy: 0.9787\n",
      "Batch 00867: setting learning rate to 0.019392091278167514.\n",
      "5376/9600 [===============>..............] - ETA: 8s - loss: 0.5949 - accuracy: 0.9788\n",
      "Batch 00868: setting learning rate to 0.019317295558946786.\n",
      "5504/9600 [================>.............] - ETA: 8s - loss: 0.5946 - accuracy: 0.9787\n",
      "Batch 00869: setting learning rate to 0.019242499839726063.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 0.5944 - accuracy: 0.9789\n",
      "Batch 00870: setting learning rate to 0.019167704120505336.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.5941 - accuracy: 0.9792\n",
      "Batch 00871: setting learning rate to 0.019092908401284612.\n",
      "5888/9600 [=================>............] - ETA: 7s - loss: 0.5945 - accuracy: 0.9793\n",
      "Batch 00872: setting learning rate to 0.0190181126820639.\n",
      "6016/9600 [=================>............] - ETA: 7s - loss: 0.5943 - accuracy: 0.9792\n",
      "Batch 00873: setting learning rate to 0.018943316962843175.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.5938 - accuracy: 0.9795\n",
      "Batch 00874: setting learning rate to 0.018868521243622448.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.5939 - accuracy: 0.9793\n",
      "Batch 00875: setting learning rate to 0.018793725524401724.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 0.5939 - accuracy: 0.9791\n",
      "Batch 00876: setting learning rate to 0.018718929805180997.\n",
      "6528/9600 [===================>..........] - ETA: 6s - loss: 0.5948 - accuracy: 0.9787\n",
      "Batch 00877: setting learning rate to 0.018644134085960273.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.5954 - accuracy: 0.9785\n",
      "Batch 00878: setting learning rate to 0.018569338366739546.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.5956 - accuracy: 0.9783\n",
      "Batch 00879: setting learning rate to 0.018494542647518823.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.5962 - accuracy: 0.9780\n",
      "Batch 00880: setting learning rate to 0.018419746928298095.\n",
      "7040/9600 [=====================>........] - ETA: 5s - loss: 0.5957 - accuracy: 0.9783\n",
      "Batch 00881: setting learning rate to 0.018344951209077382.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.5959 - accuracy: 0.9781\n",
      "Batch 00882: setting learning rate to 0.01827015548985666.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.5960 - accuracy: 0.9779\n",
      "Batch 00883: setting learning rate to 0.01819535977063593.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.5961 - accuracy: 0.9780\n",
      "Batch 00884: setting learning rate to 0.018120564051415208.\n",
      "7552/9600 [======================>.......] - ETA: 4s - loss: 0.5961 - accuracy: 0.9779\n",
      "Batch 00885: setting learning rate to 0.01804576833219448.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.5967 - accuracy: 0.9773\n",
      "Batch 00886: setting learning rate to 0.017970972612973757.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.5964 - accuracy: 0.9776\n",
      "Batch 00887: setting learning rate to 0.01789617689375303.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.5969 - accuracy: 0.9776\n",
      "Batch 00888: setting learning rate to 0.017821381174532306.\n",
      "8064/9600 [========================>.....] - ETA: 3s - loss: 0.5967 - accuracy: 0.9776\n",
      "Batch 00889: setting learning rate to 0.01774658545531158.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.5967 - accuracy: 0.9778\n",
      "Batch 00890: setting learning rate to 0.01767178973609087.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.5966 - accuracy: 0.9780\n",
      "Batch 00891: setting learning rate to 0.017596994016870142.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.5965 - accuracy: 0.9781\n",
      "Batch 00892: setting learning rate to 0.017522198297649415.\n",
      "8576/9600 [=========================>....] - ETA: 2s - loss: 0.5967 - accuracy: 0.9780\n",
      "Batch 00893: setting learning rate to 0.01744740257842869.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.5965 - accuracy: 0.9781\n",
      "Batch 00894: setting learning rate to 0.017372606859207964.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.5964 - accuracy: 0.9781\n",
      "Batch 00895: setting learning rate to 0.01729781113998724.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.5959 - accuracy: 0.9783\n",
      "Batch 00896: setting learning rate to 0.017223015420766513.\n",
      "9088/9600 [===========================>..] - ETA: 1s - loss: 0.5965 - accuracy: 0.9781\n",
      "Batch 00897: setting learning rate to 0.01714821970154579.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.5966 - accuracy: 0.9780\n",
      "Batch 00898: setting learning rate to 0.017073423982325062.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.5969 - accuracy: 0.9778\n",
      "Batch 00899: setting learning rate to 0.016998628263104353.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.5970 - accuracy: 0.9776\n",
      "Batch 00900: setting learning rate to 0.016923832543883625.\n",
      "9600/9600 [==============================] - 20s 2ms/step - loss: 0.5966 - accuracy: 0.9777 - val_loss: 0.1946 - val_accuracy: 0.9758\n",
      "Epoch 13/15\n",
      "\n",
      "Batch 00901: setting learning rate to 0.0168490368246629.\n",
      " 128/9600 [..............................] - ETA: 20s - loss: 0.5923 - accuracy: 0.9766\n",
      "Batch 00902: setting learning rate to 0.016774241105442175.\n",
      " 256/9600 [..............................] - ETA: 19s - loss: 0.5789 - accuracy: 0.9805\n",
      "Batch 00903: setting learning rate to 0.01669944538622145.\n",
      " 384/9600 [>.............................] - ETA: 19s - loss: 0.5821 - accuracy: 0.9818\n",
      "Batch 00904: setting learning rate to 0.016624649667000724.\n",
      " 512/9600 [>.............................] - ETA: 18s - loss: 0.5860 - accuracy: 0.9805\n",
      "Batch 00905: setting learning rate to 0.01654985394778.\n",
      " 640/9600 [=>............................] - ETA: 18s - loss: 0.5856 - accuracy: 0.9828\n",
      "Batch 00906: setting learning rate to 0.016475058228559273.\n",
      " 768/9600 [=>............................] - ETA: 17s - loss: 0.5844 - accuracy: 0.9831\n",
      "Batch 00907: setting learning rate to 0.01640026250933856.\n",
      " 896/9600 [=>............................] - ETA: 17s - loss: 0.5827 - accuracy: 0.9833\n",
      "Batch 00908: setting learning rate to 0.016325466790117836.\n",
      "1024/9600 [==>...........................] - ETA: 17s - loss: 0.5814 - accuracy: 0.9834\n",
      "Batch 00909: setting learning rate to 0.01625067107089711.\n",
      "1152/9600 [==>...........................] - ETA: 17s - loss: 0.5806 - accuracy: 0.9826\n",
      "Batch 00910: setting learning rate to 0.016175875351676385.\n",
      "1280/9600 [===>..........................] - ETA: 16s - loss: 0.5791 - accuracy: 0.9828\n",
      "Batch 00911: setting learning rate to 0.016101079632455658.\n",
      "1408/9600 [===>..........................] - ETA: 16s - loss: 0.5808 - accuracy: 0.9822\n",
      "Batch 00912: setting learning rate to 0.016026283913234934.\n",
      "1536/9600 [===>..........................] - ETA: 16s - loss: 0.5797 - accuracy: 0.9824\n",
      "Batch 00913: setting learning rate to 0.015951488194014207.\n",
      "1664/9600 [====>.........................] - ETA: 16s - loss: 0.5797 - accuracy: 0.9826\n",
      "Batch 00914: setting learning rate to 0.015876692474793484.\n",
      "1792/9600 [====>.........................] - ETA: 15s - loss: 0.5836 - accuracy: 0.9810\n",
      "Batch 00915: setting learning rate to 0.015801896755572756.\n",
      "1920/9600 [=====>........................] - ETA: 15s - loss: 0.5840 - accuracy: 0.9807\n",
      "Batch 00916: setting learning rate to 0.015727101036352047.\n",
      "2048/9600 [=====>........................] - ETA: 15s - loss: 0.5828 - accuracy: 0.9819\n",
      "Batch 00917: setting learning rate to 0.01565230531713132.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 0.5816 - accuracy: 0.9830\n",
      "Batch 00918: setting learning rate to 0.015577509597910594.\n",
      "2304/9600 [======>.......................] - ETA: 14s - loss: 0.5807 - accuracy: 0.9839\n",
      "Batch 00919: setting learning rate to 0.015502713878689869.\n",
      "2432/9600 [======>.......................] - ETA: 14s - loss: 0.5797 - accuracy: 0.9844\n",
      "Batch 00920: setting learning rate to 0.015427918159469143.\n",
      "2560/9600 [=======>......................] - ETA: 14s - loss: 0.5835 - accuracy: 0.9820\n",
      "Batch 00921: setting learning rate to 0.015353122440248418.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 0.5847 - accuracy: 0.9810\n",
      "Batch 00922: setting learning rate to 0.015278326721027692.\n",
      "2816/9600 [=======>......................] - ETA: 13s - loss: 0.5859 - accuracy: 0.9805\n",
      "Batch 00923: setting learning rate to 0.015203531001806967.\n",
      "2944/9600 [========>.....................] - ETA: 13s - loss: 0.5854 - accuracy: 0.9806\n",
      "Batch 00924: setting learning rate to 0.015128735282586242.\n",
      "3072/9600 [========>.....................] - ETA: 13s - loss: 0.5860 - accuracy: 0.9805\n",
      "Batch 00925: setting learning rate to 0.015053939563365528.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 0.5866 - accuracy: 0.9806\n",
      "Batch 00926: setting learning rate to 0.014979143844144803.\n",
      "3328/9600 [=========>....................] - ETA: 12s - loss: 0.5862 - accuracy: 0.9808\n",
      "Batch 00927: setting learning rate to 0.014904348124924078.\n",
      "3456/9600 [=========>....................] - ETA: 12s - loss: 0.5854 - accuracy: 0.9812\n",
      "Batch 00928: setting learning rate to 0.014829552405703352.\n",
      "3584/9600 [==========>...................] - ETA: 12s - loss: 0.5855 - accuracy: 0.9810\n",
      "Batch 00929: setting learning rate to 0.014754756686482627.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 0.5851 - accuracy: 0.9811\n",
      "Batch 00930: setting learning rate to 0.014679960967261901.\n",
      "3840/9600 [===========>..................] - ETA: 11s - loss: 0.5854 - accuracy: 0.9810\n",
      "Batch 00931: setting learning rate to 0.014605165248041176.\n",
      "3968/9600 [===========>..................] - ETA: 11s - loss: 0.5868 - accuracy: 0.9803\n",
      "Batch 00932: setting learning rate to 0.01453036952882045.\n",
      "4096/9600 [===========>..................] - ETA: 11s - loss: 0.5862 - accuracy: 0.9807\n",
      "Batch 00933: setting learning rate to 0.014455573809599725.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 0.5864 - accuracy: 0.9804\n",
      "Batch 00934: setting learning rate to 0.014380778090379014.\n",
      "4352/9600 [============>.................] - ETA: 10s - loss: 0.5861 - accuracy: 0.9802\n",
      "Batch 00935: setting learning rate to 0.014305982371158288.\n",
      "4480/9600 [=============>................] - ETA: 10s - loss: 0.5853 - accuracy: 0.9806\n",
      "Batch 00936: setting learning rate to 0.014231186651937563.\n",
      "4608/9600 [=============>................] - ETA: 10s - loss: 0.5847 - accuracy: 0.9809\n",
      "Batch 00937: setting learning rate to 0.014156390932716837.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.5854 - accuracy: 0.9804 \n",
      "Batch 00938: setting learning rate to 0.01408159521349611.\n",
      "4864/9600 [==============>...............] - ETA: 9s - loss: 0.5852 - accuracy: 0.9803\n",
      "Batch 00939: setting learning rate to 0.014006799494275385.\n",
      "4992/9600 [==============>...............] - ETA: 9s - loss: 0.5852 - accuracy: 0.9806\n",
      "Batch 00940: setting learning rate to 0.01393200377505466.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 0.5853 - accuracy: 0.9805\n",
      "Batch 00941: setting learning rate to 0.013857208055833934.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.5850 - accuracy: 0.9808\n",
      "Batch 00942: setting learning rate to 0.013782412336613223.\n",
      "5376/9600 [===============>..............] - ETA: 8s - loss: 0.5848 - accuracy: 0.9810\n",
      "Batch 00943: setting learning rate to 0.013707616617392497.\n",
      "5504/9600 [================>.............] - ETA: 8s - loss: 0.5851 - accuracy: 0.9807\n",
      "Batch 00944: setting learning rate to 0.013632820898171772.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 0.5852 - accuracy: 0.9808\n",
      "Batch 00945: setting learning rate to 0.013558025178951046.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.5847 - accuracy: 0.9812\n",
      "Batch 00946: setting learning rate to 0.013483229459730321.\n",
      "5888/9600 [=================>............] - ETA: 7s - loss: 0.5847 - accuracy: 0.9811\n",
      "Batch 00947: setting learning rate to 0.013408433740509595.\n",
      "6016/9600 [=================>............] - ETA: 7s - loss: 0.5858 - accuracy: 0.9806\n",
      "Batch 00948: setting learning rate to 0.01333363802128887.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.5853 - accuracy: 0.9806\n",
      "Batch 00949: setting learning rate to 0.013258842302068145.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.5846 - accuracy: 0.9810\n",
      "Batch 00950: setting learning rate to 0.01318404658284742.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 0.5844 - accuracy: 0.9812\n",
      "Batch 00951: setting learning rate to 0.013109250863626706.\n",
      "6528/9600 [===================>..........] - ETA: 6s - loss: 0.5849 - accuracy: 0.9812\n",
      "Batch 00952: setting learning rate to 0.01303445514440598.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.5849 - accuracy: 0.9811\n",
      "Batch 00953: setting learning rate to 0.012959659425185255.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.5853 - accuracy: 0.9810\n",
      "Batch 00954: setting learning rate to 0.01288486370596453.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.5861 - accuracy: 0.9805\n",
      "Batch 00955: setting learning rate to 0.012810067986743804.\n",
      "7040/9600 [=====================>........] - ETA: 5s - loss: 0.5855 - accuracy: 0.9808\n",
      "Batch 00956: setting learning rate to 0.012735272267523079.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.5851 - accuracy: 0.9810\n",
      "Batch 00957: setting learning rate to 0.012660476548302354.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.5849 - accuracy: 0.9811\n",
      "Batch 00958: setting learning rate to 0.012585680829081628.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.5856 - accuracy: 0.9809\n",
      "Batch 00959: setting learning rate to 0.012510885109860903.\n",
      "7552/9600 [======================>.......] - ETA: 4s - loss: 0.5855 - accuracy: 0.9808\n",
      "Batch 00960: setting learning rate to 0.012436089390640191.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.5859 - accuracy: 0.9806\n",
      "Batch 00961: setting learning rate to 0.012361293671419466.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.5858 - accuracy: 0.9803\n",
      "Batch 00962: setting learning rate to 0.01228649795219874.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.5857 - accuracy: 0.9802\n",
      "Batch 00963: setting learning rate to 0.012211702232978013.\n",
      "8064/9600 [========================>.....] - ETA: 3s - loss: 0.5858 - accuracy: 0.9803\n",
      "Batch 00964: setting learning rate to 0.012136906513757288.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.5861 - accuracy: 0.9800\n",
      "Batch 00965: setting learning rate to 0.012062110794536562.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.5862 - accuracy: 0.9799\n",
      "Batch 00966: setting learning rate to 0.011987315075315837.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.5861 - accuracy: 0.9800\n",
      "Batch 00967: setting learning rate to 0.011912519356095112.\n",
      "8576/9600 [=========================>....] - ETA: 2s - loss: 0.5863 - accuracy: 0.9798\n",
      "Batch 00968: setting learning rate to 0.011837723636874386.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.5862 - accuracy: 0.9800\n",
      "Batch 00969: setting learning rate to 0.011762927917653675.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.5869 - accuracy: 0.9796\n",
      "Batch 00970: setting learning rate to 0.01168813219843295.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.5865 - accuracy: 0.9798\n",
      "Batch 00971: setting learning rate to 0.011613336479212224.\n",
      "9088/9600 [===========================>..] - ETA: 1s - loss: 0.5862 - accuracy: 0.9800\n",
      "Batch 00972: setting learning rate to 0.011538540759991498.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.5867 - accuracy: 0.9797\n",
      "Batch 00973: setting learning rate to 0.011463745040770773.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.5870 - accuracy: 0.9795\n",
      "Batch 00974: setting learning rate to 0.011388949321550048.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.5871 - accuracy: 0.9795\n",
      "Batch 00975: setting learning rate to 0.011314153602329322.\n",
      "9600/9600 [==============================] - 20s 2ms/step - loss: 0.5870 - accuracy: 0.9795 - val_loss: 0.1842 - val_accuracy: 0.9725\n",
      "Epoch 14/15\n",
      "\n",
      "Batch 00976: setting learning rate to 0.011239357883108597.\n",
      " 128/9600 [..............................] - ETA: 20s - loss: 0.5656 - accuracy: 0.9844\n",
      "Batch 00977: setting learning rate to 0.011164562163887884.\n",
      " 256/9600 [..............................] - ETA: 19s - loss: 0.5766 - accuracy: 0.9805\n",
      "Batch 00978: setting learning rate to 0.011089766444667158.\n",
      " 384/9600 [>.............................] - ETA: 19s - loss: 0.5692 - accuracy: 0.9870\n",
      "Batch 00979: setting learning rate to 0.011014970725446433.\n",
      " 512/9600 [>.............................] - ETA: 18s - loss: 0.5741 - accuracy: 0.9863\n",
      "Batch 00980: setting learning rate to 0.010940175006225707.\n",
      " 640/9600 [=>............................] - ETA: 18s - loss: 0.5748 - accuracy: 0.9875\n",
      "Batch 00981: setting learning rate to 0.010865379287004982.\n",
      " 768/9600 [=>............................] - ETA: 17s - loss: 0.5799 - accuracy: 0.9844\n",
      "Batch 00982: setting learning rate to 0.010790583567784257.\n",
      " 896/9600 [=>............................] - ETA: 17s - loss: 0.5854 - accuracy: 0.9810\n",
      "Batch 00983: setting learning rate to 0.010715787848563531.\n",
      "1024/9600 [==>...........................] - ETA: 17s - loss: 0.5831 - accuracy: 0.9824\n",
      "Batch 00984: setting learning rate to 0.010640992129342806.\n",
      "1152/9600 [==>...........................] - ETA: 17s - loss: 0.5816 - accuracy: 0.9818\n",
      "Batch 00985: setting learning rate to 0.01056619641012208.\n",
      "1280/9600 [===>..........................] - ETA: 16s - loss: 0.5827 - accuracy: 0.9820\n",
      "Batch 00986: setting learning rate to 0.010491400690901369.\n",
      "1408/9600 [===>..........................] - ETA: 16s - loss: 0.5824 - accuracy: 0.9815\n",
      "Batch 00987: setting learning rate to 0.010416604971680643.\n",
      "1536/9600 [===>..........................] - ETA: 16s - loss: 0.5806 - accuracy: 0.9831\n",
      "Batch 00988: setting learning rate to 0.010341809252459918.\n",
      "1664/9600 [====>.........................] - ETA: 15s - loss: 0.5819 - accuracy: 0.9826\n",
      "Batch 00989: setting learning rate to 0.01026701353323919.\n",
      "1792/9600 [====>.........................] - ETA: 15s - loss: 0.5817 - accuracy: 0.9821\n",
      "Batch 00990: setting learning rate to 0.010192217814018465.\n",
      "1920/9600 [=====>........................] - ETA: 15s - loss: 0.5824 - accuracy: 0.9828\n",
      "Batch 00991: setting learning rate to 0.01011742209479774.\n",
      "2048/9600 [=====>........................] - ETA: 15s - loss: 0.5830 - accuracy: 0.9824\n",
      "Batch 00992: setting learning rate to 0.010042626375577015.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 0.5819 - accuracy: 0.9830\n",
      "Batch 00993: setting learning rate to 0.00996783065635629.\n",
      "2304/9600 [======>.......................] - ETA: 14s - loss: 0.5813 - accuracy: 0.9831\n",
      "Batch 00994: setting learning rate to 0.009893034937135564.\n",
      "2432/9600 [======>.......................] - ETA: 14s - loss: 0.5817 - accuracy: 0.9827\n",
      "Batch 00995: setting learning rate to 0.009818239217914852.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560/9600 [=======>......................] - ETA: 14s - loss: 0.5830 - accuracy: 0.9816\n",
      "Batch 00996: setting learning rate to 0.009743443498694127.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 0.5830 - accuracy: 0.9818\n",
      "Batch 00997: setting learning rate to 0.009668647779473401.\n",
      "2816/9600 [=======>......................] - ETA: 13s - loss: 0.5838 - accuracy: 0.9819\n",
      "Batch 00998: setting learning rate to 0.009593852060252676.\n",
      "2944/9600 [========>.....................] - ETA: 13s - loss: 0.5837 - accuracy: 0.9820\n",
      "Batch 00999: setting learning rate to 0.00951905634103195.\n",
      "3072/9600 [========>.....................] - ETA: 13s - loss: 0.5856 - accuracy: 0.9811\n",
      "Batch 01000: setting learning rate to 0.009444260621811225.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 0.5865 - accuracy: 0.9803\n",
      "Batch 01001: setting learning rate to 0.0093694649025905.\n",
      "3328/9600 [=========>....................] - ETA: 12s - loss: 0.5859 - accuracy: 0.9808\n",
      "Batch 01002: setting learning rate to 0.009294669183369774.\n",
      "3456/9600 [=========>....................] - ETA: 12s - loss: 0.5850 - accuracy: 0.9812\n",
      "Batch 01003: setting learning rate to 0.009219873464149047.\n",
      "3584/9600 [==========>...................] - ETA: 12s - loss: 0.5849 - accuracy: 0.9810\n",
      "Batch 01004: setting learning rate to 0.009145077744928336.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 0.5845 - accuracy: 0.9811\n",
      "Batch 01005: setting learning rate to 0.00907028202570761.\n",
      "3840/9600 [===========>..................] - ETA: 11s - loss: 0.5834 - accuracy: 0.9815\n",
      "Batch 01006: setting learning rate to 0.008995486306486885.\n",
      "3968/9600 [===========>..................] - ETA: 11s - loss: 0.5837 - accuracy: 0.9816\n",
      "Batch 01007: setting learning rate to 0.00892069058726616.\n",
      "4096/9600 [===========>..................] - ETA: 11s - loss: 0.5832 - accuracy: 0.9819\n",
      "Batch 01008: setting learning rate to 0.008845894868045434.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 0.5832 - accuracy: 0.9818\n",
      "Batch 01009: setting learning rate to 0.008771099148824709.\n",
      "4352/9600 [============>.................] - ETA: 10s - loss: 0.5843 - accuracy: 0.9809\n",
      "Batch 01010: setting learning rate to 0.008696303429603983.\n",
      "4480/9600 [=============>................] - ETA: 10s - loss: 0.5844 - accuracy: 0.9808\n",
      "Batch 01011: setting learning rate to 0.008621507710383258.\n",
      "4608/9600 [=============>................] - ETA: 10s - loss: 0.5838 - accuracy: 0.9807\n",
      "Batch 01012: setting learning rate to 0.008546711991162546.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.5841 - accuracy: 0.9804 \n",
      "Batch 01013: setting learning rate to 0.008471916271941821.\n",
      "4864/9600 [==============>...............] - ETA: 9s - loss: 0.5845 - accuracy: 0.9803\n",
      "Batch 01014: setting learning rate to 0.008397120552721094.\n",
      "4992/9600 [==============>...............] - ETA: 9s - loss: 0.5840 - accuracy: 0.9806\n",
      "Batch 01015: setting learning rate to 0.008322324833500368.\n",
      "5120/9600 [===============>..............] - ETA: 9s - loss: 0.5846 - accuracy: 0.9807\n",
      "Batch 01016: setting learning rate to 0.008247529114279643.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.5843 - accuracy: 0.9806\n",
      "Batch 01017: setting learning rate to 0.008172733395058918.\n",
      "5376/9600 [===============>..............] - ETA: 8s - loss: 0.5842 - accuracy: 0.9805\n",
      "Batch 01018: setting learning rate to 0.008097937675838192.\n",
      "5504/9600 [================>.............] - ETA: 8s - loss: 0.5836 - accuracy: 0.9807\n",
      "Batch 01019: setting learning rate to 0.008023141956617467.\n",
      "5632/9600 [================>.............] - ETA: 8s - loss: 0.5849 - accuracy: 0.9803\n",
      "Batch 01020: setting learning rate to 0.007948346237396741.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.5857 - accuracy: 0.9799\n",
      "Batch 01021: setting learning rate to 0.00787355051817603.\n",
      "5888/9600 [=================>............] - ETA: 7s - loss: 0.5863 - accuracy: 0.9796\n",
      "Batch 01022: setting learning rate to 0.0077987547989553044.\n",
      "6016/9600 [=================>............] - ETA: 7s - loss: 0.5864 - accuracy: 0.9797\n",
      "Batch 01023: setting learning rate to 0.007723959079734579.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.5859 - accuracy: 0.9800\n",
      "Batch 01024: setting learning rate to 0.007649163360513853.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.5854 - accuracy: 0.9804\n",
      "Batch 01025: setting learning rate to 0.007574367641293127.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 0.5847 - accuracy: 0.9808\n",
      "Batch 01026: setting learning rate to 0.007499571922072402.\n",
      "6528/9600 [===================>..........] - ETA: 6s - loss: 0.5851 - accuracy: 0.9802\n",
      "Batch 01027: setting learning rate to 0.0074247762028516765.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.5863 - accuracy: 0.9799\n",
      "Batch 01028: setting learning rate to 0.007349980483630951.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.5859 - accuracy: 0.9801\n",
      "Batch 01029: setting learning rate to 0.007275184764410226.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.5859 - accuracy: 0.9800\n",
      "Batch 01030: setting learning rate to 0.007200389045189513.\n",
      "7040/9600 [=====================>........] - ETA: 5s - loss: 0.5866 - accuracy: 0.9795\n",
      "Batch 01031: setting learning rate to 0.007125593325968788.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.5871 - accuracy: 0.9792\n",
      "Batch 01032: setting learning rate to 0.0070507976067480625.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.5871 - accuracy: 0.9790\n",
      "Batch 01033: setting learning rate to 0.006976001887527337.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.5872 - accuracy: 0.9789\n",
      "Batch 01034: setting learning rate to 0.006901206168306612.\n",
      "7552/9600 [======================>.......] - ETA: 4s - loss: 0.5875 - accuracy: 0.9784\n",
      "Batch 01035: setting learning rate to 0.006826410449085886.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.5879 - accuracy: 0.9784\n",
      "Batch 01036: setting learning rate to 0.006751614729865161.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.5881 - accuracy: 0.9784\n",
      "Batch 01037: setting learning rate to 0.0066768190106444355.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.5878 - accuracy: 0.9785\n",
      "Batch 01038: setting learning rate to 0.006602023291423709.\n",
      "8064/9600 [========================>.....] - ETA: 3s - loss: 0.5875 - accuracy: 0.9785\n",
      "Batch 01039: setting learning rate to 0.006527227572202998.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.5876 - accuracy: 0.9784\n",
      "Batch 01040: setting learning rate to 0.006452431852982272.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.5875 - accuracy: 0.9787\n",
      "Batch 01041: setting learning rate to 0.006377636133761547.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.5872 - accuracy: 0.9789\n",
      "Batch 01042: setting learning rate to 0.0063028404145408215.\n",
      "8576/9600 [=========================>....] - ETA: 2s - loss: 0.5872 - accuracy: 0.9790\n",
      "Batch 01043: setting learning rate to 0.006228044695320096.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.5868 - accuracy: 0.9792\n",
      "Batch 01044: setting learning rate to 0.00615324897609937.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.5872 - accuracy: 0.9791\n",
      "Batch 01045: setting learning rate to 0.006078453256878644.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.5871 - accuracy: 0.9791\n",
      "Batch 01046: setting learning rate to 0.006003657537657919.\n",
      "9088/9600 [===========================>..] - ETA: 1s - loss: 0.5871 - accuracy: 0.9793\n",
      "Batch 01047: setting learning rate to 0.0059288618184372074.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.5867 - accuracy: 0.9795\n",
      "Batch 01048: setting learning rate to 0.005854066099216482.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.5866 - accuracy: 0.9795\n",
      "Batch 01049: setting learning rate to 0.005779270379995756.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.5864 - accuracy: 0.9797\n",
      "Batch 01050: setting learning rate to 0.00570447466077503.\n",
      "9600/9600 [==============================] - 21s 2ms/step - loss: 0.5859 - accuracy: 0.9798 - val_loss: 0.1784 - val_accuracy: 0.9787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15\n",
      "\n",
      "Batch 01051: setting learning rate to 0.005629678941554305.\n",
      " 128/9600 [..............................] - ETA: 20s - loss: 0.5705 - accuracy: 0.9688\n",
      "Batch 01052: setting learning rate to 0.0055548832223335795.\n",
      " 256/9600 [..............................] - ETA: 19s - loss: 0.5817 - accuracy: 0.9766\n",
      "Batch 01053: setting learning rate to 0.005480087503112854.\n",
      " 384/9600 [>.............................] - ETA: 18s - loss: 0.5886 - accuracy: 0.9792\n",
      "Batch 01054: setting learning rate to 0.005405291783892129.\n",
      " 512/9600 [>.............................] - ETA: 18s - loss: 0.5877 - accuracy: 0.9805\n",
      "Batch 01055: setting learning rate to 0.005330496064671403.\n",
      " 640/9600 [=>............................] - ETA: 17s - loss: 0.5803 - accuracy: 0.9844\n",
      "Batch 01056: setting learning rate to 0.005255700345450691.\n",
      " 768/9600 [=>............................] - ETA: 17s - loss: 0.5838 - accuracy: 0.9792\n",
      "Batch 01057: setting learning rate to 0.0051809046262299655.\n",
      " 896/9600 [=>............................] - ETA: 17s - loss: 0.5834 - accuracy: 0.9810\n",
      "Batch 01058: setting learning rate to 0.00510610890700924.\n",
      "1024/9600 [==>...........................] - ETA: 17s - loss: 0.5794 - accuracy: 0.9834\n",
      "Batch 01059: setting learning rate to 0.005031313187788515.\n",
      "1152/9600 [==>...........................] - ETA: 16s - loss: 0.5813 - accuracy: 0.9818\n",
      "Batch 01060: setting learning rate to 0.004956517468567789.\n",
      "1280/9600 [===>..........................] - ETA: 16s - loss: 0.5836 - accuracy: 0.9820\n",
      "Batch 01061: setting learning rate to 0.004881721749347064.\n",
      "1408/9600 [===>..........................] - ETA: 16s - loss: 0.5821 - accuracy: 0.9830\n",
      "Batch 01062: setting learning rate to 0.0048069260301263385.\n",
      "1536/9600 [===>..........................] - ETA: 16s - loss: 0.5835 - accuracy: 0.9824\n",
      "Batch 01063: setting learning rate to 0.004732130310905613.\n",
      "1664/9600 [====>.........................] - ETA: 15s - loss: 0.5815 - accuracy: 0.9826\n",
      "Batch 01064: setting learning rate to 0.004657334591684887.\n",
      "1792/9600 [====>.........................] - ETA: 15s - loss: 0.5852 - accuracy: 0.9810\n",
      "Batch 01065: setting learning rate to 0.004582538872464175.\n",
      "1920/9600 [=====>........................] - ETA: 15s - loss: 0.5829 - accuracy: 0.9823\n",
      "Batch 01066: setting learning rate to 0.00450774315324345.\n",
      "2048/9600 [=====>........................] - ETA: 15s - loss: 0.5802 - accuracy: 0.9834\n",
      "Batch 01067: setting learning rate to 0.0044329474340227245.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 0.5803 - accuracy: 0.9830\n",
      "Batch 01068: setting learning rate to 0.004358151714801999.\n",
      "2304/9600 [======>.......................] - ETA: 14s - loss: 0.5812 - accuracy: 0.9822\n",
      "Batch 01069: setting learning rate to 0.004283355995581273.\n",
      "2432/9600 [======>.......................] - ETA: 14s - loss: 0.5807 - accuracy: 0.9823\n",
      "Batch 01070: setting learning rate to 0.004208560276360547.\n",
      "2560/9600 [=======>......................] - ETA: 14s - loss: 0.5828 - accuracy: 0.9812\n",
      "Batch 01071: setting learning rate to 0.004133764557139822.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 0.5822 - accuracy: 0.9818\n",
      "Batch 01072: setting learning rate to 0.0040589688379190966.\n",
      "2816/9600 [=======>......................] - ETA: 13s - loss: 0.5826 - accuracy: 0.9819\n",
      "Batch 01073: setting learning rate to 0.003984173118698371.\n",
      "2944/9600 [========>.....................] - ETA: 13s - loss: 0.5832 - accuracy: 0.9817\n",
      "Batch 01074: setting learning rate to 0.00390937739947766.\n",
      "3072/9600 [========>.....................] - ETA: 13s - loss: 0.5842 - accuracy: 0.9811\n",
      "Batch 01075: setting learning rate to 0.0038345816802569338.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 0.5832 - accuracy: 0.9819\n",
      "Batch 01076: setting learning rate to 0.0037597859610362084.\n",
      "3328/9600 [=========>....................] - ETA: 12s - loss: 0.5836 - accuracy: 0.9814\n",
      "Batch 01077: setting learning rate to 0.0036849902418154825.\n",
      "3456/9600 [=========>....................] - ETA: 12s - loss: 0.5839 - accuracy: 0.9809\n",
      "Batch 01078: setting learning rate to 0.003610194522594757.\n",
      "3584/9600 [==========>...................] - ETA: 12s - loss: 0.5849 - accuracy: 0.9802\n",
      "Batch 01079: setting learning rate to 0.0035353988033740317.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 0.5834 - accuracy: 0.9806\n",
      "Batch 01080: setting learning rate to 0.0034606030841533063.\n",
      "3840/9600 [===========>..................] - ETA: 11s - loss: 0.5829 - accuracy: 0.9807\n",
      "Batch 01081: setting learning rate to 0.003385807364932581.\n",
      "3968/9600 [===========>..................] - ETA: 11s - loss: 0.5834 - accuracy: 0.9803\n",
      "Batch 01082: setting learning rate to 0.003311011645711869.\n",
      "4096/9600 [===========>..................] - ETA: 11s - loss: 0.5831 - accuracy: 0.9807\n",
      "Batch 01083: setting learning rate to 0.003236215926491143.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 0.5822 - accuracy: 0.9813\n",
      "Batch 01084: setting learning rate to 0.0031614202072704177.\n",
      "4352/9600 [============>.................] - ETA: 10s - loss: 0.5824 - accuracy: 0.9812\n",
      "Batch 01085: setting learning rate to 0.0030866244880496923.\n",
      "4480/9600 [=============>................] - ETA: 10s - loss: 0.5828 - accuracy: 0.9810\n",
      "Batch 01086: setting learning rate to 0.003011828768828967.\n",
      "4608/9600 [=============>................] - ETA: 10s - loss: 0.5836 - accuracy: 0.9809\n",
      "Batch 01087: setting learning rate to 0.002937033049608241.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.5835 - accuracy: 0.9808 \n",
      "Batch 01088: setting learning rate to 0.0028622373303875156.\n",
      "4864/9600 [==============>...............] - ETA: 9s - loss: 0.5831 - accuracy: 0.9811\n",
      "Batch 01089: setting learning rate to 0.0027874416111667902.\n",
      "4992/9600 [==============>...............] - ETA: 9s - loss: 0.5828 - accuracy: 0.9810\n",
      "Batch 01090: setting learning rate to 0.002712645891946065.\n",
      "5120/9600 [===============>..............] - ETA: 9s - loss: 0.5824 - accuracy: 0.9812\n",
      "Batch 01091: setting learning rate to 0.002637850172725353.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.5824 - accuracy: 0.9809\n",
      "Batch 01092: setting learning rate to 0.0025630544535046275.\n",
      "5376/9600 [===============>..............] - ETA: 8s - loss: 0.5821 - accuracy: 0.9810\n",
      "Batch 01093: setting learning rate to 0.0024882587342839016.\n",
      "5504/9600 [================>.............] - ETA: 8s - loss: 0.5821 - accuracy: 0.9811\n",
      "Batch 01094: setting learning rate to 0.002413463015063176.\n",
      "5632/9600 [================>.............] - ETA: 8s - loss: 0.5820 - accuracy: 0.9810\n",
      "Batch 01095: setting learning rate to 0.002338667295842451.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.5821 - accuracy: 0.9807\n",
      "Batch 01096: setting learning rate to 0.0022638715766217254.\n",
      "5888/9600 [=================>............] - ETA: 7s - loss: 0.5819 - accuracy: 0.9808\n",
      "Batch 01097: setting learning rate to 0.0021890758574009996.\n",
      "6016/9600 [=================>............] - ETA: 7s - loss: 0.5811 - accuracy: 0.9812\n",
      "Batch 01098: setting learning rate to 0.002114280138180274.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.5807 - accuracy: 0.9814\n",
      "Batch 01099: setting learning rate to 0.0020394844189595487.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.5805 - accuracy: 0.9815\n",
      "Batch 01100: setting learning rate to 0.0019646886997388368.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 0.5804 - accuracy: 0.9816\n",
      "Batch 01101: setting learning rate to 0.0018898929805181112.\n",
      "6528/9600 [===================>..........] - ETA: 6s - loss: 0.5804 - accuracy: 0.9816\n",
      "Batch 01102: setting learning rate to 0.0018150972612973857.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.5801 - accuracy: 0.9818\n",
      "Batch 01103: setting learning rate to 0.0017403015420766603.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.5803 - accuracy: 0.9819\n",
      "Batch 01104: setting learning rate to 0.0016655058228559347.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.5802 - accuracy: 0.9821\n",
      "Batch 01105: setting learning rate to 0.0015907101036352093.\n",
      "7040/9600 [=====================>........] - ETA: 5s - loss: 0.5803 - accuracy: 0.9821\n",
      "Batch 01106: setting learning rate to 0.0015159143844144837.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.5804 - accuracy: 0.9821\n",
      "Batch 01107: setting learning rate to 0.0014411186651937583.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.5807 - accuracy: 0.9819\n",
      "Batch 01108: setting learning rate to 0.0013663229459730327.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.5807 - accuracy: 0.9820\n",
      "Batch 01109: setting learning rate to 0.0012915272267523207.\n",
      "7552/9600 [======================>.......] - ETA: 4s - loss: 0.5807 - accuracy: 0.9820\n",
      "Batch 01110: setting learning rate to 0.0012167315075315953.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.5807 - accuracy: 0.9819\n",
      "Batch 01111: setting learning rate to 0.0011419357883108697.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.5802 - accuracy: 0.9822\n",
      "Batch 01112: setting learning rate to 0.0010671400690901443.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.5801 - accuracy: 0.9822\n",
      "Batch 01113: setting learning rate to 0.0009923443498694188.\n",
      "8064/9600 [========================>.....] - ETA: 3s - loss: 0.5805 - accuracy: 0.9820\n",
      "Batch 01114: setting learning rate to 0.0009175486306486932.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.5802 - accuracy: 0.9823\n",
      "Batch 01115: setting learning rate to 0.0008427529114279677.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.5804 - accuracy: 0.9822\n",
      "Batch 01116: setting learning rate to 0.0007679571922072423.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.5806 - accuracy: 0.9820\n",
      "Batch 01117: setting learning rate to 0.0006931614729865302.\n",
      "8576/9600 [=========================>....] - ETA: 2s - loss: 0.5808 - accuracy: 0.9818\n",
      "Batch 01118: setting learning rate to 0.0006183657537658048.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.5811 - accuracy: 0.9816\n",
      "Batch 01119: setting learning rate to 0.0005435700345450793.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.5809 - accuracy: 0.9818\n",
      "Batch 01120: setting learning rate to 0.0004687743153243538.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.5810 - accuracy: 0.9817\n",
      "Batch 01121: setting learning rate to 0.0003939785961036283.\n",
      "9088/9600 [===========================>..] - ETA: 1s - loss: 0.5808 - accuracy: 0.9818\n",
      "Batch 01122: setting learning rate to 0.00031918287688290276.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.5809 - accuracy: 0.9818\n",
      "Batch 01123: setting learning rate to 0.00024438715766217725.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.5810 - accuracy: 0.9819\n",
      "Batch 01124: setting learning rate to 0.00016959143844145176.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.5812 - accuracy: 0.9818\n",
      "Batch 01125: setting learning rate to 9.479571922072626e-05.\n",
      "9600/9600 [==============================] - 21s 2ms/step - loss: 0.5817 - accuracy: 0.9817 - val_loss: 0.1654 - val_accuracy: 0.9742\n",
      "Acc -> Val acc: 0.0213,Test (last) acc: 0.0208\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de5zVdb3v8dcbGB1ABATU4aLQPpTKOAw4okmZRnlPzEwpK6TatEuP2tllWI8yK/bD9m6T+dilD9qiViSxyQt1TEPDYz6ON2jjCF4OpCjTgIwYCKEk8Dl/rN+MC1hzg/Vba9Za7+fjsVhrfX+X9ZkB5j2/y+f3U0RgZmbWkV7FLsDMzHo+h4WZmXXKYWFmZp1yWJiZWaccFmZm1qk+xS4gDUOHDo3Ro0cXuwwzs5KyfPny1yJiWK5pZRkWo0ePZtmyZcUuw8yspEh6ub1p3g1lZmadcliYmVmnHBZmZtapsjxmYWY9z9tvv01TUxNvvfVWsUupeNXV1YwcOZKqqqouL+OwMLOCaGpqYsCAAYwePRpJxS6nYkUEmzZtoqmpiTFjxnR5Oe+GMrOCeOuttxgyZIiDosgkMWTIkG5v4TkszKxgHBQ9w/78PTgszMysUw6L9syfD6NHQ69emef584tdkZkV2CGHHAJAc3MzF110Uc55TjvttE6bgG+88Ua2b9/e9v6cc85h8+bN+Su0ABwWucyfDzNnwssvQ0TmeeZMB4ZZhRo+fDiLFi3a7+X3Dov77ruPQYMG5aO0gnFY5PKNb/B69OFfTpvB00eOzYxt3w7f+EZx6zKzA/K1r32Nn/zkJ23vv/3tb3P99dczZcoUJk6cyPHHH8+99967z3Jr166ltrYWgDfffJNp06ZRV1fHJZdcwptvvtk23xe/+EUaGhoYN24c1113HQA33XQTzc3NnH766Zx++ulA5pJEr732GgBz5syhtraW2tpabrzxxrbPO/bYY/nHf/xHxo0bxxlnnLHH5xSDT53N5ZVXiOoBzD3pYwx/o4XxG1a3jZvZgbv+N6t4tvmNvK7zuOGHct1HxnU4z7Rp07j66qv50pe+BMDChQu5//77+fKXv8yhhx7Ka6+9xsknn8z555/f7kHgm2++mX79+tHY2EhjYyMTJ05smzZ79mwOO+wwdu3axZQpU2hsbOTKK69kzpw5LF26lKFDh+6xruXLl3PbbbfxxBNPEBGcdNJJfOADH2Dw4MGsXr2aO++8k5/+9KdcfPHF/PrXv+ZTn/rUAX6X9p+3LHI56igOe/MNDtr5d9YPGLrHuJmVrgkTJrBx40aam5t5+umnGTx4MDU1NXz961+nrq6OD33oQ/zlL3/h1VdfbXcdjzzySNsP7bq6Ourq6tqmLVy4kIkTJzJhwgRWrVrFs88+22E9jz76KB/96Efp378/hxxyCBdeeCF//OMfARgzZgz19fUAnHDCCaxdu/YAv/oD4y2LXGbPRjNnUrP1NdYfmoRFv34we3Zx6zIrE51tAaTpoosuYtGiRWzYsIFp06Yxf/58WlpaWL58OVVVVYwePbrTHoRcWx0vvfQSP/jBD3jqqacYPHgwl112WafriYh2px188MFtr3v37l303VDessjl0kth7lyOfHsbGwYMhaOPhrlzM+NmVtKmTZvGggULWLRoERdddBFbtmzh8MMPp6qqiqVLl/Lyy+1epRuAU089lfnJyS4rV66ksbERgDfeeIP+/fszcOBAXn31VX73u9+1LTNgwAC2bt2ac1333HMP27dv529/+xt3330373//+/P41eaPtyzac+mlDO+zgqfWvg7zv1bsaswsT8aNG8fWrVsZMWIENTU1XHrppXzkIx+hoaGB+vp6jjnmmA6X/+IXv8iMGTOoq6ujvr6eSZMmATB+/HgmTJjAuHHjeNe73sXkyZPblpk5cyZnn302NTU1LF26tG184sSJXHbZZW3r+PznP8+ECROKvsspF3W0GVSqGhoaIh83P/r+/c/zn398kRe+eza9ernz1OxAPPfccxx77LHFLsMSuf4+JC2PiIZc83s3VAdqBlbz9q7gtb/tKHYpZmZF5bDoQM3AvgBs2OJLKptZZUstLCRVS3pS0tOSVkm6Phm/XdJLklYkj/pkXJJukrRGUqOkiVnrmi5pdfKYnlbNe6sZWA3AeoeFmVW4NA9w7wA+GBHbJFUBj0pqPT3gqxGxd+/82cDY5HEScDNwkqTDgOuABiCA5ZIWR8RfU6wdyAqLzcU9Zc3MrNhS27KIjG3J26rk0dHR9KnAz5LlHgcGSaoBzgSWRMTrSUAsAc5Kq+5sh/U/iIP69GL9G96yMLPKluoxC0m9Ja0ANpL5gf9EMml2sqvph5JaO09GAOuyFm9Kxtob3/uzZkpaJmlZS0tLvuqnZmA16zc7LMyssqUaFhGxKyLqgZHAJEm1wLXAMcCJwGFAaxNDrnNTo4PxvT9rbkQ0RETDsGHD8lI/wJGHVvsAt1kZ2Lx58x4XEeyqUryceBoKcjZURGwGHgbOioj1ya6mHcBtwKRktiZgVNZiI4HmDsYLYvigvjRv8TELs1LXXljs2rWrw+VK8XLiaUjzbKhhkgYlr/sCHwKeT45DoMzFVS4AViaLLAY+k5wVdTKwJSLWAw8AZ0gaLGkwcEYyVhBHDqzm1TfeYvfu8mteNKsks2bN4s9//jP19fWceOKJnH766Xzyk5/k+OOPB+CCCy7ghBNOYNy4ccydO7dtudbLiffEy4YXUppnQ9UAd0jqTSaUFkbEbyX9QdIwMruXVgD/lMx/H3AOsAbYDswAiIjXJX0XeCqZ7zsR8XqKde9heFZj3uEDqgv1sWbl7XezYMMz+V3nkcfD2Te0O/mGG25g5cqVrFixgocffphzzz2XlStXMmbMGADmzZvHYYcdxptvvsmJJ57Ixz72MYYMGbLHOnraZcMLKbWwiIhGYEKO8Q+2M38Al7czbR4wL68FdtGRWY15Dguz8jFp0qS2oIDMTYruvvtuANatW8fq1av3CYuedtnwQvKFBDvR2mvRvPkt6kYWuRizctHBFkCh9O/fv+31ww8/zIMPPshjjz1Gv379OO2003JeXrynXTa8kHy5j060hsUGH+Q2K2ntXSYcYMuWLQwePJh+/frx/PPP8/jjjxe4up7PWxadcGOeWXkYMmQIkydPpra2lr59+3LEEUe0TTvrrLO45ZZbqKur4z3veQ8nn3xyESvtmRwWnXBjnln5+OUvf5lz/OCDD97jZkXZWo9LDB06lJUrV7aNf+UrX8l7fT2Zd0N1gRvzzKzSOSy6wI15ZlbpHBZd4MY8M6t0DosuGO475plZhXNYdMGRvmOemVU4h0UXZDfmmZlVIodFF7gxz8wqncOiC9oa87wbyszy6OGHH+a8885rd/rtt9/OFVdcUcCK2uew6IK2xjyHhVnhzJ8Po0dDr16Z5/nzi11RRXNYdJEb88wKaP58mDkTXn4ZIjLPM2ceUGCsXbuWY445hs9//vPU1tZy6aWX8uCDDzJ58mTGjh3Lk08+CcCTTz7JKaecwoQJEzjllFN44YUXAJgzZw6f/exnAXjmmWeora1l+/bte3zGSSedxKpVq9ren3baaSxfvrzddXbHyy+/zJQpU6irq2PKlCm88sorAPzXf/0XtbW1jB8/nlNPPRWAVatWMWnSJOrr66mrq2P16tXd/4btLSLK7nHCCSdEvl294L9j8g0P5X29ZpXi2Wef7frMRx8dkYmJPR9HH73fn//SSy9F7969o7GxMXbt2hUTJ06MGTNmxO7du+Oee+6JqVOnRkTEli1b4u23346IiCVLlsSFF14YERG7du2K97///XHXXXfFCSecEI8++ug+nzFnzpz41re+FRERzc3NMXbs2A7XuXTp0jj33HPbrfm2226Lyy+/PCIizjvvvLj99tsjIuLWW29tq7e2tjaampoiIuKvf/1rRERcccUV8Ytf/CIiInbs2BHbt2/fZ925/j6AZdHOz1VfG6qLshvzevXKdVtwM8ub5LfmLo930ZgxY9rujDdu3DimTJmCJI4//vi2a0Bt2bKF6dOns3r1aiTx9ttvA9CrVy9uv/126urq+MIXvsDkyZP3Wf/FF1/Mhz/8Ya6//noWLlzIxz/+8Q7X2R2PPfYYd911FwCf/vSnueaaawCYPHkyl112GRdffDEXXnghAO9973uZPXs2TU1NXHjhhYwdO7bbn7c374bqIjfmmRXQUUd1b7yLsu9H0atXr7b3vXr1YufOnQB885vf5PTTT2flypX85je/2eO+FqtXr+aQQw6hubk55/pHjBjBkCFDaGxs5Fe/+hXTpk3rdJ37K3Nnarjlllv43ve+x7p166ivr2fTpk188pOfZPHixfTt25czzzyTP/zhDwf8eQ6LLnJjnlkBzZ4N/frtOdavX2Y8ZVu2bGHEiBFA5myk7PGrrrqKRx55hE2bNrFo0aKcy0+bNo1//dd/ZcuWLW1bMe2tsztOOeUUFixYAMD8+fN53/veB8Cf//xnTjrpJL7zne8wdOhQ1q1bx4svvsi73vUurrzySs4//3waGxv36zOzpRYWkqolPSnpaUmrJF2fjI+R9ISk1ZJ+JemgZPzg5P2aZProrHVdm4y/IOnMtGruiBvzzAro0kth7lw4+miQMs9z52bGU3bNNddw7bXXMnnyZHbt2tU2/uUvf5kvfelLvPvd7+bWW29l1qxZbNy4cZ/lL7roIhYsWMDFF1/c6Tq746abbuK2226jrq6On//85/zoRz8C4Ktf/SrHH388tbW1nHrqqYwfP55f/epX1NbWUl9fz/PPP89nPvOZ/frMbMoc08g/ZbaR+kfENklVwKPAVcD/Au6KiAWSbgGejoibJX0JqIuIf5I0DfhoRFwi6TjgTmASMBx4EHh3RLT7HW9oaIhly5bl9evZtG0HJ3zvQb79keO4bPKYzhcwsz0899xzHHvsscUuwxK5/j4kLY+Ihlzzp7ZlkRxc35a8rUoeAXwQaN1+uwO4IHk9NXlPMn1KEjhTgQURsSMiXgLWkAmOgnJjnplVslSPWUjqLWkFsBFYAvwZ2BwRO5NZmoARyesRwDqAZPoWYEj2eI5lsj9rpqRlkpa1tLSk8bW4Mc/MUnHbbbdRX1+/x+Pyyy8vdll7SPXU2WRXUb2kQcDdQK5t0Nb9YLnOR40Oxvf+rLnAXMjshtqvgjtx5KHVrPf1ocz2W0S0ncVj75gxYwYzZswo2Oftz+GHgpwNFRGbgYeBk4FBklpDaiTQeg5aEzAKIJk+EHg9ezzHMgU1fFBfb1mY7afq6mo2bdq0Xz+oLH8igk2bNlFdXd2t5VLbspA0DHg7IjZL6gt8CPg+sBS4CFgATAfuTRZZnLx/LJn+h4gISYuBX0qaQ+YA91jgybTq7ogb88z238iRI2lqaiKN3cTWPdXV1YwcObJby6S5G6oGuENSbzJbMAsj4reSngUWSPoe8N/Arcn8twI/l7SGzBbFNICIWCVpIfAssBO4vKMzodKU3Zh3+IDupbJZpauqqmLMGJ9JWKpSC4uIaAQm5Bh/kRxnM0XEW8DH21nXbCD9bpxOZDfmOSzMrJK4g7sb3JhnZpXKYdENvmOemVUqh0U3uDHPzCqVw6Ib3JhnZpXKYdFNbswzs0rksOgmN+aZWSVyWHRTdmOemVmlcFh0k++YZ2aVyGHRTb5jnplVIodFN7kxz8wqkcOim9yYZ2aVyGHRTW7MM7NK5LDoJjfmmVklcljsh0xYeDeUmVUOh8V+qBnoxjwzqywOi/3gxjwzqzQOi/3gxjwzqzSphYWkUZKWSnpO0ipJVyXj35b0F0krksc5WctcK2mNpBcknZk1flYytkbSrLRq7qrWxrz17rUwswqR5j24dwL/HBF/kjQAWC5pSTLthxHxg+yZJR1H5r7b44DhwIOS3p1M/jHwYaAJeErS4oh4NsXaO9Taa7F+y1uMH1WsKszMCifNe3CvB9Ynr7dKeg4Y0cEiU4EFEbEDeEnSGt65V/ea5N7dSFqQzFv0sHBjnplVioIcs5A0GpgAPJEMXSGpUdI8SYOTsRHAuqzFmpKx9sb3/oyZkpZJWtbS0pLnr2BPbswzs0qTelhIOgT4NXB1RLwB3Az8A1BPZsvj31tnzbF4dDC+50DE3IhoiIiGYcOG5aX29rgxz8wqTZrHLJBURSYo5kfEXQAR8WrW9J8Cv03eNgHZRwBGAs3J6/bGi8aNeWZWSdI8G0rArcBzETEna7wma7aPAiuT14uBaZIOljQGGAs8CTwFjJU0RtJBZA6CL06r7q5yY56ZVZI0tywmA58GnpG0Ihn7OvAJSfVkdiWtBb4AEBGrJC0kc+B6J3B5ROwCkHQF8ADQG5gXEatSrLtLarIa83r1yrWnzMysfKR5NtSj5D7ecF8Hy8wGZucYv6+j5YqhJqsx7/AB1cUux8wsVe7g3k9uzDOzSuKw2E/ZjXlmZuXOYbGf3gkLnxFlZuXPYbGfWhvzNnjLwswqgMNiP7kxz8wqicPiALgxz8wqhcPiALgxz8wqhcPiANT4jnlmViEcFgegxnfMM7MK4bA4ADVuzDOzCuGwOABHujHPzCqEw+IAuDHPzCqFw+IAuDHPzCqFw+IAtDbmNTsszKzMOSwOUM3AajZ4N5SZlTmHxQFyY56ZVQKHxQFyY56ZVYI078E9StJSSc9JWiXpqmT8MElLJK1Ongcn45J0k6Q1kholTcxa1/Rk/tWSpqdV8/5wY56ZVYI0tyx2Av8cEccCJwOXSzoOmAU8FBFjgYeS9wBnA2OTx0zgZsiEC3AdcBIwCbiuNWB6AjfmmVklSC0sImJ9RPwpeb0VeA4YAUwF7khmuwO4IHk9FfhZZDwODJJUA5wJLImI1yPir8AS4Ky06u4uN+aZWSXoUlhIukrSocmuolsl/UnSGV39EEmjgQnAE8AREbEeMoECHJ7MNgJYl7VYUzLW3vjenzFT0jJJy1paWrpa2gFzY56ZVYKubll8NiLeAM4AhgEzgBu6sqCkQ4BfA1cn62h31hxj0cH4ngMRcyOiISIahg0b1pXS8sKNeWZWCboaFq0/sM8BbouIp8n9Q3zPhaQqMkExPyLuSoZfTXYvkTxvTMabgFFZi48EmjsY7xHcmGdmlaCrYbFc0u/JhMUDkgYAuztaQJKAW4HnImJO1qTFQOsZTdOBe7PGP5Ps6joZ2JLspnoAOEPS4OTA9hnJWI/hxjwzK3d9ujjf54B64MWI2J6coTSjk2UmA58GnpG0Ihn7OpndVwslfQ54Bfh4Mu0+MmG0Btjeuv6IeF3Sd4Gnkvm+ExGvd7HugqgZ2JcnX+pRJZmZ5VVXw+K9wIqI+JukTwETgR91tEBEPEr7u6qm5Jg/gMvbWdc8YF4Xay247Ma8Xr063TtnZlZyurob6mZgu6TxwDXAy8DPUquqxNQMrGbnbjfmmVn56mpY7Ex+858K/CgifgQMSK+s0uLGPDMrd10Ni62SriVzDOJ/S+oNVKVXVmlxY56ZlbuuhsUlwA4y/RYbyDTF/VtqVZWY4YOSLQufEWVmZapLYZEExHxgoKTzgLciwscsEoP7Vbkxz8zKWlcv93Ex8CSZ01wvBp6QdFGahZUSN+aZWbnr6qmz3wBOjIiNAJKGAQ8Ci9IqrNS4Mc/MyllXj1n0ag2KxKZuLFsRagb2pdlnQ5lZmerqlsX9kh4A7kzeX0Km49oSbswzs3LWpbCIiK9K+hiZS3gImBsRd6daWYlpa8zbtoPDD60udjlmZnnV1S0LIuLXZK4gazm0NeZtecthYWZlp8OwkLSVHPeOILN1ERFxaCpVlaDsxrzxozqZ2cysxHQYFhHhS3p0kRvzzKyc+YymPBncr4qD3ZhnZmXKYZEnbswzs3LmsMijI92YZ2ZlymGRR27MM7NylVpYSJonaaOklVlj35b0F0krksc5WdOulbRG0guSzswaPysZWyNpVlr15kN2Y56ZWTlJc8viduCsHOM/jIj65HEfgKTjgGnAuGSZn0jqndw348fA2cBxwCeSeXuk7MY8M7NyklpYRMQjwOtdnH0qsCAidkTES8AaYFLyWBMRL0bE34EFybw9UnZjnplZOSnGMYsrJDUmu6kGJ2MjgHVZ8zQlY+2N70PSTEnLJC1raWlJo+5OvdOY54PcZlZeCh0WNwP/ANQD64F/T8ZzXXkvOhjfdzBibkQ0RETDsGHD8lFrt73TmOctCzMrL12+NlQ+RMSrra8l/RT4bfK2Cci+SMZIoDl53d54j+PGPDMrVwXdspBUk/X2o0DrmVKLgWmSDpY0BhhL5s58TwFjJY2RdBCZg+CLC1lzd7gxz8zKVWpbFpLuBE4DhkpqAq4DTpNUT2ZX0lrgCwARsUrSQuBZYCdweUTsStZzBfAA0BuYFxGr0qo5H9yYZ2blKLWwiIhP5Bi+tYP5ZwOzc4zfRwndaGn4wL488VJXTwIzMysN7uDOsyPdmGdmZchhkWduzDOzcuSwyDM35plZOXJY5Jkb88ysHDks8syNeWZWjhwWedbamOewMLNy4rDIs9bGPIeFmZUTh0UK3JhnZuXGYZGC4b5jnpmVGYdFCtyYZ2blxmGRgppBfd2YZ2ZlxWGRgppDW3stvCvKzMqDwyIFbswzs3LjsEiBG/PMrNw4LFLgxjwzKzcOixS4Mc/Myo3DIiVHDqxm/WYfszCz8pBaWEiaJ2mjpJVZY4dJWiJpdfI8OBmXpJskrZHUKGli1jLTk/lXS5qeVr35NnxgX29ZmFnZSHPL4nbgrL3GZgEPRcRY4KHkPcDZwNjkMRO4GTLhQube3ScBk4DrWgOmp3NjnpmVk9TCIiIeAfa+GfVU4I7k9R3ABVnjP4uMx4FBkmqAM4ElEfF6RPwVWMK+AdQjuTHPzMpJoY9ZHBER6wGS58OT8RHAuqz5mpKx9sZ7PDfmmVk56SkHuJVjLDoY33cF0kxJyyQta2lpyWtx+6NmkBvzzKx8FDosXk12L5E8b0zGm4BRWfONBJo7GN9HRMyNiIaIaBg2bFjeC+8u34vbzMpJocNiMdB6RtN04N6s8c8kZ0WdDGxJdlM9AJwhaXByYPuMZKzHc2OemZWTPmmtWNKdwGnAUElNZM5qugFYKOlzwCvAx5PZ7wPOAdYA24EZABHxuqTvAk8l830nIvY+aN4juTHPzMpJamEREZ9oZ9KUHPMGcHk765kHzMtjaQXjxjwzKxc95QB3WXJjnpmVC4dFilob83a5Mc/MSpzDIkWtjXmb3JhnZiXOYZEiN+aZWblwWKTIjXlmVi4cFilyY56ZlQuHRYrcmGdm5cJhkSI35plZuXBYpMyNeWZWDhwWKXNjnpmVA4dFytyYZ2blwGGRMjfmmVk5cFikrLUxr9m7osyshDksUtbamLfBjXlmVsIcFilzY56ZlQOHRcrcmGdm5cBhkTI35plZOXBYFIAb88ys1BUlLCStlfSMpBWSliVjh0laIml18jw4GZekmyStkdQoaWIxaj4Qbswzs1JXzC2L0yOiPiIakvezgIciYizwUPIe4GxgbPKYCdxc8EoPkBvzzKzU9aTdUFOBO5LXdwAXZI3/LDIeBwZJqilGgfvLjXlmVuqKFRYB/F7Sckkzk7EjImI9QPJ8eDI+AliXtWxTMrYHSTMlLZO0rKWlJcXSu8+NeWZW6voU6XMnR0SzpMOBJZKe72Be5RjbZ39ORMwF5gI0NDT0qP09ezTmjRpU5GrMzLqvKFsWEdGcPG8E7gYmAa+27l5KnjcmszcBo7IWHwk0F67aA9famNe82VsWZlaaCh4WkvpLGtD6GjgDWAksBqYns00H7k1eLwY+k5wVdTKwpXV3Valobczb8IbDwsxKUzF2Qx0B3C2p9fN/GRH3S3oKWCjpc8ArwMeT+e8DzgHWANuBGYUv+cC4Mc/MSl3BwyIiXgTG5xjfBEzJMR7A5QUoLVU1A/u6Mc/MSlZPOnW2rHnLwsxKmcOiQNyYZ2alzGFRIG7MM7NS5rAoEDfmmVkpc1gUiO+YZ2alzGFRIG7MM7NS5rAoEDfmmVkpc1gUSGtjXrN7LcysBDksCqhmYF82+AC3mZUgh0UBuTHPzEqVw6KAaga5Mc/MSpPDooCOHOjGPDMrTQ6LAnJjnpmVKodFAbkxz8xKlcOigNyYZ2alymFRQG7MM7NS5bAoIDfmmVmpKpmwkHSWpBckrZE0K/UPjMg88syNeWZWiopxD+5uk9Qb+DHwYaAJeErS4oh4NrUP3boBfjgODj4EDhqQPB+S9Tyg++97V1EzsJrfNDZz1o2P0Peg3vQ7qDd9q/rQr/V18tzvoD70rcoe67Pn9Ko+Wcv3plcvpfatMDMribAAJgFrkvt3I2kBMBVILyz6HAzvuxp2bIO/b4MdW5PnbbBtYzK+NTO+e2fX1tn7YL5f1Z9v9u3D7m2tGy/B7oAg8zoCdnewioh3QuGt5AEgQbBvYGifF/sjf0GUn3q69AmWRyF/X0tFS7+xTPjKb/K+3lIJixHAuqz3TcBJ2TNImgnMBDjqqKMO/BP7HQZTvtX5fBGwc8e+gdLO+6q/b2Pwzg6a8iIIMgGyc9dudu0Odu6OzPOu3cnr3e+M7Q527cqMBbH3qrqk4/ki68+ck/aYuvdQZyVEe+86WLDdSSnsNnSvPcjfhZLy90NHp7LeUgmLXL/W7PEvOCLmAnMBGhoaCvevW4Kq6syj/9D8rBLonTzMzHqCUjnA3QSMyno/EmguUi1mZhWnVMLiKWCspDGSDgKmAYuLXJOZWcUoid1QEbFT0hXAA2T2zsyLiFVFLsvMrGKURFgARMR9wH3FrsPMrBKVym4oMzMrIoeFmZl1ymFhZmadcliYmVmnFCl0vRabpBbg5TytbijwWp7WlTbXmg7Xmg7Xmo4DqfXoiBiWa0JZhkU+SVoWEQ3FrqMrXGs6XGs6XGs60qrVu6HMzKxTDgszM+uUw6Jzc4tdQDe41nS41nS41nSkUquPWZiZWae8ZWFmZp1yWJiZWaccFu2QdJakFyStkTSr2PW0R9IoSUslPSdplaSril1TZyT1lvTfkn5b7Fo6ImmQpEWSnk++v+8tdk3tkfTl5O9/paQ7JVUXu6ZsknqB6kkAAAUqSURBVOZJ2ihpZdbYYZKWSFqdPA8uZo2t2qn135J/B42S7pY0qJg1tspVa9a0r0gKSXm5K5vDIgdJvYEfA2cDxwGfkHRccatq107gnyPiWOBk4PIeXGurq4Dnil1EF/wIuD8ijgHG00NrljQCuBJoiIhaMpfxn1bcqvZxO3DWXmOzgIciYizwUPK+J7idfWtdAtRGRB3w/4BrC11UO25n31qRNAr4MPBKvj7IYZHbJGBNRLwYEX8HFgBTi1xTThGxPiL+lLzeSuYH2ojiVtU+SSOBc4H/LHYtHZF0KHAqcCtARPw9IjYXt6oO9QH6SuoD9KOH3UkyIh4BXt9reCpwR/L6DuCCghbVjly1RsTvI2Jn8vZxMnfrLLp2vq8APwSuIY+3kXdY5DYCWJf1voke/AO4laTRwATgieJW0qEbyfwj3l3sQjrxLqAFuC3ZZfafkvoXu6hcIuIvwA/I/Ba5HtgSEb8vblVdckRErIfMLz3A4UWup6s+C/yu2EW0R9L5wF8i4ul8rtdhkZtyjPXoc4wlHQL8Grg6It4odj25SDoP2BgRy4tdSxf0ASYCN0fEBOBv9JzdJHtI9vVPBcYAw4H+kj5V3KrKk6RvkNn1O7/YteQiqR/wDeBb+V63wyK3JmBU1vuR9LDN+mySqsgExfyIuKvY9XRgMnC+pLVkdu19UNIviltSu5qApoho3UpbRCY8eqIPAS9FREtEvA3cBZxS5Jq64lVJNQDJ88Yi19MhSdOB84BLo+c2qP0DmV8ank7+n40E/iTpyANdscMit6eAsZLGSDqIzMHCxUWuKSdJIrNf/bmImFPsejoSEddGxMiIGE3me/qHiOiRvwFHxAZgnaT3JENTgGeLWFJHXgFOltQv+fcwhR56MH4vi4HpyevpwL1FrKVDks4CvgacHxHbi11PeyLimYg4PCJGJ//PmoCJyb/nA+KwyCE5kHUF8ACZ/3QLI2JVcatq12Tg02R+S1+RPM4pdlFl4n8C8yU1AvXAvxS5npySrZ9FwJ+AZ8j8v+5Rl6eQdCfwGPAeSU2SPgfcAHxY0moyZ+7cUMwaW7VT638AA4Alyf+xW4paZKKdWtP5rJ67NWVmZj2FtyzMzKxTDgszM+uUw8LMzDrlsDAzs045LMzMrFMOC7NukLQr6xTlFfm8IrGk0bmuHmrWE/QpdgFmJebNiKgvdhFmheYtC7M8kLRW0vclPZk8/kcyfrSkh5L7IDwk6ahk/IjkvghPJ4/Wy3P0lvTT5N4Uv5fUN5n/SknPJutZUKQv0yqYw8Kse/rutRvqkqxpb0TEJDLdvjcmY/8B/Cy5D8J84KZk/Cbg/0TEeDLXnGq9QsBY4McRMQ7YDHwsGZ8FTEjW809pfXFm7XEHt1k3SNoWEYfkGF8LfDAiXkwu7LghIoZIeg2oiYi3k/H1ETFUUgswMiJ2ZK1jNLAkuRkQkr4GVEXE9yTdD2wD7gHuiYhtKX+pZnvwloVZ/kQ7r9ubJ5cdWa938c5xxXPJ3L3xBGB5cpMjs4JxWJjlzyVZz48lr/8v79zi9FLg0eT1Q8AXoe2e5Ie2t1JJvYBREbGUzI2jBgH7bN2Ypcm/nZh1T19JK7Le3x8RrafPHizpCTK/hH0iGbsSmCfpq2TuvDcjGb8KmJtcJXQXmeBY385n9gZ+IWkgmRtz/bCH3+LVypCPWZjlQXLMoiEiXit2LWZp8G4oMzPrlLcszMysU96yMDOzTjkszMysUw4LMzPrlMPCzMw65bAwM7NO/X/nZ6PE8MPmjQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0213 in 6.2 min\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.021250009536743164"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clases = 100 if dataset == 'cifar100' else 10\n",
    "dm = DataManager(dataset, clases=classes, folder_var_mnist=data_folder, num_clases=num_clases) #, max_examples=8000)\n",
    "data = dm.load_data()\n",
    "fitness_cnn.set_params(data=data, verbose=True, batch_size=batch_size, reduce_plateau=redu_plat,\n",
    "               epochs=epochs, cosine_decay=cosine_dec, early_stop=early_stop, \n",
    "               warm_epochs=warm_up_epochs, base_lr=base_lr, smooth_label=smooth, find_lr=lr_find,\n",
    "               precise_epochs=precise_eps, include_time=include_time, test_eps=test_eps, augment=augment)\n",
    "\n",
    "fitness_cnn.calc(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file ../../exp_mnist_grow_v2/cifar10/genetic/0_2020-01-31-15:39/GA_experiment\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "# experiments_folder = '../../exp_cifar10_time'\n",
    "experiments_folder = '../../exp_mnist_grow_v2'\n",
    "dataset = datasets[0]\n",
    "exp_folder = os.path.join(experiments_folder, dataset)\n",
    "folder = os.path.join(exp_folder, 'genetic')\n",
    "\n",
    "\n",
    "generational = TwoLevelGA.load_genetic_algorithm(folder=folder)\n",
    "# generational.finishing_evolution(show=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||CNN|F:1.1|K:5|A:relu|D:0.05||woCAT||1||\n",
      "||CNN|F:1.0|K:5|A:relu|D:0.05||CAT||11||\n",
      "||CNN|F:0.9|K:3|A:prelu|D:0.10||CAT||101||\n",
      "HP->|GR:4.20|CELL:2|BLOCK:2|STEM:45|LR:0.0112|WU:0.2\n",
      "\n",
      "0.9375290000000001\n",
      "0.9523 0.0477\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "winner = generational.best_individual['winner']\n",
    "fit = generational.best_individual['best_fit']\n",
    "test = generational.best_individual[\"test\"]\n",
    "print(winner)\n",
    "print(1 - fit)\n",
    "print(1 - test, test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 32, 32, 3) train samples\n",
      "(10000, 32, 32, 3) validation samples\n",
      "(10000, 32, 32, 3) test samples\n",
      "cutout\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "num_clases = 100 if dataset == 'cifar100' else 10\n",
    "dm = DataManager(dataset, clases=classes, folder_var_mnist=data_folder, num_clases=num_clases) #, max_examples=8000)\n",
    "data = dm.load_data()\n",
    "fitness_cnn.set_params(data=data, verbose=verbose, batch_size=batch_size, reduce_plateau=redu_plat,\n",
    "               epochs=epochs, cosine_decay=cosine_dec, early_stop=early_stop, \n",
    "               warm_epochs=warm_up_epochs, base_lr=base_lr, smooth_label=smooth, find_lr=lr_find,\n",
    "               precise_epochs=precise_eps, include_time=include_time, test_eps=test_eps, augment=augment)\n",
    "print(fitness_cnn.augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 45)   1260        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 45)   180         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 32, 45)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 63)   70938       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 108)  0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 108)  432         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 32, 108)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 76)   205276      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 121)  0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 121)  484         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32, 32, 121)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 80)   87200       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 32, 32, 80)   81920       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 80)   320         p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32, 32, 80)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 129)  258129      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 209)  0           p_re_lu_1[0][0]                  \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 209)  836         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32, 32, 209)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 155)  810030      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 235)  0           p_re_lu_1[0][0]                  \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 235)  940         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32, 32, 235)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 165)  349140      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_2 (PReLU)               (None, 32, 32, 165)  168960      conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 165)  0           p_re_lu_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 165)  660         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 16, 16, 165)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 265)  1093390     dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 16, 16, 430)  0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 430)  1720        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 16, 16, 430)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 319)  3429569     dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 16, 16, 484)  0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 484)  1936        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 16, 16, 484)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 339)  1477023     dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_3 (PReLU)               (None, 16, 16, 339)  86784       conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 339)  1356        p_re_lu_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 16, 16, 339)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 544)  4610944     dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 16, 16, 883)  0           p_re_lu_3[0][0]                  \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 883)  3532        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 16, 16, 883)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 653)  14415628    dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 16, 16, 992)  0           p_re_lu_3[0][0]                  \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 992)  3968        concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 16, 16, 992)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 695)  6205655     dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_4 (PReLU)               (None, 16, 16, 695)  177920      conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 695)          0           p_re_lu_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           6960        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 33,553,090\n",
      "Trainable params: 33,544,908\n",
      "Non-trainable params: 8,182\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 1.5698 - accuracy: 0.5159 - val_loss: 2.3153 - val_accuracy: 0.2674\n",
      "Epoch 2/200\n",
      "50000/50000 [==============================] - 144s 3ms/step - loss: 1.2089 - accuracy: 0.7019 - val_loss: 0.8119 - val_accuracy: 0.7321\n",
      "Epoch 3/200\n",
      "50000/50000 [==============================] - 144s 3ms/step - loss: 1.0332 - accuracy: 0.7871 - val_loss: 0.6999 - val_accuracy: 0.7789\n",
      "Epoch 4/200\n",
      "50000/50000 [==============================] - 144s 3ms/step - loss: 0.8998 - accuracy: 0.8480 - val_loss: 0.5612 - val_accuracy: 0.8155\n",
      "Epoch 5/200\n",
      "50000/50000 [==============================] - 144s 3ms/step - loss: 0.8045 - accuracy: 0.8878 - val_loss: 0.5582 - val_accuracy: 0.8309\n",
      "Epoch 6/200\n",
      "50000/50000 [==============================] - 144s 3ms/step - loss: 0.7263 - accuracy: 0.9231 - val_loss: 0.6148 - val_accuracy: 0.8140\n",
      "Epoch 7/200\n",
      "  640/50000 [..............................] - ETA: 2:16 - loss: 0.6863 - accuracy: 0.9359"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ed43e591233e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfitness_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfitness_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitness_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwinner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Daniel/NeuroEvolution/utils/codification_cnn.py\u001b[0m in \u001b[0;36mcalc\u001b[0;34m(self, chromosome, test, file_model, fp, precise_mode)\u001b[0m\n\u001b[1;32m    565\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m                               verbose=self.verb)\n\u001b[0m\u001b[1;32m    568\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m                 \u001b[0;31m# model = load_model(file_model, {'BatchNormalizationF16': BatchNormalizationF16})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/genetic/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/genetic/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/genetic/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/genetic/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/genetic/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/genetic/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fitness_cnn.verb = True\n",
    "fitness_cnn.augment = False\n",
    "score = fitness_cnn.calc(winner, test=True, precise_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
