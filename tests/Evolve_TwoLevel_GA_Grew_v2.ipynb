{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from time import time\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.codification_cnn import FitnessCNNParallel\n",
    "from utils.codification_grew import FitnessGrow, ChromosomeGrow, HyperParams, Merger\n",
    "from utils.codification_grew import Inputs, MaxPooling, AvPooling, OperationBlock, CNNGrow, IdentityGrow\n",
    "from utils.datamanager import DataManager\n",
    "from GA.geneticAlgorithm import TwoLevelGA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chromosome parameters\n",
    "ChromosomeGrow._max_initial_blocks = 5\n",
    "ChromosomeGrow._grow_prob = 0.15\n",
    "ChromosomeGrow._decrease_prob = 0.25\n",
    "\n",
    "\n",
    "Merger._projection_type = ['normal', 'extend'][1]\n",
    "\n",
    "HyperParams._GROW_RATE_LIMITS = [1.5, 5.]\n",
    "HyperParams._N_CELLS = [1, 2]\n",
    "HyperParams._N_BLOCKS = [2]\n",
    "HyperParams._STEM = [32, 45]\n",
    "\n",
    "OperationBlock._change_op_prob = 0.15\n",
    "OperationBlock._change_concat_prob = 0.15\n",
    "\n",
    "CNNGrow.filters_mul_range = [0.1, 1.2]\n",
    "CNNGrow.possible_activations = ['relu', 'elu', 'prelu']\n",
    "CNNGrow.dropout_range = [0, 0.7]\n",
    "CNNGrow.possible_k = [1, 3, 5]\n",
    "CNNGrow.k_prob = 0.2\n",
    "CNNGrow.drop_prob = 0.2\n",
    "CNNGrow.filter_prob = 0.2\n",
    "CNNGrow.act_prob = 0.2\n",
    "\n",
    "Inputs._mutate_prob = 0.5\n",
    "\n",
    "    \n",
    "data_folder = '../../datasets/MNIST_variations'\n",
    "command = 'python3 ../train_gen.py'\n",
    "verbose = 0\n",
    "\n",
    "gpus = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset params:\n",
    "data_folder = data_folder\n",
    "classes = []\n",
    "\n",
    "# genetic algorithm params:\n",
    "generations = 30\n",
    "population_first_level = 20\n",
    "population_second_level = 8\n",
    "training_hours = 68\n",
    "save_progress = True\n",
    "maximize_fitness = False\n",
    "statistical_validation = False\n",
    "frequency_second_level = 3\n",
    "start_level2 = 1\n",
    "\n",
    "\n",
    "# Fitness params\n",
    "epochs = 15\n",
    "batch_size = 128\n",
    "verbose = verbose\n",
    "redu_plat = False\n",
    "early_stop = 0\n",
    "warm_up_epochs = 0\n",
    "base_lr = 0.05\n",
    "smooth = 0.1\n",
    "cosine_dec = False\n",
    "lr_find = False\n",
    "precise_eps = 75\n",
    "\n",
    "include_time = True\n",
    "test_eps = 200\n",
    "augment = False\n",
    "\n",
    "datasets = ['fashion_mnist', 'MB', 'MBI', 'MRB', 'MRD', 'MRDBI']\n",
    "datasets = ['fashion_mnist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EVOLVING IN DATASET fashion_mnist ...\n",
      "\n",
      "(48000, 28, 28, 1) train samples\n",
      "(12000, 28, 28, 1) validation samples\n",
      "(10000, 28, 28, 1) test samples\n",
      "Number of individuals eliminated by age: 0\n",
      "Genetic algorithm params\n",
      "Number of generations: 30\n",
      "Population size: 20\n",
      "Folder to save: ../../exp_finals_v2/fashion_mnist/genetic/0_2020-03-02-12:54/GA_experiment\n",
      "num parents: 5\n",
      "offspring size: 15\n",
      "\n",
      "Population size level one: 20\n",
      "Population size level two: 8\n",
      "Number of parents level one: 5\n",
      "Number of parents level two: 4\n",
      "Offspring size level one: 15\n",
      "Offspring size level two: 4\n",
      "Grow V2\n",
      "0\n",
      "30\n",
      "Creating Initial population\n",
      "\n",
      "Start evolution process...\n",
      "\n",
      "Initial population initialization...20\n",
      "\n",
      "0) Ranking level 1... Models to train: 20 ...OK (in 99.09 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "1) Ranking level 1... Models to train: 15 ...OK (in 89.05 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "1) Ranking level 2... Models to train: 8 ...OK (in 236.40 minutes)\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "2) Ranking level 1... Models to train: 15 ...OK (in 88.30 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "3) Ranking level 1... Models to train: 15 ...OK (in 94.98 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "3) Ranking level 2... Models to train: 4 ...OK (in 141.25 minutes)\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "Generation (3) in 749.05 minutes.\n",
      "Best first level fitness: 0.05710\n",
      "Best second level fitness: 0.05806\n",
      "\n",
      "4) Ranking level 1... Models to train: 15 ...OK (in 126.01 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "5) Ranking level 1... Models to train: 15 ...OK (in 115.78 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "6) Ranking level 1... Models to train: 15 ...OK (in 101.18 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "6) Ranking level 2... Models to train: 4 ...OK (in 136.65 minutes)\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "Generation (6) in 1228.67 minutes.\n",
      "Best first level fitness: 0.05710\n",
      "Best second level fitness: 0.05806\n",
      "\n",
      "7) Ranking level 1... Models to train: 15 ...OK (in 92.64 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "8) Ranking level 1... Models to train: 15 ...OK (in 102.21 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "9) Ranking level 1... Models to train: 15 ...OK (in 86.45 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "9) Ranking level 2... Models to train: 4 ...OK (in 137.71 minutes)\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "Generation (9) in 1647.68 minutes.\n",
      "Best first level fitness: 0.05637\n",
      "Best second level fitness: 0.05781\n",
      "\n",
      "10) Ranking level 1... Models to train: 15 ...OK (in 87.37 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "11) Ranking level 1... Models to train: 15 ...OK (in 97.47 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "12) Ranking level 1... Models to train: 15 ...OK (in 121.60 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "12) Ranking level 2... Models to train: 3 ...OK (in 74.31 minutes)\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "Generation (12) in 2028.44 minutes.\n",
      "Best first level fitness: 0.05637\n",
      "Best second level fitness: 0.05781\n",
      "\n",
      "13) Ranking level 1... Models to train: 15 ...OK (in 107.90 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "14) Ranking level 1... Models to train: 15 ...OK (in 102.09 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "15) Ranking level 1... Models to train: 15 ...OK (in 122.75 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "15) Ranking level 2... Models to train: 3 ...OK (in 124.06 minutes)\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "Generation (15) in 2485.23 minutes.\n",
      "Best first level fitness: 0.05637\n",
      "Best second level fitness: 0.05781\n",
      "\n",
      "16) Ranking level 1... Models to train: 15 ...OK (in 83.11 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "17) Ranking level 1... Models to train: 15 ...OK (in 70.08 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "18) Ranking level 1... Models to train: 15 ...OK (in 67.95 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.002\n",
      "\n",
      "18) Ranking level 2... Models to train: 2 ...OK (in 51.23 minutes)\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "Generation (18) in 2757.60 minutes.\n",
      "Best first level fitness: 0.05637\n",
      "Best second level fitness: 0.05593\n",
      "\n",
      "19) Ranking level 1... Models to train: 15 ...OK (in 74.46 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "20) Ranking level 1... Models to train: 15 ...OK (in 79.93 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "21) Ranking level 1... Models to train: 15 ..."
     ]
    }
   ],
   "source": [
    "fitness_cnn = FitnessGrow()    \n",
    "c = ChromosomeGrow.random_individual()   \n",
    "evolve_maxpool = False\n",
    "experiments_folder = '../../exp_finals_pool_v2' if evolve_maxpool else '../../exp_finals_v2'\n",
    "description = \"Grow V2 Maxpool and AvgPool\" if evolve_maxpool else \"Grow V2\"\n",
    "\n",
    "experiments_folder = experiments_folder\n",
    "os.makedirs(experiments_folder, exist_ok=True)\n",
    "for dataset in datasets:\n",
    "    if dataset == 'cifar10':\n",
    "        test_eps = 200\n",
    "        augment = 'cutout'\n",
    "    else:\n",
    "        test_eps = 100\n",
    "        augment = False\n",
    "    print(\"\\nEVOLVING IN DATASET %s ...\\n\" % dataset)\n",
    "    exp_folder = os.path.join(experiments_folder, dataset)\n",
    "    folder = os.path.join(exp_folder, 'genetic')\n",
    "    fitness_folder = exp_folder\n",
    "    fitness_file = os.path.join(fitness_folder, 'fitness_example')   \n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        generational = TwoLevelGA.load_genetic_algorithm(folder=folder)\n",
    "        # Load data\n",
    "        num_clases = 100 if dataset == 'cifar100' else 10\n",
    "        dm = DataManager(dataset, clases=classes, folder_var_mnist=data_folder, num_clases=num_clases) #, max_examples=8000)\n",
    "        data = dm.load_data()\n",
    "        fitness_cnn.set_params(data=data, verbose=verbose, batch_size=batch_size, reduce_plateau=redu_plat,\n",
    "                       epochs=epochs, cosine_decay=cosine_dec, early_stop=early_stop, \n",
    "                       warm_epochs=warm_up_epochs, base_lr=base_lr, smooth_label=smooth, find_lr=lr_find,\n",
    "                       precise_epochs=precise_eps, include_time=include_time, test_eps=test_eps, augment=augment)\n",
    "\n",
    "        fitness_cnn.save(fitness_file)\n",
    "    except:\n",
    "        # Load data\n",
    "        num_clases = 100 if dataset == 'cifar100' else 10\n",
    "        dm = DataManager(dataset, clases=classes, folder_var_mnist=data_folder, num_clases=num_clases) #, max_examples=8000)\n",
    "        data = dm.load_data()\n",
    "        fitness_cnn.set_params(data=data, verbose=verbose, batch_size=batch_size, reduce_plateau=redu_plat,\n",
    "                       epochs=epochs, cosine_decay=cosine_dec, early_stop=early_stop, \n",
    "                       warm_epochs=warm_up_epochs, base_lr=base_lr, smooth_label=smooth, find_lr=lr_find,\n",
    "                       precise_epochs=precise_eps, include_time=include_time, test_eps=test_eps,  augment=augment)\n",
    "\n",
    "        fitness_cnn.save(fitness_file)\n",
    "\n",
    "        del dm, data\n",
    "\n",
    "        fitness = FitnessCNNParallel()\n",
    "        fitness.set_params(chrom_files_folder=fitness_folder, fitness_file=fitness_file, max_gpus=gpus,\n",
    "                       fp=32, main_line=command)\n",
    "        generational = TwoLevelGA(chromosome=c,\n",
    "                                  fitness=fitness,\n",
    "                                  generations=generations,\n",
    "                                  population_first_level=population_first_level,\n",
    "                                  population_second_level=population_second_level,\n",
    "                                  training_hours=training_hours,\n",
    "                                  save_progress=save_progress,\n",
    "                                  maximize_fitness=maximize_fitness,\n",
    "                                  statistical_validation=statistical_validation,\n",
    "                                  folder=folder,\n",
    "                                  start_level2=start_level2,\n",
    "                                  frequency_second_level=frequency_second_level)\n",
    "        generational.print_genetic(description)\n",
    "\n",
    "\n",
    "    ti_all = time()\n",
    "    print(generational.generation)\n",
    "    print(generational.num_generations)\n",
    "    if generational.generation < generational.num_generations:\n",
    "        winner, best_fit, ranking = generational.evolve()\n",
    "    print(\"Total elapsed time: %0.3f\" % (time() - ti_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EVOLVING IN DATASET fashion_mnist ...\n",
      "\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 6us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 3s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 2s 0us/step\n",
      "(48000, 28, 28, 1) train samples\n",
      "(12000, 28, 28, 1) validation samples\n",
      "(10000, 28, 28, 1) test samples\n",
      "Number of individuals eliminated by age: 0\n",
      "Genetic algorithm params\n",
      "Number of generations: 30\n",
      "Population size: 20\n",
      "Folder to save: ../../exp_finals_pool/fashion_mnist/genetic/0_2020-02-06-17:48/GA_experiment\n",
      "num parents: 5\n",
      "offspring size: 15\n",
      "\n",
      "Population size level one: 20\n",
      "Population size level two: 8\n",
      "Number of parents level one: 5\n",
      "Number of parents level two: 4\n",
      "Offspring size level one: 15\n",
      "Offspring size level two: 4\n",
      "Grow V2 Maxpool and AvgPool\n",
      "0\n",
      "30\n",
      "Creating Initial population\n",
      "\n",
      "Start evolution process...\n",
      "\n",
      "Initial population initialization...20\n",
      "\n",
      "0) Ranking level 1... Models to train: 20 ...OK (in 5.04 minutes)\n",
      "Statistical validation in 0.00 minutes\n",
      "Saving... Elapsed saved time: 0.001\n",
      "\n",
      "1) Ranking level 1... Models to train: 15 ..."
     ]
    }
   ],
   "source": [
    "for evolve_maxpool in [True, False]:\n",
    "    if evolve_maxpool:\n",
    "        OperationBlock._operations = [CNNGrow, IdentityGrow, MaxPooling, AvPooling]\n",
    "    else:\n",
    "        OperationBlock._operations = [CNNGrow, IdentityGrow]\n",
    "        \n",
    "    fitness_cnn = FitnessGrow()    \n",
    "    c = ChromosomeGrow.random_individual()   \n",
    "    experiments_folder = '../../exp_finals_pool' if evolve_maxpool else '../../exp_finals'\n",
    "    description = \"Grow V2 Maxpool and AvgPool\" if evolve_maxpool else \"Grow V2\"\n",
    "    \n",
    "    experiments_folder = experiments_folder\n",
    "    os.makedirs(experiments_folder, exist_ok=True)\n",
    "    for dataset in datasets:\n",
    "        if dataset == 'cifar10':\n",
    "            test_eps = 200\n",
    "            augment = 'cutout'\n",
    "        print(\"\\nEVOLVING IN DATASET %s ...\\n\" % dataset)\n",
    "        exp_folder = os.path.join(experiments_folder, dataset)\n",
    "        folder = os.path.join(exp_folder, 'genetic')\n",
    "        fitness_folder = exp_folder\n",
    "        fitness_file = os.path.join(fitness_folder, 'fitness_example')   \n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            generational = TwoLevelGA.load_genetic_algorithm(folder=folder)\n",
    "            # Load data\n",
    "            num_clases = 100 if dataset == 'cifar100' else 10\n",
    "            dm = DataManager(dataset, clases=classes, folder_var_mnist=data_folder, num_clases=num_clases) #, max_examples=8000)\n",
    "            data = dm.load_data()\n",
    "            fitness_cnn.set_params(data=data, verbose=verbose, batch_size=batch_size, reduce_plateau=redu_plat,\n",
    "                           epochs=epochs, cosine_decay=cosine_dec, early_stop=early_stop, \n",
    "                           warm_epochs=warm_up_epochs, base_lr=base_lr, smooth_label=smooth, find_lr=lr_find,\n",
    "                           precise_epochs=precise_eps, include_time=include_time, test_eps=test_eps, augment=augment)\n",
    "\n",
    "            fitness_cnn.save(fitness_file)\n",
    "        except:\n",
    "            # Load data\n",
    "            num_clases = 100 if dataset == 'cifar100' else 10\n",
    "            dm = DataManager(dataset, clases=classes, folder_var_mnist=data_folder, num_clases=num_clases) #, max_examples=8000)\n",
    "            data = dm.load_data()\n",
    "            fitness_cnn.set_params(data=data, verbose=verbose, batch_size=batch_size, reduce_plateau=redu_plat,\n",
    "                           epochs=epochs, cosine_decay=cosine_dec, early_stop=early_stop, \n",
    "                           warm_epochs=warm_up_epochs, base_lr=base_lr, smooth_label=smooth, find_lr=lr_find,\n",
    "                           precise_epochs=precise_eps, include_time=include_time, test_eps=test_eps,  augment=augment)\n",
    "\n",
    "            fitness_cnn.save(fitness_file)\n",
    "\n",
    "            del dm, data\n",
    "\n",
    "            fitness = FitnessCNNParallel()\n",
    "            fitness.set_params(chrom_files_folder=fitness_folder, fitness_file=fitness_file, max_gpus=gpus,\n",
    "                           fp=32, main_line=command)\n",
    "            generational = TwoLevelGA(chromosome=c,\n",
    "                                      fitness=fitness,\n",
    "                                      generations=generations,\n",
    "                                      population_first_level=population_first_level,\n",
    "                                      population_second_level=population_second_level,\n",
    "                                      training_hours=training_hours,\n",
    "                                      save_progress=save_progress,\n",
    "                                      maximize_fitness=maximize_fitness,\n",
    "                                      statistical_validation=statistical_validation,\n",
    "                                      folder=folder,\n",
    "                                      start_level2=start_level2,\n",
    "                                      frequency_second_level=frequency_second_level)\n",
    "            generational.print_genetic(description)\n",
    "\n",
    "\n",
    "        ti_all = time()\n",
    "        print(generational.generation)\n",
    "        print(generational.num_generations)\n",
    "        if generational.generation < generational.num_generations:\n",
    "            winner, best_fit, ranking = generational.evolve()\n",
    "        print(\"Total elapsed time: %0.3f\" % (time() - ti_all))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9600, 28, 28, 1) train samples\n",
      "(2400, 28, 28, 1) validation samples\n",
      "(50000, 28, 28, 1) test samples\n",
      "Training with learning rate: 0.07855550518176016\n",
      "\n",
      "Epochs: 15\n",
      "Warmup epochs: 1\n",
      "Training... Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 28, 28, 32)   320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 28, 28, 32)   128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 28, 28, 32)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 28, 28, 8)    6408        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 28, 28, 8)    32          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 28, 28, 32)   128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 28, 28, 50)   450         batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 28, 28, 26)   858         batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 28, 28, 58)   0           conv2d_2[0][0]                   \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 28, 28, 58)   0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 28, 28, 58)   0           concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 28, 28, 58)   232         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 28, 28, 58)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 28, 28, 31)   16213       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 28, 28, 32)   128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 28, 28, 8)    32          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 28, 28, 31)   124         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 28, 28, 39)   1287        batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 28, 28, 63)   567         batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 28, 28, 40)   1280        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 28, 28, 71)   0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 28, 28, 71)   0           conv2d_2[0][0]                   \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 28, 28, 71)   0           conv2d_5[0][0]                   \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 28, 28, 71)   0           concatenate_3[0][0]              \n",
      "                                                                 concatenate_4[0][0]              \n",
      "                                                                 concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 28, 28, 71)   284         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 28, 28, 71)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 28, 28, 39)   69264       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 28, 28, 39)   156         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 28, 28, 39)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 28, 28, 19)   18544       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 28, 28, 19)   76          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 28, 28, 39)   156         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 28, 28, 111)  2220        batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 28, 28, 91)   3640        batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 28, 28, 130)  0           conv2d_10[0][0]                  \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 28, 28, 130)  0           conv2d_9[0][0]                   \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 28, 28, 130)  0           concatenate_6[0][0]              \n",
      "                                                                 concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 28, 28, 130)  520         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 28, 28, 130)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 28, 28, 70)   81970       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 28, 28, 39)   156         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 28, 28, 19)   76          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 28, 28, 70)   280         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 28, 28, 120)  4800        batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 28, 28, 140)  2800        batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 28, 28, 89)   6319        batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 28, 28, 159)  0           conv2d_9[0][0]                   \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 28, 28, 159)  0           conv2d_10[0][0]                  \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 28, 28, 159)  0           conv2d_13[0][0]                  \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 28, 28, 159)  0           concatenate_8[0][0]              \n",
      "                                                                 concatenate_9[0][0]              \n",
      "                                                                 concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 28, 28, 159)  636         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 28, 28, 159)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 28, 28, 87)   345912      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 14, 14, 87)   0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 14, 14, 87)   348         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 14, 14, 87)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 14, 14, 43)   93568       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 14, 14, 43)   172         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 14, 14, 87)   348         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 14, 14, 246)  10824       batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 14, 14, 202)  17776       batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 14, 14, 289)  0           conv2d_18[0][0]                  \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 14, 14, 289)  0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 14, 14, 289)  0           concatenate_11[0][0]             \n",
      "                                                                 concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 14, 14, 289)  1156        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 14, 14, 289)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 14, 14, 157)  408514      dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 14, 14, 87)   348         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 14, 14, 43)   172         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 14, 14, 157)  628         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 14, 14, 266)  23408       batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 14, 14, 310)  13640       batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 14, 14, 196)  30968       batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 14, 14, 353)  0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 14, 14, 353)  0           conv2d_18[0][0]                  \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 14, 14, 353)  0           conv2d_21[0][0]                  \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 14, 14, 353)  0           concatenate_13[0][0]             \n",
      "                                                                 concatenate_14[0][0]             \n",
      "                                                                 concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 14, 14, 353)  1412        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 14, 14, 353)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 14, 14, 194)  1712244     dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 14, 14, 194)  776         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 14, 14, 194)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 14, 14, 96)   465696      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 14, 14, 96)   384         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 14, 14, 194)  776         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 14, 14, 548)  53156       batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 14, 14, 450)  87750       batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 14, 14, 644)  0           conv2d_26[0][0]                  \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 14, 14, 644)  0           conv2d_25[0][0]                  \n",
      "                                                                 conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 14, 14, 644)  0           concatenate_16[0][0]             \n",
      "                                                                 concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 14, 14, 644)  2576        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 14, 14, 644)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 14, 14, 349)  2023153     dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 14, 14, 194)  776         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 14, 14, 96)   384         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 14, 14, 349)  1396        conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 14, 14, 593)  115635      batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 14, 14, 691)  67027       batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 14, 14, 438)  153300      batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 14, 14, 787)  0           conv2d_25[0][0]                  \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 14, 14, 787)  0           conv2d_26[0][0]                  \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 14, 14, 787)  0           conv2d_29[0][0]                  \n",
      "                                                                 conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 14, 14, 787)  0           concatenate_18[0][0]             \n",
      "                                                                 concatenate_19[0][0]             \n",
      "                                                                 concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 14, 14, 787)  3148        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 14, 14, 787)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 14, 14, 432)  8500032     dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 432)          0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           4330        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 14,361,817\n",
      "Trainable params: 14,352,845\n",
      "Non-trainable params: 8,972\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9600 samples, validate on 2400 samples\n",
      "Epoch 1/15\n",
      "\n",
      "Batch 00001: setting learning rate to 2e-05.\n",
      " 128/9600 [..............................] - ETA: 15:35 - loss: 2.4506 - accuracy: 0.1328\n",
      "Batch 00002: setting learning rate to 0.0010671400690901354.\n",
      " 256/9600 [..............................] - ETA: 7:50 - loss: 2.4812 - accuracy: 0.1250 \n",
      "Batch 00003: setting learning rate to 0.0021142801381802707.\n",
      " 384/9600 [>.............................] - ETA: 5:14 - loss: 3.2472 - accuracy: 0.1224\n",
      "Batch 00004: setting learning rate to 0.003161420207270406.\n",
      " 512/9600 [>.............................] - ETA: 3:56 - loss: 3.8623 - accuracy: 0.0996\n",
      "Batch 00005: setting learning rate to 0.004208560276360541.\n",
      " 640/9600 [=>............................] - ETA: 3:10 - loss: 6.4704 - accuracy: 0.0922\n",
      "Batch 00006: setting learning rate to 0.005255700345450677.\n",
      " 768/9600 [=>............................] - ETA: 2:38 - loss: 10.6788 - accuracy: 0.0859\n",
      "Batch 00007: setting learning rate to 0.006302840414540812.\n",
      " 896/9600 [=>............................] - ETA: 2:16 - loss: 13.0253 - accuracy: 0.0938\n",
      "Batch 00008: setting learning rate to 0.0073499804836309485.\n",
      "1024/9600 [==>...........................] - ETA: 1:59 - loss: 16.8907 - accuracy: 0.0928\n",
      "Batch 00009: setting learning rate to 0.008397120552721082.\n",
      "1152/9600 [==>...........................] - ETA: 1:46 - loss: 21.6798 - accuracy: 0.1059\n",
      "Batch 00010: setting learning rate to 0.009444260621811217.\n",
      "1280/9600 [===>..........................] - ETA: 1:35 - loss: 23.2855 - accuracy: 0.1094\n",
      "Batch 00011: setting learning rate to 0.010491400690901353.\n",
      "1408/9600 [===>..........................] - ETA: 1:27 - loss: 24.9217 - accuracy: 0.1186\n",
      "Batch 00012: setting learning rate to 0.011538540759991488.\n",
      "1536/9600 [===>..........................] - ETA: 1:19 - loss: 27.3302 - accuracy: 0.1335\n",
      "Batch 00013: setting learning rate to 0.012585680829081623.\n",
      "1664/9600 [====>.........................] - ETA: 1:13 - loss: 30.0255 - accuracy: 0.1394\n",
      "Batch 00014: setting learning rate to 0.01363282089817176.\n",
      "1792/9600 [====>.........................] - ETA: 1:08 - loss: 32.3622 - accuracy: 0.1501\n",
      "Batch 00015: setting learning rate to 0.014679960967261896.\n",
      "1920/9600 [=====>........................] - ETA: 1:03 - loss: 35.1688 - accuracy: 0.1536\n",
      "Batch 00016: setting learning rate to 0.01572710103635203.\n",
      "2048/9600 [=====>........................] - ETA: 59s - loss: 38.3233 - accuracy: 0.1587 \n",
      "Batch 00017: setting learning rate to 0.016774241105442164.\n",
      "2176/9600 [=====>........................] - ETA: 55s - loss: 40.6841 - accuracy: 0.1618\n",
      "Batch 00018: setting learning rate to 0.017821381174532303.\n",
      "2304/9600 [======>.......................] - ETA: 52s - loss: 41.9173 - accuracy: 0.1697\n",
      "Batch 00019: setting learning rate to 0.018868521243622434.\n",
      "2432/9600 [======>.......................] - ETA: 49s - loss: 43.3692 - accuracy: 0.1719\n",
      "Batch 00020: setting learning rate to 0.019915661312712572.\n",
      "2560/9600 [=======>......................] - ETA: 47s - loss: 45.3562 - accuracy: 0.1730\n",
      "Batch 00021: setting learning rate to 0.020962801381802707.\n",
      "2688/9600 [=======>......................] - ETA: 44s - loss: 47.5646 - accuracy: 0.1756\n",
      "Batch 00022: setting learning rate to 0.022009941450892842.\n",
      "2816/9600 [=======>......................] - ETA: 42s - loss: 47.9102 - accuracy: 0.1818\n",
      "Batch 00023: setting learning rate to 0.023057081519982977.\n",
      "2944/9600 [========>.....................] - ETA: 40s - loss: 47.6492 - accuracy: 0.1895\n",
      "Batch 00024: setting learning rate to 0.024104221589073115.\n",
      "3072/9600 [========>.....................] - ETA: 38s - loss: 47.8895 - accuracy: 0.1930\n",
      "Batch 00025: setting learning rate to 0.025151361658163247.\n",
      "3200/9600 [=========>....................] - ETA: 36s - loss: 47.6438 - accuracy: 0.1953\n",
      "Batch 00026: setting learning rate to 0.026198501727253385.\n",
      "3328/9600 [=========>....................] - ETA: 34s - loss: 47.5446 - accuracy: 0.1950\n",
      "Batch 00027: setting learning rate to 0.02724564179634352.\n",
      "3456/9600 [=========>....................] - ETA: 33s - loss: 47.2764 - accuracy: 0.1947\n",
      "Batch 00028: setting learning rate to 0.02829278186543365.\n",
      "3584/9600 [==========>...................] - ETA: 31s - loss: 47.2231 - accuracy: 0.1956\n",
      "Batch 00029: setting learning rate to 0.029339921934523793.\n",
      "3712/9600 [==========>...................] - ETA: 30s - loss: 47.0684 - accuracy: 0.1994\n",
      "Batch 00030: setting learning rate to 0.030387062003613925.\n",
      "3840/9600 [===========>..................] - ETA: 29s - loss: 47.1159 - accuracy: 0.2008\n",
      "Batch 00031: setting learning rate to 0.03143420207270406.\n",
      "3968/9600 [===========>..................] - ETA: 28s - loss: 47.1392 - accuracy: 0.2016\n",
      "Batch 00032: setting learning rate to 0.0324813421417942.\n",
      "4096/9600 [===========>..................] - ETA: 26s - loss: 47.0333 - accuracy: 0.2029\n",
      "Batch 00033: setting learning rate to 0.03352848221088433.\n",
      "4224/9600 [============>.................] - ETA: 25s - loss: 46.4626 - accuracy: 0.2069\n",
      "Batch 00034: setting learning rate to 0.03457562227997447.\n",
      "4352/9600 [============>.................] - ETA: 24s - loss: 45.6658 - accuracy: 0.2105\n",
      "Batch 00035: setting learning rate to 0.035622762349064606.\n",
      "4480/9600 [=============>................] - ETA: 23s - loss: 44.7842 - accuracy: 0.2129\n",
      "Batch 00036: setting learning rate to 0.03666990241815474.\n",
      "4608/9600 [=============>................] - ETA: 22s - loss: 43.7235 - accuracy: 0.2196\n",
      "Batch 00037: setting learning rate to 0.03771704248724487.\n",
      "4736/9600 [=============>................] - ETA: 21s - loss: 42.6646 - accuracy: 0.2242\n",
      "Batch 00038: setting learning rate to 0.03876418255633501.\n",
      "4864/9600 [==============>...............] - ETA: 20s - loss: 41.6417 - accuracy: 0.2284\n",
      "Batch 00039: setting learning rate to 0.039811322625425145.\n",
      "4992/9600 [==============>...............] - ETA: 19s - loss: 40.6587 - accuracy: 0.2318\n",
      "Batch 00040: setting learning rate to 0.04085846269451528.\n",
      "5120/9600 [===============>..............] - ETA: 19s - loss: 39.6972 - accuracy: 0.2377\n",
      "Batch 00041: setting learning rate to 0.041905602763605415.\n",
      "5248/9600 [===============>..............] - ETA: 18s - loss: 38.7741 - accuracy: 0.2439\n",
      "Batch 00042: setting learning rate to 0.04295274283269555.\n",
      "5376/9600 [===============>..............] - ETA: 17s - loss: 37.8962 - accuracy: 0.2480\n",
      "Batch 00043: setting learning rate to 0.043999882901785685.\n",
      "5504/9600 [================>.............] - ETA: 16s - loss: 37.0615 - accuracy: 0.2498\n",
      "Batch 00044: setting learning rate to 0.04504702297087582.\n",
      "5632/9600 [================>.............] - ETA: 16s - loss: 36.2598 - accuracy: 0.2537\n",
      "Batch 00045: setting learning rate to 0.046094163039965955.\n",
      "5760/9600 [=================>............] - ETA: 15s - loss: 35.4964 - accuracy: 0.2556\n",
      "Batch 00046: setting learning rate to 0.047141303109056086.\n",
      "5888/9600 [=================>............] - ETA: 14s - loss: 34.7673 - accuracy: 0.2590\n",
      "Batch 00047: setting learning rate to 0.04818844317814623.\n",
      "6016/9600 [=================>............] - ETA: 14s - loss: 34.0691 - accuracy: 0.2625\n",
      "Batch 00048: setting learning rate to 0.04923558324723636.\n",
      "6144/9600 [==================>...........] - ETA: 13s - loss: 33.3993 - accuracy: 0.2669\n",
      "Batch 00049: setting learning rate to 0.050282723316326494.\n",
      "6272/9600 [==================>...........] - ETA: 12s - loss: 32.7551 - accuracy: 0.2717\n",
      "Batch 00050: setting learning rate to 0.05132986338541663.\n",
      "6400/9600 [===================>..........] - ETA: 12s - loss: 32.1356 - accuracy: 0.2761\n",
      "Batch 00051: setting learning rate to 0.05237700345450677.\n",
      "6528/9600 [===================>..........] - ETA: 11s - loss: 31.5422 - accuracy: 0.2785\n",
      "Batch 00052: setting learning rate to 0.05342414352359691.\n",
      "6656/9600 [===================>..........] - ETA: 10s - loss: 30.9710 - accuracy: 0.2800\n",
      "Batch 00053: setting learning rate to 0.05447128359268704.\n",
      "6784/9600 [====================>.........] - ETA: 10s - loss: 30.4227 - accuracy: 0.2817\n",
      "Batch 00054: setting learning rate to 0.05551842366177717.\n",
      "6912/9600 [====================>.........] - ETA: 9s - loss: 29.8906 - accuracy: 0.2867 \n",
      "Batch 00055: setting learning rate to 0.0565655637308673.\n",
      "7040/9600 [=====================>........] - ETA: 9s - loss: 29.3757 - accuracy: 0.2912\n",
      "Batch 00056: setting learning rate to 0.05761270379995744.\n",
      "7168/9600 [=====================>........] - ETA: 8s - loss: 28.8802 - accuracy: 0.2955\n",
      "Batch 00057: setting learning rate to 0.05865984386904759.\n",
      "7296/9600 [=====================>........] - ETA: 8s - loss: 28.4003 - accuracy: 0.3007\n",
      "Batch 00058: setting learning rate to 0.05970698393813772.\n",
      "7424/9600 [======================>.......] - ETA: 7s - loss: 27.9373 - accuracy: 0.3048\n",
      "Batch 00059: setting learning rate to 0.06075412400722785.\n",
      "7552/9600 [======================>.......] - ETA: 7s - loss: 27.4897 - accuracy: 0.3089\n",
      "Batch 00060: setting learning rate to 0.06180126407631799.\n",
      "7680/9600 [=======================>......] - ETA: 6s - loss: 27.0582 - accuracy: 0.3121\n",
      "Batch 00061: setting learning rate to 0.06284840414540813.\n",
      "7808/9600 [=======================>......] - ETA: 6s - loss: 26.6380 - accuracy: 0.3172\n",
      "Batch 00062: setting learning rate to 0.06389554421449826.\n",
      "7936/9600 [=======================>......] - ETA: 5s - loss: 26.2304 - accuracy: 0.3237\n",
      "Batch 00063: setting learning rate to 0.0649426842835884.\n",
      "8064/9600 [========================>.....] - ETA: 5s - loss: 25.8351 - accuracy: 0.3292\n",
      "Batch 00064: setting learning rate to 0.06598982435267854.\n",
      "8192/9600 [========================>.....] - ETA: 4s - loss: 25.4539 - accuracy: 0.3345\n",
      "Batch 00065: setting learning rate to 0.06703696442176867.\n",
      "8320/9600 [=========================>....] - ETA: 4s - loss: 25.0834 - accuracy: 0.3395\n",
      "Batch 00066: setting learning rate to 0.0680841044908588.\n",
      "8448/9600 [=========================>....] - ETA: 3s - loss: 24.7245 - accuracy: 0.3452\n",
      "Batch 00067: setting learning rate to 0.06913124455994894.\n",
      "8576/9600 [=========================>....] - ETA: 3s - loss: 24.3737 - accuracy: 0.3509\n",
      "Batch 00068: setting learning rate to 0.07017838462903908.\n",
      "8704/9600 [==========================>...] - ETA: 2s - loss: 24.0343 - accuracy: 0.3558\n",
      "Batch 00069: setting learning rate to 0.07122552469812922.\n",
      "8832/9600 [==========================>...] - ETA: 2s - loss: 23.7028 - accuracy: 0.3611\n",
      "Batch 00070: setting learning rate to 0.07227266476721934.\n",
      "8960/9600 [===========================>..] - ETA: 2s - loss: 23.3840 - accuracy: 0.3651\n",
      "Batch 00071: setting learning rate to 0.07331980483630948.\n",
      "9088/9600 [===========================>..] - ETA: 1s - loss: 23.0739 - accuracy: 0.3684\n",
      "Batch 00072: setting learning rate to 0.07436694490539962.\n",
      "9216/9600 [===========================>..] - ETA: 1s - loss: 22.7703 - accuracy: 0.3738\n",
      "Batch 00073: setting learning rate to 0.07541408497448975.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 22.4742 - accuracy: 0.3792\n",
      "Batch 00074: setting learning rate to 0.0764612250435799.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 22.1870 - accuracy: 0.3842\n",
      "Batch 00075: setting learning rate to 0.07750836511267002.\n",
      "9600/9600 [==============================] - 33s 3ms/step - loss: 21.9075 - accuracy: 0.3884 - val_loss: 3714.7026 - val_accuracy: 0.1096\n",
      "Epoch 2/15\n",
      "\n",
      "Batch 00076: setting learning rate to 0.07855550518176017.\n",
      " 128/9600 [..............................] - ETA: 17s - loss: 1.2556 - accuracy: 0.7031\n",
      "Batch 00077: setting learning rate to 0.07848070946253945.\n",
      " 256/9600 [..............................] - ETA: 17s - loss: 1.2045 - accuracy: 0.7227\n",
      "Batch 00078: setting learning rate to 0.07840591374331872.\n",
      " 384/9600 [>.............................] - ETA: 17s - loss: 1.1984 - accuracy: 0.7318\n",
      "Batch 00079: setting learning rate to 0.07833111802409799.\n",
      " 512/9600 [>.............................] - ETA: 17s - loss: 1.1893 - accuracy: 0.7285\n",
      "Batch 00080: setting learning rate to 0.07825632230487727.\n",
      " 640/9600 [=>............................] - ETA: 17s - loss: 1.1812 - accuracy: 0.7375\n",
      "Batch 00081: setting learning rate to 0.07818152658565654.\n",
      " 768/9600 [=>............................] - ETA: 17s - loss: 1.1753 - accuracy: 0.7422\n",
      "Batch 00082: setting learning rate to 0.07810673086643581.\n",
      " 896/9600 [=>............................] - ETA: 16s - loss: 1.1727 - accuracy: 0.7422\n",
      "Batch 00083: setting learning rate to 0.07803193514721508.\n",
      "1024/9600 [==>...........................] - ETA: 16s - loss: 1.1785 - accuracy: 0.7344\n",
      "Batch 00084: setting learning rate to 0.07795713942799437.\n",
      "1152/9600 [==>...........................] - ETA: 16s - loss: 1.1844 - accuracy: 0.7292\n",
      "Batch 00085: setting learning rate to 0.07788234370877366.\n",
      "1280/9600 [===>..........................] - ETA: 16s - loss: 1.1776 - accuracy: 0.7367\n",
      "Batch 00086: setting learning rate to 0.07780754798955293.\n",
      "1408/9600 [===>..........................] - ETA: 15s - loss: 1.1629 - accuracy: 0.7443\n",
      "Batch 00087: setting learning rate to 0.0777327522703322.\n",
      "1536/9600 [===>..........................] - ETA: 15s - loss: 1.1630 - accuracy: 0.7428\n",
      "Batch 00088: setting learning rate to 0.07765795655111148.\n",
      "1664/9600 [====>.........................] - ETA: 15s - loss: 1.1629 - accuracy: 0.7428\n",
      "Batch 00089: setting learning rate to 0.07758316083189075.\n",
      "1792/9600 [====>.........................] - ETA: 15s - loss: 1.1674 - accuracy: 0.7405\n",
      "Batch 00090: setting learning rate to 0.07750836511267002.\n",
      "1920/9600 [=====>........................] - ETA: 14s - loss: 1.1666 - accuracy: 0.7391\n",
      "Batch 00091: setting learning rate to 0.0774335693934493.\n",
      "2048/9600 [=====>........................] - ETA: 14s - loss: 1.1604 - accuracy: 0.7422\n",
      "Batch 00092: setting learning rate to 0.07735877367422857.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 1.1635 - accuracy: 0.7436\n",
      "Batch 00093: setting learning rate to 0.07728397795500785.\n",
      "2304/9600 [======>.......................] - ETA: 14s - loss: 1.1587 - accuracy: 0.7426\n",
      "Batch 00094: setting learning rate to 0.07720918223578714.\n",
      "2432/9600 [======>.......................] - ETA: 13s - loss: 1.1563 - accuracy: 0.7405\n",
      "Batch 00095: setting learning rate to 0.07713438651656641.\n",
      "2560/9600 [=======>......................] - ETA: 13s - loss: 1.1560 - accuracy: 0.7414\n",
      "Batch 00096: setting learning rate to 0.07705959079734569.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 1.1493 - accuracy: 0.7429\n",
      "Batch 00097: setting learning rate to 0.07698479507812496.\n",
      "2816/9600 [=======>......................] - ETA: 13s - loss: 1.1450 - accuracy: 0.7461\n",
      "Batch 00098: setting learning rate to 0.07690999935890423.\n",
      "2944/9600 [========>.....................] - ETA: 12s - loss: 1.1353 - accuracy: 0.7524\n",
      "Batch 00099: setting learning rate to 0.0768352036396835.\n",
      "3072/9600 [========>.....................] - ETA: 12s - loss: 1.1291 - accuracy: 0.7555\n",
      "Batch 00100: setting learning rate to 0.07676040792046278.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 1.1280 - accuracy: 0.7528\n",
      "Batch 00101: setting learning rate to 0.07668561220124205.\n",
      "3328/9600 [=========>....................] - ETA: 12s - loss: 1.1240 - accuracy: 0.7542\n",
      "Batch 00102: setting learning rate to 0.07661081648202135.\n",
      "3456/9600 [=========>....................] - ETA: 11s - loss: 1.1220 - accuracy: 0.7526\n",
      "Batch 00103: setting learning rate to 0.07653602076280062.\n",
      "3584/9600 [==========>...................] - ETA: 11s - loss: 1.1185 - accuracy: 0.7550\n",
      "Batch 00104: setting learning rate to 0.0764612250435799.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 1.1204 - accuracy: 0.7557\n",
      "Batch 00105: setting learning rate to 0.07638642932435917.\n",
      "3840/9600 [===========>..................] - ETA: 11s - loss: 1.1145 - accuracy: 0.7591\n",
      "Batch 00106: setting learning rate to 0.07631163360513844.\n",
      "3968/9600 [===========>..................] - ETA: 10s - loss: 1.1100 - accuracy: 0.7616\n",
      "Batch 00107: setting learning rate to 0.07623683788591772.\n",
      "4096/9600 [===========>..................] - ETA: 10s - loss: 1.1081 - accuracy: 0.7634\n",
      "Batch 00108: setting learning rate to 0.07616204216669699.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 1.1031 - accuracy: 0.7661\n",
      "Batch 00109: setting learning rate to 0.07608724644747626.\n",
      "4352/9600 [============>.................] - ETA: 10s - loss: 1.1008 - accuracy: 0.7684\n",
      "Batch 00110: setting learning rate to 0.07601245072825555.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4480/9600 [=============>................] - ETA: 9s - loss: 1.1013 - accuracy: 0.7683 \n",
      "Batch 00111: setting learning rate to 0.07593765500903484.\n",
      "4608/9600 [=============>................] - ETA: 9s - loss: 1.1002 - accuracy: 0.7689\n",
      "Batch 00112: setting learning rate to 0.07586285928981411.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 1.0981 - accuracy: 0.7690\n",
      "Batch 00113: setting learning rate to 0.07578806357059338.\n",
      "4864/9600 [==============>...............] - ETA: 9s - loss: 1.0959 - accuracy: 0.7699\n",
      "Batch 00114: setting learning rate to 0.07571326785137265.\n",
      "4992/9600 [==============>...............] - ETA: 8s - loss: 1.0929 - accuracy: 0.7716\n",
      "Batch 00115: setting learning rate to 0.07563847213215193.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 1.0895 - accuracy: 0.7736\n",
      "Batch 00116: setting learning rate to 0.0755636764129312.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 1.0862 - accuracy: 0.7761\n",
      "Batch 00117: setting learning rate to 0.07548888069371047.\n",
      "5376/9600 [===============>..............] - ETA: 8s - loss: 1.0825 - accuracy: 0.7775\n",
      "Batch 00118: setting learning rate to 0.07541408497448975.\n",
      "5504/9600 [================>.............] - ETA: 7s - loss: 1.0809 - accuracy: 0.7783\n",
      "Batch 00119: setting learning rate to 0.07533928925526905.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 1.0771 - accuracy: 0.7800\n",
      "Batch 00120: setting learning rate to 0.07526449353604832.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 1.0760 - accuracy: 0.7806\n",
      "Batch 00121: setting learning rate to 0.07518969781682759.\n",
      "5888/9600 [=================>............] - ETA: 7s - loss: 1.0736 - accuracy: 0.7824\n",
      "Batch 00122: setting learning rate to 0.07511490209760686.\n",
      "6016/9600 [=================>............] - ETA: 6s - loss: 1.0693 - accuracy: 0.7847\n",
      "Batch 00123: setting learning rate to 0.07504010637838614.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 1.0659 - accuracy: 0.7868\n",
      "Batch 00124: setting learning rate to 0.07496531065916541.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 1.0646 - accuracy: 0.7878\n",
      "Batch 00125: setting learning rate to 0.07489051493994468.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 1.0610 - accuracy: 0.7889\n",
      "Batch 00126: setting learning rate to 0.07481571922072396.\n",
      "6528/9600 [===================>..........] - ETA: 5s - loss: 1.0569 - accuracy: 0.7906\n",
      "Batch 00127: setting learning rate to 0.07474092350150323.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 1.0540 - accuracy: 0.7924\n",
      "Batch 00128: setting learning rate to 0.07466612778228253.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 1.0512 - accuracy: 0.7935\n",
      "Batch 00129: setting learning rate to 0.0745913320630618.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 1.0496 - accuracy: 0.7946\n",
      "Batch 00130: setting learning rate to 0.07451653634384107.\n",
      "7040/9600 [=====================>........] - ETA: 4s - loss: 1.0475 - accuracy: 0.7957\n",
      "Batch 00131: setting learning rate to 0.07444174062462035.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 1.0466 - accuracy: 0.7960\n",
      "Batch 00132: setting learning rate to 0.07436694490539962.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 1.0439 - accuracy: 0.7971\n",
      "Batch 00133: setting learning rate to 0.0742921491861789.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 1.0424 - accuracy: 0.7982\n",
      "Batch 00134: setting learning rate to 0.07421735346695817.\n",
      "7552/9600 [======================>.......] - ETA: 3s - loss: 1.0406 - accuracy: 0.7994\n",
      "Batch 00135: setting learning rate to 0.07414255774773744.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 1.0382 - accuracy: 0.8005\n",
      "Batch 00136: setting learning rate to 0.07406776202851671.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 1.0360 - accuracy: 0.8014\n",
      "Batch 00137: setting learning rate to 0.07399296630929601.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 1.0357 - accuracy: 0.8015\n",
      "Batch 00138: setting learning rate to 0.07391817059007529.\n",
      "8064/9600 [========================>.....] - ETA: 2s - loss: 1.0335 - accuracy: 0.8030\n",
      "Batch 00139: setting learning rate to 0.07384337487085456.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 1.0308 - accuracy: 0.8042\n",
      "Batch 00140: setting learning rate to 0.07376857915163383.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 1.0296 - accuracy: 0.8044\n",
      "Batch 00141: setting learning rate to 0.0736937834324131.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 1.0270 - accuracy: 0.8054\n",
      "Batch 00142: setting learning rate to 0.07361898771319238.\n",
      "8576/9600 [=========================>....] - ETA: 1s - loss: 1.0258 - accuracy: 0.8060\n",
      "Batch 00143: setting learning rate to 0.07354419199397165.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 1.0228 - accuracy: 0.8077\n",
      "Batch 00144: setting learning rate to 0.07346939627475092.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 1.0202 - accuracy: 0.8088\n",
      "Batch 00145: setting learning rate to 0.07339460055553021.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 1.0186 - accuracy: 0.8094\n",
      "Batch 00146: setting learning rate to 0.0733198048363095.\n",
      "9088/9600 [===========================>..] - ETA: 0s - loss: 1.0169 - accuracy: 0.8100\n",
      "Batch 00147: setting learning rate to 0.07324500911708877.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 1.0155 - accuracy: 0.8112\n",
      "Batch 00148: setting learning rate to 0.07317021339786804.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 1.0127 - accuracy: 0.8126\n",
      "Batch 00149: setting learning rate to 0.07309541767864731.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 1.0113 - accuracy: 0.8133\n",
      "Batch 00150: setting learning rate to 0.07302062195942659.\n",
      "9600/9600 [==============================] - 20s 2ms/step - loss: 1.0105 - accuracy: 0.8140 - val_loss: 19.9549 - val_accuracy: 0.2512\n",
      "Epoch 3/15\n",
      "\n",
      "Batch 00151: setting learning rate to 0.07294582624020586.\n",
      " 128/9600 [..............................] - ETA: 18s - loss: 0.8663 - accuracy: 0.9219\n",
      "Batch 00152: setting learning rate to 0.07287103052098513.\n",
      " 256/9600 [..............................] - ETA: 17s - loss: 0.8997 - accuracy: 0.9023\n",
      "Batch 00153: setting learning rate to 0.0727962348017644.\n",
      " 384/9600 [>.............................] - ETA: 17s - loss: 0.8858 - accuracy: 0.9036\n",
      "Batch 00154: setting learning rate to 0.07272143908254369.\n",
      " 512/9600 [>.............................] - ETA: 16s - loss: 0.8641 - accuracy: 0.9043\n",
      "Batch 00155: setting learning rate to 0.07264664336332298.\n",
      " 640/9600 [=>............................] - ETA: 16s - loss: 0.8641 - accuracy: 0.8953\n",
      "Batch 00156: setting learning rate to 0.07257184764410225.\n",
      " 768/9600 [=>............................] - ETA: 16s - loss: 0.8629 - accuracy: 0.8906\n",
      "Batch 00157: setting learning rate to 0.07249705192488153.\n",
      " 896/9600 [=>............................] - ETA: 15s - loss: 0.8638 - accuracy: 0.8940\n",
      "Batch 00158: setting learning rate to 0.0724222562056608.\n",
      "1024/9600 [==>...........................] - ETA: 15s - loss: 0.8559 - accuracy: 0.8955\n",
      "Batch 00159: setting learning rate to 0.07234746048644007.\n",
      "1152/9600 [==>...........................] - ETA: 15s - loss: 0.8576 - accuracy: 0.8967\n",
      "Batch 00160: setting learning rate to 0.07227266476721934.\n",
      "1280/9600 [===>..........................] - ETA: 15s - loss: 0.8587 - accuracy: 0.8977\n",
      "Batch 00161: setting learning rate to 0.07219786904799862.\n",
      "1408/9600 [===>..........................] - ETA: 14s - loss: 0.8676 - accuracy: 0.8942\n",
      "Batch 00162: setting learning rate to 0.07212307332877789.\n",
      "1536/9600 [===>..........................] - ETA: 14s - loss: 0.8677 - accuracy: 0.8913\n",
      "Batch 00163: setting learning rate to 0.07204827760955718.\n",
      "1664/9600 [====>.........................] - ETA: 14s - loss: 0.8649 - accuracy: 0.8894\n",
      "Batch 00164: setting learning rate to 0.07197348189033646.\n",
      "1792/9600 [====>.........................] - ETA: 14s - loss: 0.8622 - accuracy: 0.8917\n",
      "Batch 00165: setting learning rate to 0.07189868617111574.\n",
      "1920/9600 [=====>........................] - ETA: 14s - loss: 0.8631 - accuracy: 0.8911\n",
      "Batch 00166: setting learning rate to 0.07182389045189501.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048/9600 [=====>........................] - ETA: 13s - loss: 0.8668 - accuracy: 0.8906\n",
      "Batch 00167: setting learning rate to 0.07174909473267428.\n",
      "2176/9600 [=====>........................] - ETA: 13s - loss: 0.8638 - accuracy: 0.8920\n",
      "Batch 00168: setting learning rate to 0.07167429901345355.\n",
      "2304/9600 [======>.......................] - ETA: 13s - loss: 0.8646 - accuracy: 0.8915\n",
      "Batch 00169: setting learning rate to 0.07159950329423283.\n",
      "2432/9600 [======>.......................] - ETA: 13s - loss: 0.8669 - accuracy: 0.8882\n",
      "Batch 00170: setting learning rate to 0.0715247075750121.\n",
      "2560/9600 [=======>......................] - ETA: 13s - loss: 0.8634 - accuracy: 0.8891\n",
      "Batch 00171: setting learning rate to 0.07144991185579139.\n",
      "2688/9600 [=======>......................] - ETA: 12s - loss: 0.8621 - accuracy: 0.8906\n",
      "Batch 00172: setting learning rate to 0.07137511613657066.\n",
      "2816/9600 [=======>......................] - ETA: 12s - loss: 0.8645 - accuracy: 0.8896\n",
      "Batch 00173: setting learning rate to 0.07130032041734995.\n",
      "2944/9600 [========>.....................] - ETA: 12s - loss: 0.8616 - accuracy: 0.8913\n",
      "Batch 00174: setting learning rate to 0.07122552469812922.\n",
      "3072/9600 [========>.....................] - ETA: 12s - loss: 0.8612 - accuracy: 0.8910\n",
      "Batch 00175: setting learning rate to 0.07115072897890849.\n",
      "3200/9600 [=========>....................] - ETA: 11s - loss: 0.8617 - accuracy: 0.8909\n",
      "Batch 00176: setting learning rate to 0.07107593325968777.\n",
      "3328/9600 [=========>....................] - ETA: 11s - loss: 0.8606 - accuracy: 0.8909\n",
      "Batch 00177: setting learning rate to 0.07100113754046704.\n",
      "3456/9600 [=========>....................] - ETA: 11s - loss: 0.8588 - accuracy: 0.8909\n",
      "Batch 00178: setting learning rate to 0.07092634182124631.\n",
      "3584/9600 [==========>...................] - ETA: 11s - loss: 0.8586 - accuracy: 0.8895\n",
      "Batch 00179: setting learning rate to 0.07085154610202558.\n",
      "3712/9600 [==========>...................] - ETA: 10s - loss: 0.8582 - accuracy: 0.8895\n",
      "Batch 00180: setting learning rate to 0.07077675038280488.\n",
      "3840/9600 [===========>..................] - ETA: 10s - loss: 0.8618 - accuracy: 0.8878\n",
      "Batch 00181: setting learning rate to 0.07070195466358416.\n",
      "3968/9600 [===========>..................] - ETA: 10s - loss: 0.8602 - accuracy: 0.8889\n",
      "Batch 00182: setting learning rate to 0.07062715894436343.\n",
      "4096/9600 [===========>..................] - ETA: 10s - loss: 0.8580 - accuracy: 0.8896\n",
      "Batch 00183: setting learning rate to 0.0705523632251427.\n",
      "4224/9600 [============>.................] - ETA: 9s - loss: 0.8587 - accuracy: 0.8885 \n",
      "Batch 00184: setting learning rate to 0.07047756750592198.\n",
      "4352/9600 [============>.................] - ETA: 9s - loss: 0.8573 - accuracy: 0.8886\n",
      "Batch 00185: setting learning rate to 0.07040277178670125.\n",
      "4480/9600 [=============>................] - ETA: 9s - loss: 0.8577 - accuracy: 0.8877\n",
      "Batch 00186: setting learning rate to 0.07032797606748052.\n",
      "4608/9600 [=============>................] - ETA: 9s - loss: 0.8551 - accuracy: 0.8882\n",
      "Batch 00187: setting learning rate to 0.0702531803482598.\n",
      "4736/9600 [=============>................] - ETA: 8s - loss: 0.8547 - accuracy: 0.8889\n",
      "Batch 00188: setting learning rate to 0.07017838462903907.\n",
      "4864/9600 [==============>...............] - ETA: 8s - loss: 0.8531 - accuracy: 0.8894\n",
      "Batch 00189: setting learning rate to 0.07010358890981837.\n",
      "4992/9600 [==============>...............] - ETA: 8s - loss: 0.8514 - accuracy: 0.8900\n",
      "Batch 00190: setting learning rate to 0.07002879319059764.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 0.8525 - accuracy: 0.8904\n",
      "Batch 00191: setting learning rate to 0.06995399747137691.\n",
      "5248/9600 [===============>..............] - ETA: 7s - loss: 0.8509 - accuracy: 0.8912\n",
      "Batch 00192: setting learning rate to 0.06987920175215619.\n",
      "5376/9600 [===============>..............] - ETA: 7s - loss: 0.8497 - accuracy: 0.8914\n",
      "Batch 00193: setting learning rate to 0.06980440603293546.\n",
      "5504/9600 [================>.............] - ETA: 7s - loss: 0.8516 - accuracy: 0.8908\n",
      "Batch 00194: setting learning rate to 0.06972961031371473.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 0.8523 - accuracy: 0.8908\n",
      "Batch 00195: setting learning rate to 0.069654814594494.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.8536 - accuracy: 0.8903\n",
      "Batch 00196: setting learning rate to 0.06958001887527328.\n",
      "5888/9600 [=================>............] - ETA: 6s - loss: 0.8522 - accuracy: 0.8905\n",
      "Batch 00197: setting learning rate to 0.06950522315605256.\n",
      "6016/9600 [=================>............] - ETA: 6s - loss: 0.8512 - accuracy: 0.8908\n",
      "Batch 00198: setting learning rate to 0.06943042743683185.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.8521 - accuracy: 0.8905\n",
      "Batch 00199: setting learning rate to 0.06935563171761112.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.8514 - accuracy: 0.8913\n",
      "Batch 00200: setting learning rate to 0.0692808359983904.\n",
      "6400/9600 [===================>..........] - ETA: 5s - loss: 0.8506 - accuracy: 0.8919\n",
      "Batch 00201: setting learning rate to 0.06920604027916967.\n",
      "6528/9600 [===================>..........] - ETA: 5s - loss: 0.8490 - accuracy: 0.8925\n",
      "Batch 00202: setting learning rate to 0.06913124455994894.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.8481 - accuracy: 0.8929\n",
      "Batch 00203: setting learning rate to 0.06905644884072822.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.8479 - accuracy: 0.8927\n",
      "Batch 00204: setting learning rate to 0.06898165312150749.\n",
      "6912/9600 [====================>.........] - ETA: 4s - loss: 0.8476 - accuracy: 0.8929\n",
      "Batch 00205: setting learning rate to 0.06890685740228676.\n",
      "7040/9600 [=====================>........] - ETA: 4s - loss: 0.8462 - accuracy: 0.8936\n",
      "Batch 00206: setting learning rate to 0.06883206168306605.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.8462 - accuracy: 0.8934\n",
      "Batch 00207: setting learning rate to 0.06875726596384533.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.8445 - accuracy: 0.8941\n",
      "Batch 00208: setting learning rate to 0.06868247024462461.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.8438 - accuracy: 0.8944\n",
      "Batch 00209: setting learning rate to 0.06860767452540388.\n",
      "7552/9600 [======================>.......] - ETA: 3s - loss: 0.8436 - accuracy: 0.8942\n",
      "Batch 00210: setting learning rate to 0.06853287880618315.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.8417 - accuracy: 0.8949\n",
      "Batch 00211: setting learning rate to 0.06845808308696243.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.8411 - accuracy: 0.8951\n",
      "Batch 00212: setting learning rate to 0.0683832873677417.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.8408 - accuracy: 0.8953\n",
      "Batch 00213: setting learning rate to 0.06830849164852097.\n",
      "8064/9600 [========================>.....] - ETA: 2s - loss: 0.8399 - accuracy: 0.8957\n",
      "Batch 00214: setting learning rate to 0.06823369592930024.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.8397 - accuracy: 0.8954\n",
      "Batch 00215: setting learning rate to 0.06815890021007953.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.8389 - accuracy: 0.8954\n",
      "Batch 00216: setting learning rate to 0.06808410449085882.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.8386 - accuracy: 0.8955\n",
      "Batch 00217: setting learning rate to 0.06800930877163809.\n",
      "8576/9600 [=========================>....] - ETA: 1s - loss: 0.8390 - accuracy: 0.8953\n",
      "Batch 00218: setting learning rate to 0.06793451305241736.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.8377 - accuracy: 0.8959\n",
      "Batch 00219: setting learning rate to 0.06785971733319664.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.8370 - accuracy: 0.8963\n",
      "Batch 00220: setting learning rate to 0.06778492161397591.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.8363 - accuracy: 0.8969\n",
      "Batch 00221: setting learning rate to 0.06771012589475518.\n",
      "9088/9600 [===========================>..] - ETA: 0s - loss: 0.8351 - accuracy: 0.8977\n",
      "Batch 00222: setting learning rate to 0.06763533017553446.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.8347 - accuracy: 0.8976\n",
      "Batch 00223: setting learning rate to 0.06756053445631374.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.8357 - accuracy: 0.8970\n",
      "Batch 00224: setting learning rate to 0.06748573873709302.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.8367 - accuracy: 0.8969\n",
      "Batch 00225: setting learning rate to 0.0674109430178723.\n",
      "9600/9600 [==============================] - 19s 2ms/step - loss: 0.8354 - accuracy: 0.8974 - val_loss: 2.3439 - val_accuracy: 0.4867\n",
      "Epoch 4/15\n",
      "\n",
      "Batch 00226: setting learning rate to 0.06733614729865157.\n",
      " 128/9600 [..............................] - ETA: 19s - loss: 0.7844 - accuracy: 0.9141\n",
      "Batch 00227: setting learning rate to 0.06726135157943085.\n",
      " 256/9600 [..............................] - ETA: 18s - loss: 0.7971 - accuracy: 0.9102\n",
      "Batch 00228: setting learning rate to 0.06718655586021012.\n",
      " 384/9600 [>.............................] - ETA: 18s - loss: 0.7927 - accuracy: 0.9089\n",
      "Batch 00229: setting learning rate to 0.0671117601409894.\n",
      " 512/9600 [>.............................] - ETA: 17s - loss: 0.7852 - accuracy: 0.9141\n",
      "Batch 00230: setting learning rate to 0.06703696442176867.\n",
      " 640/9600 [=>............................] - ETA: 17s - loss: 0.7949 - accuracy: 0.9141\n",
      "Batch 00231: setting learning rate to 0.06696216870254794.\n",
      " 768/9600 [=>............................] - ETA: 17s - loss: 0.8077 - accuracy: 0.9102\n",
      "Batch 00232: setting learning rate to 0.06688737298332724.\n",
      " 896/9600 [=>............................] - ETA: 17s - loss: 0.8038 - accuracy: 0.9118\n",
      "Batch 00233: setting learning rate to 0.0668125772641065.\n",
      "1024/9600 [==>...........................] - ETA: 16s - loss: 0.8078 - accuracy: 0.9092\n",
      "Batch 00234: setting learning rate to 0.06673778154488579.\n",
      "1152/9600 [==>...........................] - ETA: 16s - loss: 0.8132 - accuracy: 0.9054\n",
      "Batch 00235: setting learning rate to 0.06666298582566506.\n",
      "1280/9600 [===>..........................] - ETA: 16s - loss: 0.8100 - accuracy: 0.9055\n",
      "Batch 00236: setting learning rate to 0.06658819010644433.\n",
      "1408/9600 [===>..........................] - ETA: 16s - loss: 0.8094 - accuracy: 0.9055\n",
      "Batch 00237: setting learning rate to 0.0665133943872236.\n",
      "1536/9600 [===>..........................] - ETA: 15s - loss: 0.8106 - accuracy: 0.9056\n",
      "Batch 00238: setting learning rate to 0.06643859866800288.\n",
      "1664/9600 [====>.........................] - ETA: 15s - loss: 0.8090 - accuracy: 0.9050\n",
      "Batch 00239: setting learning rate to 0.06636380294878215.\n",
      "1792/9600 [====>.........................] - ETA: 15s - loss: 0.8076 - accuracy: 0.9040\n",
      "Batch 00240: setting learning rate to 0.06628900722956142.\n",
      "1920/9600 [=====>........................] - ETA: 15s - loss: 0.8086 - accuracy: 0.9031\n",
      "Batch 00241: setting learning rate to 0.06621421151034072.\n",
      "2048/9600 [=====>........................] - ETA: 14s - loss: 0.8137 - accuracy: 0.8989\n",
      "Batch 00242: setting learning rate to 0.06613941579111998.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 0.8108 - accuracy: 0.9003\n",
      "Batch 00243: setting learning rate to 0.06606462007189927.\n",
      "2304/9600 [======>.......................] - ETA: 14s - loss: 0.8106 - accuracy: 0.9015\n",
      "Batch 00244: setting learning rate to 0.06598982435267854.\n",
      "2432/9600 [======>.......................] - ETA: 14s - loss: 0.8088 - accuracy: 0.9025\n",
      "Batch 00245: setting learning rate to 0.06591502863345781.\n",
      "2560/9600 [=======>......................] - ETA: 13s - loss: 0.8063 - accuracy: 0.9035\n",
      "Batch 00246: setting learning rate to 0.06584023291423709.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 0.8050 - accuracy: 0.9040\n",
      "Batch 00247: setting learning rate to 0.06576543719501636.\n",
      "2816/9600 [=======>......................] - ETA: 13s - loss: 0.8010 - accuracy: 0.9059\n",
      "Batch 00248: setting learning rate to 0.06569064147579563.\n",
      "2944/9600 [========>.....................] - ETA: 13s - loss: 0.7997 - accuracy: 0.9066\n",
      "Batch 00249: setting learning rate to 0.06561584575657492.\n",
      "3072/9600 [========>.....................] - ETA: 12s - loss: 0.7994 - accuracy: 0.9059\n",
      "Batch 00250: setting learning rate to 0.0655410500373542.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 0.7945 - accuracy: 0.9078\n",
      "Batch 00251: setting learning rate to 0.06546625431813347.\n",
      "3328/9600 [=========>....................] - ETA: 12s - loss: 0.7934 - accuracy: 0.9087\n",
      "Batch 00252: setting learning rate to 0.06539145859891275.\n",
      "3456/9600 [=========>....................] - ETA: 12s - loss: 0.7917 - accuracy: 0.9094\n",
      "Batch 00253: setting learning rate to 0.06531666287969203.\n",
      "3584/9600 [==========>...................] - ETA: 11s - loss: 0.7912 - accuracy: 0.9099\n",
      "Batch 00254: setting learning rate to 0.0652418671604713.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 0.7899 - accuracy: 0.9106\n",
      "Batch 00255: setting learning rate to 0.06516707144125057.\n",
      "3840/9600 [===========>..................] - ETA: 11s - loss: 0.7898 - accuracy: 0.9107\n",
      "Batch 00256: setting learning rate to 0.06509227572202984.\n",
      "3968/9600 [===========>..................] - ETA: 11s - loss: 0.7887 - accuracy: 0.9118\n",
      "Batch 00257: setting learning rate to 0.06501748000280912.\n",
      "4096/9600 [===========>..................] - ETA: 10s - loss: 0.7866 - accuracy: 0.9126\n",
      "Batch 00258: setting learning rate to 0.0649426842835884.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 0.7862 - accuracy: 0.9134\n",
      "Batch 00259: setting learning rate to 0.06486788856436769.\n",
      "4352/9600 [============>.................] - ETA: 10s - loss: 0.7849 - accuracy: 0.9138\n",
      "Batch 00260: setting learning rate to 0.06479309284514695.\n",
      "4480/9600 [=============>................] - ETA: 10s - loss: 0.7834 - accuracy: 0.9143\n",
      "Batch 00261: setting learning rate to 0.06471829712592624.\n",
      "4608/9600 [=============>................] - ETA: 9s - loss: 0.7812 - accuracy: 0.9149 \n",
      "Batch 00262: setting learning rate to 0.06464350140670551.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.7781 - accuracy: 0.9160\n",
      "Batch 00263: setting learning rate to 0.06456870568748478.\n",
      "4864/9600 [==============>...............] - ETA: 9s - loss: 0.7775 - accuracy: 0.9157\n",
      "Batch 00264: setting learning rate to 0.06449390996826405.\n",
      "4992/9600 [==============>...............] - ETA: 9s - loss: 0.7751 - accuracy: 0.9169\n",
      "Batch 00265: setting learning rate to 0.06441911424904333.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 0.7744 - accuracy: 0.9172\n",
      "Batch 00266: setting learning rate to 0.0643443185298226.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.7745 - accuracy: 0.9173\n",
      "Batch 00267: setting learning rate to 0.06426952281060189.\n",
      "5376/9600 [===============>..............] - ETA: 8s - loss: 0.7737 - accuracy: 0.9174\n",
      "Batch 00268: setting learning rate to 0.06419472709138117.\n",
      "5504/9600 [================>.............] - ETA: 8s - loss: 0.7720 - accuracy: 0.9179\n",
      "Batch 00269: setting learning rate to 0.06411993137216043.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 0.7709 - accuracy: 0.9183\n",
      "Batch 00270: setting learning rate to 0.06404513565293972.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.7717 - accuracy: 0.9181\n",
      "Batch 00271: setting learning rate to 0.06397033993371899.\n",
      "5888/9600 [=================>............] - ETA: 7s - loss: 0.7699 - accuracy: 0.9192\n",
      "Batch 00272: setting learning rate to 0.06389554421449826.\n",
      "6016/9600 [=================>............] - ETA: 6s - loss: 0.7700 - accuracy: 0.9192\n",
      "Batch 00273: setting learning rate to 0.06382074849527754.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.7670 - accuracy: 0.9206\n",
      "Batch 00274: setting learning rate to 0.06374595277605681.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.7658 - accuracy: 0.9212\n",
      "Batch 00275: setting learning rate to 0.0636711570568361.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 0.7663 - accuracy: 0.9209\n",
      "Batch 00276: setting learning rate to 0.06359636133761537.\n",
      "6528/9600 [===================>..........] - ETA: 5s - loss: 0.7670 - accuracy: 0.9202\n",
      "Batch 00277: setting learning rate to 0.06352156561839466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.7670 - accuracy: 0.9205\n",
      "Batch 00278: setting learning rate to 0.06344676989917393.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.7673 - accuracy: 0.9207\n",
      "Batch 00279: setting learning rate to 0.0633719741799532.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.7660 - accuracy: 0.9213\n",
      "Batch 00280: setting learning rate to 0.06329717846073248.\n",
      "7040/9600 [=====================>........] - ETA: 4s - loss: 0.7656 - accuracy: 0.9213\n",
      "Batch 00281: setting learning rate to 0.06322238274151175.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.7647 - accuracy: 0.9220\n",
      "Batch 00282: setting learning rate to 0.06314758702229102.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.7643 - accuracy: 0.9219\n",
      "Batch 00283: setting learning rate to 0.0630727913030703.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.7637 - accuracy: 0.9219\n",
      "Batch 00284: setting learning rate to 0.06299799558384958.\n",
      "7552/9600 [======================>.......] - ETA: 3s - loss: 0.7626 - accuracy: 0.9224\n",
      "Batch 00285: setting learning rate to 0.06292319986462885.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.7614 - accuracy: 0.9229\n",
      "Batch 00286: setting learning rate to 0.06284840414540814.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.7608 - accuracy: 0.9233\n",
      "Batch 00287: setting learning rate to 0.06277360842618741.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.7601 - accuracy: 0.9236\n",
      "Batch 00288: setting learning rate to 0.06269881270696669.\n",
      "8064/9600 [========================>.....] - ETA: 3s - loss: 0.7595 - accuracy: 0.9242\n",
      "Batch 00289: setting learning rate to 0.06262401698774596.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.7594 - accuracy: 0.9246\n",
      "Batch 00290: setting learning rate to 0.06254922126852523.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.7601 - accuracy: 0.9243\n",
      "Batch 00291: setting learning rate to 0.06247442554930451.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.7609 - accuracy: 0.9238\n",
      "Batch 00292: setting learning rate to 0.062399629830083785.\n",
      "8576/9600 [=========================>....] - ETA: 2s - loss: 0.7614 - accuracy: 0.9236\n",
      "Batch 00293: setting learning rate to 0.06232483411086306.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.7612 - accuracy: 0.9239\n",
      "Batch 00294: setting learning rate to 0.06225003839164233.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.7612 - accuracy: 0.9240\n",
      "Batch 00295: setting learning rate to 0.062175242672421624.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.7606 - accuracy: 0.9242\n",
      "Batch 00296: setting learning rate to 0.0621004469532009.\n",
      "9088/9600 [===========================>..] - ETA: 1s - loss: 0.7602 - accuracy: 0.9245\n",
      "Batch 00297: setting learning rate to 0.06202565123398017.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.7604 - accuracy: 0.9245\n",
      "Batch 00298: setting learning rate to 0.06195085551475944.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.7601 - accuracy: 0.9244\n",
      "Batch 00299: setting learning rate to 0.06187605979553872.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.7595 - accuracy: 0.9247\n",
      "Batch 00300: setting learning rate to 0.061801264076317995.\n",
      "9600/9600 [==============================] - 20s 2ms/step - loss: 0.7605 - accuracy: 0.9246 - val_loss: 1.7092 - val_accuracy: 0.4729\n",
      "Epoch 5/15\n",
      "\n",
      "Batch 00301: setting learning rate to 0.06172646835709727.\n",
      " 128/9600 [..............................] - ETA: 19s - loss: 0.6514 - accuracy: 0.9766\n",
      "Batch 00302: setting learning rate to 0.061651672637876555.\n",
      " 256/9600 [..............................] - ETA: 18s - loss: 0.7308 - accuracy: 0.9492\n",
      "Batch 00303: setting learning rate to 0.061576876918655814.\n",
      " 384/9600 [>.............................] - ETA: 18s - loss: 0.7289 - accuracy: 0.9505\n",
      "Batch 00304: setting learning rate to 0.06150208119943511.\n",
      " 512/9600 [>.............................] - ETA: 17s - loss: 0.7380 - accuracy: 0.9414\n",
      "Batch 00305: setting learning rate to 0.06142728548021438.\n",
      " 640/9600 [=>............................] - ETA: 17s - loss: 0.7509 - accuracy: 0.9406\n",
      "Batch 00306: setting learning rate to 0.06135248976099365.\n",
      " 768/9600 [=>............................] - ETA: 17s - loss: 0.7360 - accuracy: 0.9479\n",
      "Batch 00307: setting learning rate to 0.061277694041772926.\n",
      " 896/9600 [=>............................] - ETA: 17s - loss: 0.7286 - accuracy: 0.9487\n",
      "Batch 00308: setting learning rate to 0.061202898322552206.\n",
      "1024/9600 [==>...........................] - ETA: 16s - loss: 0.7299 - accuracy: 0.9463\n",
      "Batch 00309: setting learning rate to 0.06112810260333148.\n",
      "1152/9600 [==>...........................] - ETA: 16s - loss: 0.7343 - accuracy: 0.9418\n",
      "Batch 00310: setting learning rate to 0.06105330688411075.\n",
      "1280/9600 [===>..........................] - ETA: 16s - loss: 0.7304 - accuracy: 0.9430\n",
      "Batch 00311: setting learning rate to 0.06097851116489004.\n",
      "1408/9600 [===>..........................] - ETA: 16s - loss: 0.7272 - accuracy: 0.9439\n",
      "Batch 00312: setting learning rate to 0.06090371544566931.\n",
      "1536/9600 [===>..........................] - ETA: 15s - loss: 0.7305 - accuracy: 0.9414\n",
      "Batch 00313: setting learning rate to 0.06082891972644859.\n",
      "1664/9600 [====>.........................] - ETA: 15s - loss: 0.7283 - accuracy: 0.9423\n",
      "Batch 00314: setting learning rate to 0.060754124007227864.\n",
      "1792/9600 [====>.........................] - ETA: 15s - loss: 0.7274 - accuracy: 0.9425\n",
      "Batch 00315: setting learning rate to 0.06067932828800714.\n",
      "1920/9600 [=====>........................] - ETA: 15s - loss: 0.7215 - accuracy: 0.9443\n",
      "Batch 00316: setting learning rate to 0.06060453256878641.\n",
      "2048/9600 [=====>........................] - ETA: 14s - loss: 0.7185 - accuracy: 0.9458\n",
      "Batch 00317: setting learning rate to 0.06052973684956569.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 0.7229 - accuracy: 0.9430\n",
      "Batch 00318: setting learning rate to 0.06045494113034496.\n",
      "2304/9600 [======>.......................] - ETA: 14s - loss: 0.7218 - accuracy: 0.9440\n",
      "Batch 00319: setting learning rate to 0.060380145411124235.\n",
      "2432/9600 [======>.......................] - ETA: 14s - loss: 0.7197 - accuracy: 0.9449\n",
      "Batch 00320: setting learning rate to 0.060305349691903515.\n",
      "2560/9600 [=======>......................] - ETA: 13s - loss: 0.7199 - accuracy: 0.9441\n",
      "Batch 00321: setting learning rate to 0.060230553972682795.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 0.7200 - accuracy: 0.9435\n",
      "Batch 00322: setting learning rate to 0.060155758253462074.\n",
      "2816/9600 [=======>......................] - ETA: 13s - loss: 0.7197 - accuracy: 0.9442\n",
      "Batch 00323: setting learning rate to 0.06008096253424135.\n",
      "2944/9600 [========>.....................] - ETA: 12s - loss: 0.7178 - accuracy: 0.9453\n",
      "Batch 00324: setting learning rate to 0.06000616681502062.\n",
      "3072/9600 [========>.....................] - ETA: 12s - loss: 0.7184 - accuracy: 0.9443\n",
      "Batch 00325: setting learning rate to 0.0599313710957999.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 0.7188 - accuracy: 0.9438\n",
      "Batch 00326: setting learning rate to 0.05985657537657917.\n",
      "3328/9600 [=========>....................] - ETA: 12s - loss: 0.7168 - accuracy: 0.9444\n",
      "Batch 00327: setting learning rate to 0.059781779657358446.\n",
      "3456/9600 [=========>....................] - ETA: 11s - loss: 0.7177 - accuracy: 0.9442\n",
      "Batch 00328: setting learning rate to 0.05970698393813772.\n",
      "3584/9600 [==========>...................] - ETA: 11s - loss: 0.7158 - accuracy: 0.9445\n",
      "Batch 00329: setting learning rate to 0.059632188218917.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 0.7137 - accuracy: 0.9445\n",
      "Batch 00330: setting learning rate to 0.05955739249969628.\n",
      "3840/9600 [===========>..................] - ETA: 11s - loss: 0.7142 - accuracy: 0.9440\n",
      "Batch 00331: setting learning rate to 0.05948259678047556.\n",
      "3968/9600 [===========>..................] - ETA: 10s - loss: 0.7146 - accuracy: 0.9438\n",
      "Batch 00332: setting learning rate to 0.05940780106125483.\n",
      "4096/9600 [===========>..................] - ETA: 10s - loss: 0.7139 - accuracy: 0.9436\n",
      "Batch 00333: setting learning rate to 0.059333005342034104.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 0.7130 - accuracy: 0.9439\n",
      "Batch 00334: setting learning rate to 0.05925820962281338.\n",
      "4352/9600 [============>.................] - ETA: 10s - loss: 0.7115 - accuracy: 0.9444\n",
      "Batch 00335: setting learning rate to 0.059183413903592656.\n",
      "4480/9600 [=============>................] - ETA: 10s - loss: 0.7119 - accuracy: 0.9435\n",
      "Batch 00336: setting learning rate to 0.05910861818437193.\n",
      "4608/9600 [=============>................] - ETA: 9s - loss: 0.7129 - accuracy: 0.9427 \n",
      "Batch 00337: setting learning rate to 0.05903382246515121.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.7124 - accuracy: 0.9428\n",
      "Batch 00338: setting learning rate to 0.05895902674593049.\n",
      "4864/9600 [==============>...............] - ETA: 9s - loss: 0.7112 - accuracy: 0.9435\n",
      "Batch 00339: setting learning rate to 0.05888423102670976.\n",
      "4992/9600 [==============>...............] - ETA: 8s - loss: 0.7108 - accuracy: 0.9439\n",
      "Batch 00340: setting learning rate to 0.05880943530748904.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 0.7098 - accuracy: 0.9449\n",
      "Batch 00341: setting learning rate to 0.058734639588268314.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.7097 - accuracy: 0.9446\n",
      "Batch 00342: setting learning rate to 0.05865984386904759.\n",
      "5376/9600 [===============>..............] - ETA: 8s - loss: 0.7097 - accuracy: 0.9446\n",
      "Batch 00343: setting learning rate to 0.05858504814982687.\n",
      "5504/9600 [================>.............] - ETA: 8s - loss: 0.7090 - accuracy: 0.9451\n",
      "Batch 00344: setting learning rate to 0.05851025243060614.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 0.7080 - accuracy: 0.9455\n",
      "Batch 00345: setting learning rate to 0.05843545671138541.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.7091 - accuracy: 0.9444\n",
      "Batch 00346: setting learning rate to 0.05836066099216469.\n",
      "5888/9600 [=================>............] - ETA: 7s - loss: 0.7090 - accuracy: 0.9440\n",
      "Batch 00347: setting learning rate to 0.05828586527294397.\n",
      "6016/9600 [=================>............] - ETA: 7s - loss: 0.7100 - accuracy: 0.9430\n",
      "Batch 00348: setting learning rate to 0.05821106955372325.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.7104 - accuracy: 0.9425\n",
      "Batch 00349: setting learning rate to 0.058136273834502525.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.7096 - accuracy: 0.9428\n",
      "Batch 00350: setting learning rate to 0.0580614781152818.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 0.7082 - accuracy: 0.9434\n",
      "Batch 00351: setting learning rate to 0.05798668239606108.\n",
      "6528/9600 [===================>..........] - ETA: 6s - loss: 0.7091 - accuracy: 0.9430\n",
      "Batch 00352: setting learning rate to 0.05791188667684035.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.7087 - accuracy: 0.9432\n",
      "Batch 00353: setting learning rate to 0.05783709095761962.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.7075 - accuracy: 0.9440\n",
      "Batch 00354: setting learning rate to 0.057762295238398896.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.7081 - accuracy: 0.9439\n",
      "Batch 00355: setting learning rate to 0.057687499519178176.\n",
      "7040/9600 [=====================>........] - ETA: 5s - loss: 0.7079 - accuracy: 0.9440\n",
      "Batch 00356: setting learning rate to 0.057612703799957456.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.7079 - accuracy: 0.9436\n",
      "Batch 00357: setting learning rate to 0.057537908080736735.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.7079 - accuracy: 0.9433\n",
      "Batch 00358: setting learning rate to 0.05746311236151601.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.7068 - accuracy: 0.9437\n",
      "Batch 00359: setting learning rate to 0.05738831664229528.\n",
      "7552/9600 [======================>.......] - ETA: 4s - loss: 0.7059 - accuracy: 0.9439\n",
      "Batch 00360: setting learning rate to 0.05731352092307456.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.7064 - accuracy: 0.9438\n",
      "Batch 00361: setting learning rate to 0.057238725203853834.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.7053 - accuracy: 0.9444\n",
      "Batch 00362: setting learning rate to 0.05716392948463311.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.7053 - accuracy: 0.9443\n",
      "Batch 00363: setting learning rate to 0.05708913376541238.\n",
      "8064/9600 [========================>.....] - ETA: 3s - loss: 0.7051 - accuracy: 0.9441\n",
      "Batch 00364: setting learning rate to 0.057014338046191666.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.7049 - accuracy: 0.9442\n",
      "Batch 00365: setting learning rate to 0.05693954232697094.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.7049 - accuracy: 0.9441\n",
      "Batch 00366: setting learning rate to 0.05686474660775022.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.7057 - accuracy: 0.9441\n",
      "Batch 00367: setting learning rate to 0.05678995088852949.\n",
      "8576/9600 [=========================>....] - ETA: 2s - loss: 0.7065 - accuracy: 0.9439\n",
      "Batch 00368: setting learning rate to 0.056715155169308765.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.7061 - accuracy: 0.9439\n",
      "Batch 00369: setting learning rate to 0.056640359450088044.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.7051 - accuracy: 0.9444\n",
      "Batch 00370: setting learning rate to 0.05656556373086732.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.7047 - accuracy: 0.9444\n",
      "Batch 00371: setting learning rate to 0.05649076801164659.\n",
      "9088/9600 [===========================>..] - ETA: 1s - loss: 0.7051 - accuracy: 0.9442\n",
      "Batch 00372: setting learning rate to 0.05641597229242587.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.7052 - accuracy: 0.9440\n",
      "Batch 00373: setting learning rate to 0.05634117657320515.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.7048 - accuracy: 0.9439\n",
      "Batch 00374: setting learning rate to 0.05626638085398442.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.7057 - accuracy: 0.9437\n",
      "Batch 00375: setting learning rate to 0.0561915851347637.\n",
      "9600/9600 [==============================] - 20s 2ms/step - loss: 0.7047 - accuracy: 0.9442 - val_loss: 1.0540 - val_accuracy: 0.6517\n",
      "Epoch 6/15\n",
      "\n",
      "Batch 00376: setting learning rate to 0.056116789415542975.\n",
      " 128/9600 [..............................] - ETA: 18s - loss: 0.6416 - accuracy: 0.9844\n",
      "Batch 00377: setting learning rate to 0.056041993696322255.\n",
      " 256/9600 [..............................] - ETA: 18s - loss: 0.6725 - accuracy: 0.9609\n",
      "Batch 00378: setting learning rate to 0.05596719797710153.\n",
      " 384/9600 [>.............................] - ETA: 17s - loss: 0.6787 - accuracy: 0.9583\n",
      "Batch 00379: setting learning rate to 0.0558924022578808.\n",
      " 512/9600 [>.............................] - ETA: 17s - loss: 0.7019 - accuracy: 0.9492\n",
      "Batch 00380: setting learning rate to 0.055817606538660074.\n",
      " 640/9600 [=>............................] - ETA: 17s - loss: 0.6915 - accuracy: 0.9516\n",
      "Batch 00381: setting learning rate to 0.05574281081943935.\n",
      " 768/9600 [=>............................] - ETA: 16s - loss: 0.6980 - accuracy: 0.9479\n",
      "Batch 00382: setting learning rate to 0.05566801510021863.\n",
      " 896/9600 [=>............................] - ETA: 16s - loss: 0.6969 - accuracy: 0.9464\n",
      "Batch 00383: setting learning rate to 0.05559321938099791.\n",
      "1024/9600 [==>...........................] - ETA: 16s - loss: 0.6882 - accuracy: 0.9492\n",
      "Batch 00384: setting learning rate to 0.055518423661777186.\n",
      "1152/9600 [==>...........................] - ETA: 16s - loss: 0.6887 - accuracy: 0.9505\n",
      "Batch 00385: setting learning rate to 0.05544362794255646.\n",
      "1280/9600 [===>..........................] - ETA: 16s - loss: 0.6850 - accuracy: 0.9516\n",
      "Batch 00386: setting learning rate to 0.05536883222333574.\n",
      "1408/9600 [===>..........................] - ETA: 15s - loss: 0.6852 - accuracy: 0.9524\n",
      "Batch 00387: setting learning rate to 0.05529403650411501.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536/9600 [===>..........................] - ETA: 15s - loss: 0.6840 - accuracy: 0.9518\n",
      "Batch 00388: setting learning rate to 0.055219240784894284.\n",
      "1664/9600 [====>.........................] - ETA: 15s - loss: 0.6800 - accuracy: 0.9531\n",
      "Batch 00389: setting learning rate to 0.05514444506567356.\n",
      "1792/9600 [====>.........................] - ETA: 15s - loss: 0.6814 - accuracy: 0.9515\n",
      "Batch 00390: setting learning rate to 0.055069649346452844.\n",
      "1920/9600 [=====>........................] - ETA: 14s - loss: 0.6830 - accuracy: 0.9521\n",
      "Batch 00391: setting learning rate to 0.05499485362723212.\n",
      "2048/9600 [=====>........................] - ETA: 14s - loss: 0.6841 - accuracy: 0.9526\n",
      "Batch 00392: setting learning rate to 0.054920057908011397.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 0.6854 - accuracy: 0.9522\n",
      "Batch 00393: setting learning rate to 0.05484526218879067.\n",
      "2304/9600 [======>.......................] - ETA: 14s - loss: 0.6833 - accuracy: 0.9536\n",
      "Batch 00394: setting learning rate to 0.05477046646956994.\n",
      "2432/9600 [======>.......................] - ETA: 13s - loss: 0.6844 - accuracy: 0.9527\n",
      "Batch 00395: setting learning rate to 0.05469567075034922.\n",
      "2560/9600 [=======>......................] - ETA: 13s - loss: 0.6814 - accuracy: 0.9539\n",
      "Batch 00396: setting learning rate to 0.054620875031128495.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 0.6820 - accuracy: 0.9539\n",
      "Batch 00397: setting learning rate to 0.05454607931190777.\n",
      "2816/9600 [=======>......................] - ETA: 12s - loss: 0.6814 - accuracy: 0.9542\n",
      "Batch 00398: setting learning rate to 0.05447128359268704.\n",
      "2944/9600 [========>.....................] - ETA: 12s - loss: 0.6786 - accuracy: 0.9552\n",
      "Batch 00399: setting learning rate to 0.05439648787346633.\n",
      "3072/9600 [========>.....................] - ETA: 12s - loss: 0.6796 - accuracy: 0.9554\n",
      "Batch 00400: setting learning rate to 0.0543216921542456.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 0.6781 - accuracy: 0.9559\n",
      "Batch 00401: setting learning rate to 0.05424689643502488.\n",
      "3328/9600 [=========>....................] - ETA: 11s - loss: 0.6749 - accuracy: 0.9573\n",
      "Batch 00402: setting learning rate to 0.05417210071580415.\n",
      "3456/9600 [=========>....................] - ETA: 11s - loss: 0.6743 - accuracy: 0.9572\n",
      "Batch 00403: setting learning rate to 0.05409730499658343.\n",
      "3584/9600 [==========>...................] - ETA: 11s - loss: 0.6738 - accuracy: 0.9568\n",
      "Batch 00404: setting learning rate to 0.054022509277362706.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 0.6724 - accuracy: 0.9572\n",
      "Batch 00405: setting learning rate to 0.05394771355814198.\n",
      "3840/9600 [===========>..................] - ETA: 11s - loss: 0.6703 - accuracy: 0.9583\n",
      "Batch 00406: setting learning rate to 0.05387291783892125.\n",
      "3968/9600 [===========>..................] - ETA: 10s - loss: 0.6725 - accuracy: 0.9577\n",
      "Batch 00407: setting learning rate to 0.05379812211970053.\n",
      "4096/9600 [===========>..................] - ETA: 10s - loss: 0.6709 - accuracy: 0.9587\n",
      "Batch 00408: setting learning rate to 0.05372332640047981.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 0.6714 - accuracy: 0.9579\n",
      "Batch 00409: setting learning rate to 0.053648530681259084.\n",
      "4352/9600 [============>.................] - ETA: 10s - loss: 0.6713 - accuracy: 0.9580\n",
      "Batch 00410: setting learning rate to 0.05357373496203836.\n",
      "4480/9600 [=============>................] - ETA: 9s - loss: 0.6715 - accuracy: 0.9578 \n",
      "Batch 00411: setting learning rate to 0.053498939242817636.\n",
      "4608/9600 [=============>................] - ETA: 9s - loss: 0.6704 - accuracy: 0.9579\n",
      "Batch 00412: setting learning rate to 0.053424143523596916.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.6717 - accuracy: 0.9567\n",
      "Batch 00413: setting learning rate to 0.05334934780437619.\n",
      "4864/9600 [==============>...............] - ETA: 9s - loss: 0.6710 - accuracy: 0.9568\n",
      "Batch 00414: setting learning rate to 0.05327455208515546.\n",
      "4992/9600 [==============>...............] - ETA: 8s - loss: 0.6717 - accuracy: 0.9563\n",
      "Batch 00415: setting learning rate to 0.05319975636593474.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 0.6714 - accuracy: 0.9563\n",
      "Batch 00416: setting learning rate to 0.05312496064671402.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.6702 - accuracy: 0.9569\n",
      "Batch 00417: setting learning rate to 0.053050164927493294.\n",
      "5376/9600 [===============>..............] - ETA: 8s - loss: 0.6710 - accuracy: 0.9565\n",
      "Batch 00418: setting learning rate to 0.05297536920827257.\n",
      "5504/9600 [================>.............] - ETA: 7s - loss: 0.6723 - accuracy: 0.9564\n",
      "Batch 00419: setting learning rate to 0.05290057348905185.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 0.6740 - accuracy: 0.9554\n",
      "Batch 00420: setting learning rate to 0.05282577776983112.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.6745 - accuracy: 0.9552\n",
      "Batch 00421: setting learning rate to 0.0527509820506104.\n",
      "5888/9600 [=================>............] - ETA: 7s - loss: 0.6735 - accuracy: 0.9557\n",
      "Batch 00422: setting learning rate to 0.05267618633138967.\n",
      "6016/9600 [=================>............] - ETA: 6s - loss: 0.6728 - accuracy: 0.9555\n",
      "Batch 00423: setting learning rate to 0.052601390612168945.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.6727 - accuracy: 0.9554\n",
      "Batch 00424: setting learning rate to 0.052526594892948225.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.6720 - accuracy: 0.9555\n",
      "Batch 00425: setting learning rate to 0.052451799173727505.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 0.6720 - accuracy: 0.9555\n",
      "Batch 00426: setting learning rate to 0.05237700345450678.\n",
      "6528/9600 [===================>..........] - ETA: 5s - loss: 0.6721 - accuracy: 0.9554\n",
      "Batch 00427: setting learning rate to 0.05230220773528605.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.6723 - accuracy: 0.9554\n",
      "Batch 00428: setting learning rate to 0.05222741201606533.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.6716 - accuracy: 0.9556\n",
      "Batch 00429: setting learning rate to 0.05215261629684461.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.6707 - accuracy: 0.9560\n",
      "Batch 00430: setting learning rate to 0.05207782057762388.\n",
      "7040/9600 [=====================>........] - ETA: 4s - loss: 0.6700 - accuracy: 0.9560\n",
      "Batch 00431: setting learning rate to 0.052003024858403156.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.6703 - accuracy: 0.9559\n",
      "Batch 00432: setting learning rate to 0.05192822913918243.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.6699 - accuracy: 0.9561\n",
      "Batch 00433: setting learning rate to 0.05185343341996171.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.6702 - accuracy: 0.9562\n",
      "Batch 00434: setting learning rate to 0.05177863770074099.\n",
      "7552/9600 [======================>.......] - ETA: 3s - loss: 0.6697 - accuracy: 0.9563\n",
      "Batch 00435: setting learning rate to 0.05170384198152026.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.6701 - accuracy: 0.9563\n",
      "Batch 00436: setting learning rate to 0.051629046262299534.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.6695 - accuracy: 0.9565\n",
      "Batch 00437: setting learning rate to 0.051554250543078814.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.6699 - accuracy: 0.9563\n",
      "Batch 00438: setting learning rate to 0.051479454823858094.\n",
      "8064/9600 [========================>.....] - ETA: 2s - loss: 0.6705 - accuracy: 0.9557\n",
      "Batch 00439: setting learning rate to 0.05140465910463737.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.6709 - accuracy: 0.9554\n",
      "Batch 00440: setting learning rate to 0.05132986338541664.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.6706 - accuracy: 0.9555\n",
      "Batch 00441: setting learning rate to 0.05125506766619592.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.6713 - accuracy: 0.9555\n",
      "Batch 00442: setting learning rate to 0.0511802719469752.\n",
      "8576/9600 [=========================>....] - ETA: 1s - loss: 0.6718 - accuracy: 0.9552\n",
      "Batch 00443: setting learning rate to 0.05110547622775447.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.6723 - accuracy: 0.9552\n",
      "Batch 00444: setting learning rate to 0.051030680508533745.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.6716 - accuracy: 0.9555\n",
      "Batch 00445: setting learning rate to 0.05095588478931302.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.6710 - accuracy: 0.9556\n",
      "Batch 00446: setting learning rate to 0.0508810890700923.\n",
      "9088/9600 [===========================>..] - ETA: 1s - loss: 0.6719 - accuracy: 0.9552\n",
      "Batch 00447: setting learning rate to 0.05080629335087158.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.6719 - accuracy: 0.9551\n",
      "Batch 00448: setting learning rate to 0.05073149763165085.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.6711 - accuracy: 0.9554\n",
      "Batch 00449: setting learning rate to 0.05065670191243012.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.6704 - accuracy: 0.9557\n",
      "Batch 00450: setting learning rate to 0.0505819061932094.\n",
      "9600/9600 [==============================] - 20s 2ms/step - loss: 0.6698 - accuracy: 0.9560 - val_loss: 0.4852 - val_accuracy: 0.9108\n",
      "Epoch 7/15\n",
      "\n",
      "Batch 00451: setting learning rate to 0.05050711047398868.\n",
      " 128/9600 [..............................] - ETA: 19s - loss: 0.7234 - accuracy: 0.9219\n",
      "Batch 00452: setting learning rate to 0.050432314754767955.\n",
      " 256/9600 [..............................] - ETA: 18s - loss: 0.6675 - accuracy: 0.9570\n",
      "Batch 00453: setting learning rate to 0.05035751903554723.\n",
      " 384/9600 [>.............................] - ETA: 18s - loss: 0.6793 - accuracy: 0.9505\n",
      "Batch 00454: setting learning rate to 0.05028272331632651.\n",
      " 512/9600 [>.............................] - ETA: 17s - loss: 0.6821 - accuracy: 0.9453\n",
      "Batch 00455: setting learning rate to 0.05020792759710579.\n",
      " 640/9600 [=>............................] - ETA: 17s - loss: 0.6793 - accuracy: 0.9453\n",
      "Batch 00456: setting learning rate to 0.05013313187788506.\n",
      " 768/9600 [=>............................] - ETA: 17s - loss: 0.6719 - accuracy: 0.9479\n",
      "Batch 00457: setting learning rate to 0.050058336158664334.\n",
      " 896/9600 [=>............................] - ETA: 17s - loss: 0.6669 - accuracy: 0.9487\n",
      "Batch 00458: setting learning rate to 0.049983540439443606.\n",
      "1024/9600 [==>...........................] - ETA: 16s - loss: 0.6641 - accuracy: 0.9492\n",
      "Batch 00459: setting learning rate to 0.049908744720222886.\n",
      "1152/9600 [==>...........................] - ETA: 16s - loss: 0.6616 - accuracy: 0.9514\n",
      "Batch 00460: setting learning rate to 0.049833949001002166.\n",
      "1280/9600 [===>..........................] - ETA: 16s - loss: 0.6599 - accuracy: 0.9523\n",
      "Batch 00461: setting learning rate to 0.04975915328178144.\n",
      "1408/9600 [===>..........................] - ETA: 16s - loss: 0.6618 - accuracy: 0.9524\n",
      "Batch 00462: setting learning rate to 0.04968435756256071.\n",
      "1536/9600 [===>..........................] - ETA: 15s - loss: 0.6611 - accuracy: 0.9525\n",
      "Batch 00463: setting learning rate to 0.04960956184333999.\n",
      "1664/9600 [====>.........................] - ETA: 15s - loss: 0.6642 - accuracy: 0.9513\n",
      "Batch 00464: setting learning rate to 0.04953476612411927.\n",
      "1792/9600 [====>.........................] - ETA: 15s - loss: 0.6650 - accuracy: 0.9515\n",
      "Batch 00465: setting learning rate to 0.049459970404898544.\n",
      "1920/9600 [=====>........................] - ETA: 15s - loss: 0.6638 - accuracy: 0.9521\n",
      "Batch 00466: setting learning rate to 0.04938517468567782.\n",
      "2048/9600 [=====>........................] - ETA: 14s - loss: 0.6650 - accuracy: 0.9507\n",
      "Batch 00467: setting learning rate to 0.04931037896645709.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 0.6637 - accuracy: 0.9517\n",
      "Batch 00468: setting learning rate to 0.04923558324723638.\n",
      "2304/9600 [======>.......................] - ETA: 14s - loss: 0.6636 - accuracy: 0.9510\n",
      "Batch 00469: setting learning rate to 0.04916078752801565.\n",
      "2432/9600 [======>.......................] - ETA: 14s - loss: 0.6602 - accuracy: 0.9535\n",
      "Batch 00470: setting learning rate to 0.04908599180879492.\n",
      "2560/9600 [=======>......................] - ETA: 13s - loss: 0.6596 - accuracy: 0.9543\n",
      "Batch 00471: setting learning rate to 0.049011196089574195.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 0.6585 - accuracy: 0.9546\n",
      "Batch 00472: setting learning rate to 0.048936400370353475.\n",
      "2816/9600 [=======>......................] - ETA: 13s - loss: 0.6589 - accuracy: 0.9538\n",
      "Batch 00473: setting learning rate to 0.048861604651132755.\n",
      "2944/9600 [========>.....................] - ETA: 13s - loss: 0.6587 - accuracy: 0.9548\n",
      "Batch 00474: setting learning rate to 0.04878680893191203.\n",
      "3072/9600 [========>.....................] - ETA: 12s - loss: 0.6605 - accuracy: 0.9541\n",
      "Batch 00475: setting learning rate to 0.0487120132126913.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 0.6588 - accuracy: 0.9547\n",
      "Batch 00476: setting learning rate to 0.04863721749347058.\n",
      "3328/9600 [=========>....................] - ETA: 12s - loss: 0.6606 - accuracy: 0.9537\n",
      "Batch 00477: setting learning rate to 0.04856242177424986.\n",
      "3456/9600 [=========>....................] - ETA: 12s - loss: 0.6593 - accuracy: 0.9540\n",
      "Batch 00478: setting learning rate to 0.04848762605502913.\n",
      "3584/9600 [==========>...................] - ETA: 11s - loss: 0.6563 - accuracy: 0.9554\n",
      "Batch 00479: setting learning rate to 0.048412830335808406.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 0.6557 - accuracy: 0.9553\n",
      "Batch 00480: setting learning rate to 0.04833803461658768.\n",
      "3840/9600 [===========>..................] - ETA: 11s - loss: 0.6551 - accuracy: 0.9555\n",
      "Batch 00481: setting learning rate to 0.04826323889736696.\n",
      "3968/9600 [===========>..................] - ETA: 11s - loss: 0.6544 - accuracy: 0.9556\n",
      "Batch 00482: setting learning rate to 0.04818844317814624.\n",
      "4096/9600 [===========>..................] - ETA: 10s - loss: 0.6550 - accuracy: 0.9556\n",
      "Batch 00483: setting learning rate to 0.04811364745892551.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 0.6545 - accuracy: 0.9557\n",
      "Batch 00484: setting learning rate to 0.048038851739704784.\n",
      "4352/9600 [============>.................] - ETA: 10s - loss: 0.6524 - accuracy: 0.9570\n",
      "Batch 00485: setting learning rate to 0.047964056020484064.\n",
      "4480/9600 [=============>................] - ETA: 10s - loss: 0.6531 - accuracy: 0.9567\n",
      "Batch 00486: setting learning rate to 0.047889260301263344.\n",
      "4608/9600 [=============>................] - ETA: 9s - loss: 0.6532 - accuracy: 0.9572 \n",
      "Batch 00487: setting learning rate to 0.047814464582042616.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.6525 - accuracy: 0.9576\n",
      "Batch 00488: setting learning rate to 0.04773966886282189.\n",
      "4864/9600 [==============>...............] - ETA: 9s - loss: 0.6510 - accuracy: 0.9585\n",
      "Batch 00489: setting learning rate to 0.04766487314360117.\n",
      "4992/9600 [==============>...............] - ETA: 9s - loss: 0.6507 - accuracy: 0.9583\n",
      "Batch 00490: setting learning rate to 0.04759007742438045.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 0.6508 - accuracy: 0.9586\n",
      "Batch 00491: setting learning rate to 0.04751528170515972.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.6518 - accuracy: 0.9588\n",
      "Batch 00492: setting learning rate to 0.047440485985938995.\n",
      "5376/9600 [===============>..............] - ETA: 8s - loss: 0.6519 - accuracy: 0.9591\n",
      "Batch 00493: setting learning rate to 0.04736569026671827.\n",
      "5504/9600 [================>.............] - ETA: 8s - loss: 0.6512 - accuracy: 0.9595\n",
      "Batch 00494: setting learning rate to 0.04729089454749755.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 0.6505 - accuracy: 0.9599\n",
      "Batch 00495: setting learning rate to 0.04721609882827683.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.6505 - accuracy: 0.9597\n",
      "Batch 00496: setting learning rate to 0.0471413031090561.\n",
      "5888/9600 [=================>............] - ETA: 7s - loss: 0.6525 - accuracy: 0.9591\n",
      "Batch 00497: setting learning rate to 0.04706650738983537.\n",
      "6016/9600 [=================>............] - ETA: 7s - loss: 0.6537 - accuracy: 0.9586\n",
      "Batch 00498: setting learning rate to 0.04699171167061465.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.6527 - accuracy: 0.9590\n",
      "Batch 00499: setting learning rate to 0.04691691595139393.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.6523 - accuracy: 0.9593\n",
      "Batch 00500: setting learning rate to 0.046842120232173205.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 0.6520 - accuracy: 0.9597\n",
      "Batch 00501: setting learning rate to 0.04676732451295248.\n",
      "6528/9600 [===================>..........] - ETA: 6s - loss: 0.6513 - accuracy: 0.9600\n",
      "Batch 00502: setting learning rate to 0.04669252879373175.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.6506 - accuracy: 0.9603\n",
      "Batch 00503: setting learning rate to 0.04661773307451104.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.6511 - accuracy: 0.9602\n",
      "Batch 00504: setting learning rate to 0.04654293735529031.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.6505 - accuracy: 0.9606\n",
      "Batch 00505: setting learning rate to 0.04646814163606958.\n",
      "7040/9600 [=====================>........] - ETA: 5s - loss: 0.6512 - accuracy: 0.9602\n",
      "Batch 00506: setting learning rate to 0.046393345916848856.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.6518 - accuracy: 0.9600\n",
      "Batch 00507: setting learning rate to 0.046318550197628136.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.6514 - accuracy: 0.9603\n",
      "Batch 00508: setting learning rate to 0.046243754478407416.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.6510 - accuracy: 0.9607\n",
      "Batch 00509: setting learning rate to 0.04616895875918669.\n",
      "7552/9600 [======================>.......] - ETA: 4s - loss: 0.6507 - accuracy: 0.9608\n",
      "Batch 00510: setting learning rate to 0.04609416303996596.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.6499 - accuracy: 0.9611\n",
      "Batch 00511: setting learning rate to 0.04601936732074524.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.6503 - accuracy: 0.9611\n",
      "Batch 00512: setting learning rate to 0.04594457160152452.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.6508 - accuracy: 0.9609\n",
      "Batch 00513: setting learning rate to 0.045869775882303794.\n",
      "8064/9600 [========================>.....] - ETA: 3s - loss: 0.6513 - accuracy: 0.9611\n",
      "Batch 00514: setting learning rate to 0.04579498016308307.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.6509 - accuracy: 0.9609\n",
      "Batch 00515: setting learning rate to 0.04572018444386234.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.6509 - accuracy: 0.9607\n",
      "Batch 00516: setting learning rate to 0.045645388724641626.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.6506 - accuracy: 0.9611\n",
      "Batch 00517: setting learning rate to 0.0455705930054209.\n",
      "8576/9600 [=========================>....] - ETA: 2s - loss: 0.6515 - accuracy: 0.9606\n",
      "Batch 00518: setting learning rate to 0.04549579728620017.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.6511 - accuracy: 0.9607\n",
      "Batch 00519: setting learning rate to 0.045421001566979445.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.6505 - accuracy: 0.9609\n",
      "Batch 00520: setting learning rate to 0.045346205847758725.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.6509 - accuracy: 0.9607\n",
      "Batch 00521: setting learning rate to 0.045271410128538005.\n",
      "9088/9600 [===========================>..] - ETA: 1s - loss: 0.6519 - accuracy: 0.9606\n",
      "Batch 00522: setting learning rate to 0.04519661440931728.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.6518 - accuracy: 0.9606\n",
      "Batch 00523: setting learning rate to 0.04512181869009655.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.6517 - accuracy: 0.9607\n",
      "Batch 00524: setting learning rate to 0.04504702297087583.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.6520 - accuracy: 0.9605\n",
      "Batch 00525: setting learning rate to 0.04497222725165511.\n",
      "9600/9600 [==============================] - 20s 2ms/step - loss: 0.6523 - accuracy: 0.9602 - val_loss: 0.3871 - val_accuracy: 0.9442\n",
      "Epoch 8/15\n",
      "\n",
      "Batch 00526: setting learning rate to 0.04489743153243438.\n",
      " 128/9600 [..............................] - ETA: 20s - loss: 0.6316 - accuracy: 0.9375\n",
      "Batch 00527: setting learning rate to 0.044822635813213656.\n",
      " 256/9600 [..............................] - ETA: 19s - loss: 0.6381 - accuracy: 0.9453\n",
      "Batch 00528: setting learning rate to 0.04474784009399293.\n",
      " 384/9600 [>.............................] - ETA: 18s - loss: 0.6181 - accuracy: 0.9635\n",
      "Batch 00529: setting learning rate to 0.044673044374772215.\n",
      " 512/9600 [>.............................] - ETA: 18s - loss: 0.6296 - accuracy: 0.9609\n",
      "Batch 00530: setting learning rate to 0.04459824865555149.\n",
      " 640/9600 [=>............................] - ETA: 18s - loss: 0.6234 - accuracy: 0.9641\n",
      "Batch 00531: setting learning rate to 0.04452345293633076.\n",
      " 768/9600 [=>............................] - ETA: 17s - loss: 0.6211 - accuracy: 0.9688\n",
      "Batch 00532: setting learning rate to 0.044448657217110034.\n",
      " 896/9600 [=>............................] - ETA: 17s - loss: 0.6298 - accuracy: 0.9643\n",
      "Batch 00533: setting learning rate to 0.044373861497889314.\n",
      "1024/9600 [==>...........................] - ETA: 17s - loss: 0.6305 - accuracy: 0.9629\n",
      "Batch 00534: setting learning rate to 0.04429906577866859.\n",
      "1152/9600 [==>...........................] - ETA: 16s - loss: 0.6335 - accuracy: 0.9635\n",
      "Batch 00535: setting learning rate to 0.044224270059447866.\n",
      "1280/9600 [===>..........................] - ETA: 16s - loss: 0.6310 - accuracy: 0.9656\n",
      "Batch 00536: setting learning rate to 0.04414947434022714.\n",
      "1408/9600 [===>..........................] - ETA: 16s - loss: 0.6311 - accuracy: 0.9659\n",
      "Batch 00537: setting learning rate to 0.04407467862100641.\n",
      "1536/9600 [===>..........................] - ETA: 15s - loss: 0.6360 - accuracy: 0.9655\n",
      "Batch 00538: setting learning rate to 0.0439998829017857.\n",
      "1664/9600 [====>.........................] - ETA: 15s - loss: 0.6390 - accuracy: 0.9651\n",
      "Batch 00539: setting learning rate to 0.04392508718256497.\n",
      "1792/9600 [====>.........................] - ETA: 15s - loss: 0.6367 - accuracy: 0.9671\n",
      "Batch 00540: setting learning rate to 0.043850291463344244.\n",
      "1920/9600 [=====>........................] - ETA: 15s - loss: 0.6356 - accuracy: 0.9667\n",
      "Batch 00541: setting learning rate to 0.04377549574412352.\n",
      "2048/9600 [=====>........................] - ETA: 14s - loss: 0.6362 - accuracy: 0.9663\n",
      "Batch 00542: setting learning rate to 0.043700700024902804.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 0.6363 - accuracy: 0.9660\n",
      "Batch 00543: setting learning rate to 0.04362590430568208.\n",
      "2304/9600 [======>.......................] - ETA: 14s - loss: 0.6365 - accuracy: 0.9657\n",
      "Batch 00544: setting learning rate to 0.04355110858646135.\n",
      "2432/9600 [======>.......................] - ETA: 13s - loss: 0.6357 - accuracy: 0.9655\n",
      "Batch 00545: setting learning rate to 0.04347631286724062.\n",
      "2560/9600 [=======>......................] - ETA: 13s - loss: 0.6382 - accuracy: 0.9648\n",
      "Batch 00546: setting learning rate to 0.0434015171480199.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 0.6369 - accuracy: 0.9658\n",
      "Batch 00547: setting learning rate to 0.04332672142879918.\n",
      "2816/9600 [=======>......................] - ETA: 13s - loss: 0.6375 - accuracy: 0.9652\n",
      "Batch 00548: setting learning rate to 0.043251925709578455.\n",
      "2944/9600 [========>.....................] - ETA: 12s - loss: 0.6366 - accuracy: 0.9657\n",
      "Batch 00549: setting learning rate to 0.04317712999035773.\n",
      "3072/9600 [========>.....................] - ETA: 12s - loss: 0.6367 - accuracy: 0.9658\n",
      "Batch 00550: setting learning rate to 0.043102334271137.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 0.6353 - accuracy: 0.9663\n",
      "Batch 00551: setting learning rate to 0.04302753855191629.\n",
      "3328/9600 [=========>....................] - ETA: 11s - loss: 0.6335 - accuracy: 0.9669\n",
      "Batch 00552: setting learning rate to 0.04295274283269556.\n",
      "3456/9600 [=========>....................] - ETA: 11s - loss: 0.6335 - accuracy: 0.9667\n",
      "Batch 00553: setting learning rate to 0.04287794711347483.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3584/9600 [==========>...................] - ETA: 11s - loss: 0.6358 - accuracy: 0.9657\n",
      "Batch 00554: setting learning rate to 0.042803151394254106.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 0.6368 - accuracy: 0.9652\n",
      "Batch 00555: setting learning rate to 0.04272835567503339.\n",
      "3840/9600 [===========>..................] - ETA: 10s - loss: 0.6361 - accuracy: 0.9651\n",
      "Batch 00556: setting learning rate to 0.042653559955812666.\n",
      "3968/9600 [===========>..................] - ETA: 10s - loss: 0.6360 - accuracy: 0.9652\n",
      "Batch 00557: setting learning rate to 0.04257876423659194.\n",
      "4096/9600 [===========>..................] - ETA: 10s - loss: 0.6365 - accuracy: 0.9646\n",
      "Batch 00558: setting learning rate to 0.04250396851737121.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 0.6364 - accuracy: 0.9652\n",
      "Batch 00559: setting learning rate to 0.04242917279815049.\n",
      "4352/9600 [============>.................] - ETA: 9s - loss: 0.6359 - accuracy: 0.9653 \n",
      "Batch 00560: setting learning rate to 0.04235437707892977.\n",
      "4480/9600 [=============>................] - ETA: 9s - loss: 0.6363 - accuracy: 0.9647\n",
      "Batch 00561: setting learning rate to 0.042279581359709044.\n",
      "4608/9600 [=============>................] - ETA: 9s - loss: 0.6363 - accuracy: 0.9642\n",
      "Batch 00562: setting learning rate to 0.04220478564048832.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.6350 - accuracy: 0.9649\n",
      "Batch 00563: setting learning rate to 0.04212998992126759.\n",
      "4864/9600 [==============>...............] - ETA: 8s - loss: 0.6350 - accuracy: 0.9646\n",
      "Batch 00564: setting learning rate to 0.042055194202046876.\n",
      "4992/9600 [==============>...............] - ETA: 8s - loss: 0.6338 - accuracy: 0.9651\n",
      "Batch 00565: setting learning rate to 0.04198039848282615.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 0.6328 - accuracy: 0.9660\n",
      "Batch 00566: setting learning rate to 0.04190560276360542.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.6339 - accuracy: 0.9657\n",
      "Batch 00567: setting learning rate to 0.041830807044384695.\n",
      "5376/9600 [===============>..............] - ETA: 7s - loss: 0.6330 - accuracy: 0.9663\n",
      "Batch 00568: setting learning rate to 0.04175601132516398.\n",
      "5504/9600 [================>.............] - ETA: 7s - loss: 0.6317 - accuracy: 0.9669\n",
      "Batch 00569: setting learning rate to 0.041681215605943255.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 0.6321 - accuracy: 0.9670\n",
      "Batch 00570: setting learning rate to 0.04160641988672253.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.6322 - accuracy: 0.9668\n",
      "Batch 00571: setting learning rate to 0.0415316241675018.\n",
      "5888/9600 [=================>............] - ETA: 6s - loss: 0.6320 - accuracy: 0.9671\n",
      "Batch 00572: setting learning rate to 0.04145682844828107.\n",
      "6016/9600 [=================>............] - ETA: 6s - loss: 0.6315 - accuracy: 0.9669\n",
      "Batch 00573: setting learning rate to 0.04138203272906036.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.6309 - accuracy: 0.9670\n",
      "Batch 00574: setting learning rate to 0.04130723700983963.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.6303 - accuracy: 0.9670\n",
      "Batch 00575: setting learning rate to 0.041232441290618906.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 0.6301 - accuracy: 0.9672\n",
      "Batch 00576: setting learning rate to 0.04115764557139818.\n",
      "6528/9600 [===================>..........] - ETA: 5s - loss: 0.6297 - accuracy: 0.9671\n",
      "Batch 00577: setting learning rate to 0.041082849852177465.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.6293 - accuracy: 0.9672\n",
      "Batch 00578: setting learning rate to 0.04100805413295674.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.6293 - accuracy: 0.9671\n",
      "Batch 00579: setting learning rate to 0.04093325841373601.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.6292 - accuracy: 0.9670\n",
      "Batch 00580: setting learning rate to 0.040858462694515284.\n",
      "7040/9600 [=====================>........] - ETA: 4s - loss: 0.6294 - accuracy: 0.9670\n",
      "Batch 00581: setting learning rate to 0.04078366697529457.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.6294 - accuracy: 0.9671\n",
      "Batch 00582: setting learning rate to 0.04070887125607384.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.6303 - accuracy: 0.9664\n",
      "Batch 00583: setting learning rate to 0.040634075536853116.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.6298 - accuracy: 0.9666\n",
      "Batch 00584: setting learning rate to 0.04055927981763239.\n",
      "7552/9600 [======================>.......] - ETA: 3s - loss: 0.6303 - accuracy: 0.9662\n",
      "Batch 00585: setting learning rate to 0.04048448409841166.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.6305 - accuracy: 0.9663\n",
      "Batch 00586: setting learning rate to 0.04040968837919095.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.6301 - accuracy: 0.9662\n",
      "Batch 00587: setting learning rate to 0.04033489265997022.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.6299 - accuracy: 0.9662\n",
      "Batch 00588: setting learning rate to 0.040260096940749494.\n",
      "8064/9600 [========================>.....] - ETA: 2s - loss: 0.6301 - accuracy: 0.9661\n",
      "Batch 00589: setting learning rate to 0.04018530122152877.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.6301 - accuracy: 0.9661\n",
      "Batch 00590: setting learning rate to 0.040110505502308054.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.6301 - accuracy: 0.9659\n",
      "Batch 00591: setting learning rate to 0.04003570978308733.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.6301 - accuracy: 0.9658\n",
      "Batch 00592: setting learning rate to 0.0399609140638666.\n",
      "8576/9600 [=========================>....] - ETA: 1s - loss: 0.6297 - accuracy: 0.9658\n",
      "Batch 00593: setting learning rate to 0.03988611834464587.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.6293 - accuracy: 0.9661\n",
      "Batch 00594: setting learning rate to 0.03981132262542516.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.6298 - accuracy: 0.9658\n",
      "Batch 00595: setting learning rate to 0.03973652690620443.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.6301 - accuracy: 0.9656\n",
      "Batch 00596: setting learning rate to 0.039661731186983705.\n",
      "9088/9600 [===========================>..] - ETA: 0s - loss: 0.6298 - accuracy: 0.9658\n",
      "Batch 00597: setting learning rate to 0.03958693546776298.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.6302 - accuracy: 0.9655\n",
      "Batch 00598: setting learning rate to 0.03951213974854225.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.6300 - accuracy: 0.9658\n",
      "Batch 00599: setting learning rate to 0.03943734402932154.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.6303 - accuracy: 0.9657\n",
      "Batch 00600: setting learning rate to 0.03936254831010081.\n",
      "9600/9600 [==============================] - 19s 2ms/step - loss: 0.6304 - accuracy: 0.9656 - val_loss: 0.2672 - val_accuracy: 0.9592\n",
      "Epoch 9/15\n",
      "\n",
      "Batch 00601: setting learning rate to 0.03928775259088008.\n",
      " 128/9600 [..............................] - ETA: 19s - loss: 0.5994 - accuracy: 0.9766\n",
      "Batch 00602: setting learning rate to 0.039212956871659356.\n",
      " 256/9600 [..............................] - ETA: 17s - loss: 0.6376 - accuracy: 0.9648\n",
      "Batch 00603: setting learning rate to 0.03913816115243864.\n",
      " 384/9600 [>.............................] - ETA: 17s - loss: 0.6357 - accuracy: 0.9661\n",
      "Batch 00604: setting learning rate to 0.039063365433217916.\n",
      " 512/9600 [>.............................] - ETA: 17s - loss: 0.6290 - accuracy: 0.9727\n",
      "Batch 00605: setting learning rate to 0.03898856971399719.\n",
      " 640/9600 [=>............................] - ETA: 16s - loss: 0.6340 - accuracy: 0.9719\n",
      "Batch 00606: setting learning rate to 0.03891377399477646.\n",
      " 768/9600 [=>............................] - ETA: 16s - loss: 0.6402 - accuracy: 0.9701\n",
      "Batch 00607: setting learning rate to 0.03883897827555574.\n",
      " 896/9600 [=>............................] - ETA: 16s - loss: 0.6358 - accuracy: 0.9699\n",
      "Batch 00608: setting learning rate to 0.03876418255633502.\n",
      "1024/9600 [==>...........................] - ETA: 15s - loss: 0.6302 - accuracy: 0.9727\n",
      "Batch 00609: setting learning rate to 0.038689386837114294.\n",
      "1152/9600 [==>...........................] - ETA: 15s - loss: 0.6292 - accuracy: 0.9731\n",
      "Batch 00610: setting learning rate to 0.03861459111789357.\n",
      "1280/9600 [===>..........................] - ETA: 15s - loss: 0.6298 - accuracy: 0.9719\n",
      "Batch 00611: setting learning rate to 0.03853979539867284.\n",
      "1408/9600 [===>..........................] - ETA: 15s - loss: 0.6307 - accuracy: 0.9723\n",
      "Batch 00612: setting learning rate to 0.038464999679452126.\n",
      "1536/9600 [===>..........................] - ETA: 14s - loss: 0.6302 - accuracy: 0.9727\n",
      "Batch 00613: setting learning rate to 0.0383902039602314.\n",
      "1664/9600 [====>.........................] - ETA: 14s - loss: 0.6307 - accuracy: 0.9724\n",
      "Batch 00614: setting learning rate to 0.03831540824101067.\n",
      "1792/9600 [====>.........................] - ETA: 14s - loss: 0.6281 - accuracy: 0.9732\n",
      "Batch 00615: setting learning rate to 0.038240612521789945.\n",
      "1920/9600 [=====>........................] - ETA: 14s - loss: 0.6276 - accuracy: 0.9734\n",
      "Batch 00616: setting learning rate to 0.03816581680256923.\n",
      "2048/9600 [=====>........................] - ETA: 13s - loss: 0.6295 - accuracy: 0.9736\n",
      "Batch 00617: setting learning rate to 0.038091021083348504.\n",
      "2176/9600 [=====>........................] - ETA: 13s - loss: 0.6294 - accuracy: 0.9733\n",
      "Batch 00618: setting learning rate to 0.03801622536412778.\n",
      "2304/9600 [======>.......................] - ETA: 13s - loss: 0.6328 - accuracy: 0.9705\n",
      "Batch 00619: setting learning rate to 0.03794142964490705.\n",
      "2432/9600 [======>.......................] - ETA: 13s - loss: 0.6307 - accuracy: 0.9708\n",
      "Batch 00620: setting learning rate to 0.03786663392568633.\n",
      "2560/9600 [=======>......................] - ETA: 12s - loss: 0.6293 - accuracy: 0.9707\n",
      "Batch 00621: setting learning rate to 0.03779183820646561.\n",
      "2688/9600 [=======>......................] - ETA: 12s - loss: 0.6315 - accuracy: 0.9699\n",
      "Batch 00622: setting learning rate to 0.03771704248724488.\n",
      "2816/9600 [=======>......................] - ETA: 12s - loss: 0.6323 - accuracy: 0.9695\n",
      "Batch 00623: setting learning rate to 0.037642246768024155.\n",
      "2944/9600 [========>.....................] - ETA: 12s - loss: 0.6305 - accuracy: 0.9701\n",
      "Batch 00624: setting learning rate to 0.03756745104880343.\n",
      "3072/9600 [========>.....................] - ETA: 12s - loss: 0.6301 - accuracy: 0.9697\n",
      "Batch 00625: setting learning rate to 0.037492655329582715.\n",
      "3200/9600 [=========>....................] - ETA: 11s - loss: 0.6293 - accuracy: 0.9706\n",
      "Batch 00626: setting learning rate to 0.03741785961036199.\n",
      "3328/9600 [=========>....................] - ETA: 11s - loss: 0.6297 - accuracy: 0.9703\n",
      "Batch 00627: setting learning rate to 0.03734306389114126.\n",
      "3456/9600 [=========>....................] - ETA: 11s - loss: 0.6282 - accuracy: 0.9711\n",
      "Batch 00628: setting learning rate to 0.037268268171920534.\n",
      "3584/9600 [==========>...................] - ETA: 11s - loss: 0.6297 - accuracy: 0.9699\n",
      "Batch 00629: setting learning rate to 0.03719347245269982.\n",
      "3712/9600 [==========>...................] - ETA: 10s - loss: 0.6302 - accuracy: 0.9690\n",
      "Batch 00630: setting learning rate to 0.03711867673347909.\n",
      "3840/9600 [===========>..................] - ETA: 10s - loss: 0.6283 - accuracy: 0.9698\n",
      "Batch 00631: setting learning rate to 0.037043881014258366.\n",
      "3968/9600 [===========>..................] - ETA: 10s - loss: 0.6283 - accuracy: 0.9693\n",
      "Batch 00632: setting learning rate to 0.03696908529503764.\n",
      "4096/9600 [===========>..................] - ETA: 10s - loss: 0.6282 - accuracy: 0.9690\n",
      "Batch 00633: setting learning rate to 0.03689428957581692.\n",
      "4224/9600 [============>.................] - ETA: 9s - loss: 0.6268 - accuracy: 0.9697 \n",
      "Batch 00634: setting learning rate to 0.0368194938565962.\n",
      "4352/9600 [============>.................] - ETA: 9s - loss: 0.6260 - accuracy: 0.9701\n",
      "Batch 00635: setting learning rate to 0.03674469813737547.\n",
      "4480/9600 [=============>................] - ETA: 9s - loss: 0.6250 - accuracy: 0.9705\n",
      "Batch 00636: setting learning rate to 0.036669902418154744.\n",
      "4608/9600 [=============>................] - ETA: 9s - loss: 0.6246 - accuracy: 0.9707\n",
      "Batch 00637: setting learning rate to 0.03659510669893402.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.6230 - accuracy: 0.9713\n",
      "Batch 00638: setting learning rate to 0.036520310979713304.\n",
      "4864/9600 [==============>...............] - ETA: 8s - loss: 0.6231 - accuracy: 0.9712\n",
      "Batch 00639: setting learning rate to 0.03644551526049258.\n",
      "4992/9600 [==============>...............] - ETA: 8s - loss: 0.6238 - accuracy: 0.9706\n",
      "Batch 00640: setting learning rate to 0.03637071954127185.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 0.6232 - accuracy: 0.9707\n",
      "Batch 00641: setting learning rate to 0.03629592382205112.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.6222 - accuracy: 0.9714\n",
      "Batch 00642: setting learning rate to 0.0362211281028304.\n",
      "5376/9600 [===============>..............] - ETA: 7s - loss: 0.6221 - accuracy: 0.9715\n",
      "Batch 00643: setting learning rate to 0.03614633238360968.\n",
      "5504/9600 [================>.............] - ETA: 7s - loss: 0.6218 - accuracy: 0.9717\n",
      "Batch 00644: setting learning rate to 0.036071536664388955.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 0.6218 - accuracy: 0.9718\n",
      "Batch 00645: setting learning rate to 0.03599674094516823.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.6216 - accuracy: 0.9715\n",
      "Batch 00646: setting learning rate to 0.03592194522594751.\n",
      "5888/9600 [=================>............] - ETA: 6s - loss: 0.6207 - accuracy: 0.9720\n",
      "Batch 00647: setting learning rate to 0.03584714950672679.\n",
      "6016/9600 [=================>............] - ETA: 6s - loss: 0.6202 - accuracy: 0.9722\n",
      "Batch 00648: setting learning rate to 0.03577235378750606.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.6194 - accuracy: 0.9725\n",
      "Batch 00649: setting learning rate to 0.03569755806828533.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.6182 - accuracy: 0.9729\n",
      "Batch 00650: setting learning rate to 0.035622762349064606.\n",
      "6400/9600 [===================>..........] - ETA: 5s - loss: 0.6187 - accuracy: 0.9727\n",
      "Batch 00651: setting learning rate to 0.03554796662984389.\n",
      "6528/9600 [===================>..........] - ETA: 5s - loss: 0.6181 - accuracy: 0.9727\n",
      "Batch 00652: setting learning rate to 0.035473170910623165.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.6180 - accuracy: 0.9725\n",
      "Batch 00653: setting learning rate to 0.03539837519140244.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.6173 - accuracy: 0.9726\n",
      "Batch 00654: setting learning rate to 0.03532357947218171.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.6176 - accuracy: 0.9722\n",
      "Batch 00655: setting learning rate to 0.03524878375296099.\n",
      "7040/9600 [=====================>........] - ETA: 4s - loss: 0.6186 - accuracy: 0.9716\n",
      "Batch 00656: setting learning rate to 0.03517398803374027.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.6187 - accuracy: 0.9717\n",
      "Batch 00657: setting learning rate to 0.035099192314519544.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.6182 - accuracy: 0.9718\n",
      "Batch 00658: setting learning rate to 0.035024396595298816.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.6187 - accuracy: 0.9714\n",
      "Batch 00659: setting learning rate to 0.034949600876078096.\n",
      "7552/9600 [======================>.......] - ETA: 3s - loss: 0.6186 - accuracy: 0.9715\n",
      "Batch 00660: setting learning rate to 0.034874805156857376.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.6187 - accuracy: 0.9715\n",
      "Batch 00661: setting learning rate to 0.03480000943763665.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.6184 - accuracy: 0.9714\n",
      "Batch 00662: setting learning rate to 0.03472521371841592.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.6185 - accuracy: 0.9710\n",
      "Batch 00663: setting learning rate to 0.034650417999195195.\n",
      "8064/9600 [========================>.....] - ETA: 2s - loss: 0.6188 - accuracy: 0.9711\n",
      "Batch 00664: setting learning rate to 0.03457562227997448.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.6197 - accuracy: 0.9709\n",
      "Batch 00665: setting learning rate to 0.034500826560753754.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.6197 - accuracy: 0.9709\n",
      "Batch 00666: setting learning rate to 0.03442603084153303.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.6199 - accuracy: 0.9711\n",
      "Batch 00667: setting learning rate to 0.0343512351223123.\n",
      "8576/9600 [=========================>....] - ETA: 1s - loss: 0.6201 - accuracy: 0.9711\n",
      "Batch 00668: setting learning rate to 0.03427643940309158.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.6202 - accuracy: 0.9710\n",
      "Batch 00669: setting learning rate to 0.03420164368387086.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.6202 - accuracy: 0.9711\n",
      "Batch 00670: setting learning rate to 0.03412684796465013.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.6207 - accuracy: 0.9708\n",
      "Batch 00671: setting learning rate to 0.034052052245429405.\n",
      "9088/9600 [===========================>..] - ETA: 0s - loss: 0.6203 - accuracy: 0.9708\n",
      "Batch 00672: setting learning rate to 0.033977256526208685.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.6203 - accuracy: 0.9709\n",
      "Batch 00673: setting learning rate to 0.033902460806987965.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.6198 - accuracy: 0.9712\n",
      "Batch 00674: setting learning rate to 0.03382766508776724.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.6198 - accuracy: 0.9714\n",
      "Batch 00675: setting learning rate to 0.03375286936854651.\n",
      "9600/9600 [==============================] - 19s 2ms/step - loss: 0.6200 - accuracy: 0.9712 - val_loss: 0.2303 - val_accuracy: 0.9625\n",
      "Epoch 10/15\n",
      "\n",
      "Batch 00676: setting learning rate to 0.03367807364932578.\n",
      " 128/9600 [..............................] - ETA: 18s - loss: 0.6041 - accuracy: 0.9844\n",
      "Batch 00677: setting learning rate to 0.03360327793010506.\n",
      " 256/9600 [..............................] - ETA: 18s - loss: 0.6079 - accuracy: 0.9766\n",
      "Batch 00678: setting learning rate to 0.03352848221088434.\n",
      " 384/9600 [>.............................] - ETA: 18s - loss: 0.6000 - accuracy: 0.9844\n",
      "Batch 00679: setting learning rate to 0.033453686491663616.\n",
      " 512/9600 [>.............................] - ETA: 18s - loss: 0.5985 - accuracy: 0.9824\n",
      "Batch 00680: setting learning rate to 0.03337889077244289.\n",
      " 640/9600 [=>............................] - ETA: 17s - loss: 0.5971 - accuracy: 0.9844\n",
      "Batch 00681: setting learning rate to 0.03330409505322217.\n",
      " 768/9600 [=>............................] - ETA: 17s - loss: 0.5950 - accuracy: 0.9844\n",
      "Batch 00682: setting learning rate to 0.03322929933400145.\n",
      " 896/9600 [=>............................] - ETA: 17s - loss: 0.5966 - accuracy: 0.9844\n",
      "Batch 00683: setting learning rate to 0.03315450361478072.\n",
      "1024/9600 [==>...........................] - ETA: 16s - loss: 0.5996 - accuracy: 0.9805\n",
      "Batch 00684: setting learning rate to 0.033079707895559994.\n",
      "1152/9600 [==>...........................] - ETA: 16s - loss: 0.5975 - accuracy: 0.9818\n",
      "Batch 00685: setting learning rate to 0.033004912176339274.\n",
      "1280/9600 [===>..........................] - ETA: 16s - loss: 0.5972 - accuracy: 0.9805\n",
      "Batch 00686: setting learning rate to 0.032930116457118554.\n",
      "1408/9600 [===>..........................] - ETA: 16s - loss: 0.6006 - accuracy: 0.9780\n",
      "Batch 00687: setting learning rate to 0.03285532073789783.\n",
      "1536/9600 [===>..........................] - ETA: 15s - loss: 0.5999 - accuracy: 0.9785\n",
      "Batch 00688: setting learning rate to 0.0327805250186771.\n",
      "1664/9600 [====>.........................] - ETA: 15s - loss: 0.6006 - accuracy: 0.9784\n",
      "Batch 00689: setting learning rate to 0.03270572929945637.\n",
      "1792/9600 [====>.........................] - ETA: 15s - loss: 0.6009 - accuracy: 0.9782\n",
      "Batch 00690: setting learning rate to 0.03263093358023565.\n",
      "1920/9600 [=====>........................] - ETA: 15s - loss: 0.6010 - accuracy: 0.9792\n",
      "Batch 00691: setting learning rate to 0.03255613786101493.\n",
      "2048/9600 [=====>........................] - ETA: 14s - loss: 0.6009 - accuracy: 0.9790\n",
      "Batch 00692: setting learning rate to 0.032481342141794205.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 0.5994 - accuracy: 0.9802\n",
      "Batch 00693: setting learning rate to 0.03240654642257348.\n",
      "2304/9600 [======>.......................] - ETA: 14s - loss: 0.6002 - accuracy: 0.9800\n",
      "Batch 00694: setting learning rate to 0.03233175070335276.\n",
      "2432/9600 [======>.......................] - ETA: 14s - loss: 0.6021 - accuracy: 0.9790\n",
      "Batch 00695: setting learning rate to 0.03225695498413204.\n",
      "2560/9600 [=======>......................] - ETA: 13s - loss: 0.6025 - accuracy: 0.9785\n",
      "Batch 00696: setting learning rate to 0.03218215926491131.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 0.6023 - accuracy: 0.9784\n",
      "Batch 00697: setting learning rate to 0.03210736354569058.\n",
      "2816/9600 [=======>......................] - ETA: 13s - loss: 0.6039 - accuracy: 0.9776\n",
      "Batch 00698: setting learning rate to 0.03203256782646986.\n",
      "2944/9600 [========>.....................] - ETA: 13s - loss: 0.6041 - accuracy: 0.9772\n",
      "Batch 00699: setting learning rate to 0.03195777210724914.\n",
      "3072/9600 [========>.....................] - ETA: 12s - loss: 0.6046 - accuracy: 0.9766\n",
      "Batch 00700: setting learning rate to 0.031882976388028415.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 0.6047 - accuracy: 0.9762\n",
      "Batch 00701: setting learning rate to 0.03180818066880769.\n",
      "3328/9600 [=========>....................] - ETA: 12s - loss: 0.6040 - accuracy: 0.9769\n",
      "Batch 00702: setting learning rate to 0.03173338494958696.\n",
      "3456/9600 [=========>....................] - ETA: 12s - loss: 0.6063 - accuracy: 0.9748\n",
      "Batch 00703: setting learning rate to 0.03165858923036624.\n",
      "3584/9600 [==========>...................] - ETA: 11s - loss: 0.6062 - accuracy: 0.9749\n",
      "Batch 00704: setting learning rate to 0.03158379351114552.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 0.6062 - accuracy: 0.9747\n",
      "Batch 00705: setting learning rate to 0.031508997791924793.\n",
      "3840/9600 [===========>..................] - ETA: 11s - loss: 0.6058 - accuracy: 0.9753\n",
      "Batch 00706: setting learning rate to 0.031434202072704066.\n",
      "3968/9600 [===========>..................] - ETA: 11s - loss: 0.6057 - accuracy: 0.9756\n",
      "Batch 00707: setting learning rate to 0.031359406353483346.\n",
      "4096/9600 [===========>..................] - ETA: 10s - loss: 0.6051 - accuracy: 0.9758\n",
      "Batch 00708: setting learning rate to 0.031284610634262626.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 0.6060 - accuracy: 0.9756\n",
      "Batch 00709: setting learning rate to 0.0312098149150419.\n",
      "4352/9600 [============>.................] - ETA: 10s - loss: 0.6062 - accuracy: 0.9756\n",
      "Batch 00710: setting learning rate to 0.031135019195821175.\n",
      "4480/9600 [=============>................] - ETA: 10s - loss: 0.6068 - accuracy: 0.9754\n",
      "Batch 00711: setting learning rate to 0.031060223476600448.\n",
      "4608/9600 [=============>................] - ETA: 9s - loss: 0.6057 - accuracy: 0.9761 \n",
      "Batch 00712: setting learning rate to 0.030985427757379724.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.6064 - accuracy: 0.9757\n",
      "Batch 00713: setting learning rate to 0.030910632038159004.\n",
      "4864/9600 [==============>...............] - ETA: 9s - loss: 0.6061 - accuracy: 0.9755\n",
      "Batch 00714: setting learning rate to 0.030835836318938277.\n",
      "4992/9600 [==============>...............] - ETA: 9s - loss: 0.6059 - accuracy: 0.9756\n",
      "Batch 00715: setting learning rate to 0.030761040599717553.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 0.6057 - accuracy: 0.9754\n",
      "Batch 00716: setting learning rate to 0.030686244880496826.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.6050 - accuracy: 0.9756\n",
      "Batch 00717: setting learning rate to 0.03061144916127611.\n",
      "5376/9600 [===============>..............] - ETA: 8s - loss: 0.6050 - accuracy: 0.9758\n",
      "Batch 00718: setting learning rate to 0.030536653442055382.\n",
      "5504/9600 [================>.............] - ETA: 8s - loss: 0.6057 - accuracy: 0.9758\n",
      "Batch 00719: setting learning rate to 0.03046185772283466.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 0.6053 - accuracy: 0.9760\n",
      "Batch 00720: setting learning rate to 0.03038706200361393.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.6053 - accuracy: 0.9759\n",
      "Batch 00721: setting learning rate to 0.030312266284393215.\n",
      "5888/9600 [=================>............] - ETA: 7s - loss: 0.6046 - accuracy: 0.9762\n",
      "Batch 00722: setting learning rate to 0.030237470565172488.\n",
      "6016/9600 [=================>............] - ETA: 7s - loss: 0.6045 - accuracy: 0.9762\n",
      "Batch 00723: setting learning rate to 0.030162674845951764.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.6054 - accuracy: 0.9759\n",
      "Batch 00724: setting learning rate to 0.030087879126731037.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.6053 - accuracy: 0.9759\n",
      "Batch 00725: setting learning rate to 0.030013083407510313.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 0.6050 - accuracy: 0.9758\n",
      "Batch 00726: setting learning rate to 0.029938287688289593.\n",
      "6528/9600 [===================>..........] - ETA: 6s - loss: 0.6049 - accuracy: 0.9759\n",
      "Batch 00727: setting learning rate to 0.029863491969068866.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.6059 - accuracy: 0.9754\n",
      "Batch 00728: setting learning rate to 0.029788696249848142.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.6061 - accuracy: 0.9751\n",
      "Batch 00729: setting learning rate to 0.029713900530627415.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.6061 - accuracy: 0.9753\n",
      "Batch 00730: setting learning rate to 0.029639104811406698.\n",
      "7040/9600 [=====================>........] - ETA: 5s - loss: 0.6057 - accuracy: 0.9756\n",
      "Batch 00731: setting learning rate to 0.02956430909218597.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.6059 - accuracy: 0.9753\n",
      "Batch 00732: setting learning rate to 0.029489513372965247.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.6068 - accuracy: 0.9751\n",
      "Batch 00733: setting learning rate to 0.02941471765374452.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.6072 - accuracy: 0.9749\n",
      "Batch 00734: setting learning rate to 0.029339921934523804.\n",
      "7552/9600 [======================>.......] - ETA: 4s - loss: 0.6067 - accuracy: 0.9751\n",
      "Batch 00735: setting learning rate to 0.029265126215303076.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.6063 - accuracy: 0.9753\n",
      "Batch 00736: setting learning rate to 0.029190330496082353.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.6071 - accuracy: 0.9750\n",
      "Batch 00737: setting learning rate to 0.029115534776861626.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.6072 - accuracy: 0.9751\n",
      "Batch 00738: setting learning rate to 0.029040739057640902.\n",
      "8064/9600 [========================>.....] - ETA: 3s - loss: 0.6083 - accuracy: 0.9745\n",
      "Batch 00739: setting learning rate to 0.02896594333842018.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.6085 - accuracy: 0.9744\n",
      "Batch 00740: setting learning rate to 0.028891147619199455.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.6085 - accuracy: 0.9743\n",
      "Batch 00741: setting learning rate to 0.02881635189997873.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.6087 - accuracy: 0.9740\n",
      "Batch 00742: setting learning rate to 0.028741556180758004.\n",
      "8576/9600 [=========================>....] - ETA: 2s - loss: 0.6087 - accuracy: 0.9738\n",
      "Batch 00743: setting learning rate to 0.028666760461537287.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.6086 - accuracy: 0.9738\n",
      "Batch 00744: setting learning rate to 0.02859196474231656.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.6089 - accuracy: 0.9736\n",
      "Batch 00745: setting learning rate to 0.028517169023095836.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.6089 - accuracy: 0.9738\n",
      "Batch 00746: setting learning rate to 0.02844237330387511.\n",
      "9088/9600 [===========================>..] - ETA: 1s - loss: 0.6092 - accuracy: 0.9736\n",
      "Batch 00747: setting learning rate to 0.028367577584654385.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.6093 - accuracy: 0.9734\n",
      "Batch 00748: setting learning rate to 0.028292781865433665.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.6098 - accuracy: 0.9732\n",
      "Batch 00749: setting learning rate to 0.02821798614621294.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.6099 - accuracy: 0.9730\n",
      "Batch 00750: setting learning rate to 0.028143190426992214.\n",
      "9600/9600 [==============================] - 20s 2ms/step - loss: 0.6100 - accuracy: 0.9729 - val_loss: 0.1963 - val_accuracy: 0.9717\n",
      "Epoch 11/15\n",
      "\n",
      "Batch 00751: setting learning rate to 0.02806839470777149.\n",
      " 128/9600 [..............................] - ETA: 20s - loss: 0.5996 - accuracy: 0.9844\n",
      "Batch 00752: setting learning rate to 0.02799359898855077.\n",
      " 256/9600 [..............................] - ETA: 19s - loss: 0.5828 - accuracy: 0.9883\n",
      "Batch 00753: setting learning rate to 0.027918803269330043.\n",
      " 384/9600 [>.............................] - ETA: 18s - loss: 0.5975 - accuracy: 0.9792\n",
      "Batch 00754: setting learning rate to 0.02784400755010932.\n",
      " 512/9600 [>.............................] - ETA: 18s - loss: 0.5989 - accuracy: 0.9785\n",
      "Batch 00755: setting learning rate to 0.027769211830888593.\n",
      " 640/9600 [=>............................] - ETA: 18s - loss: 0.5939 - accuracy: 0.9812\n",
      "Batch 00756: setting learning rate to 0.027694416111667876.\n",
      " 768/9600 [=>............................] - ETA: 17s - loss: 0.5972 - accuracy: 0.9792\n",
      "Batch 00757: setting learning rate to 0.02761962039244715.\n",
      " 896/9600 [=>............................] - ETA: 17s - loss: 0.5934 - accuracy: 0.9799\n",
      "Batch 00758: setting learning rate to 0.027544824673226425.\n",
      "1024/9600 [==>...........................] - ETA: 17s - loss: 0.5930 - accuracy: 0.9795\n",
      "Batch 00759: setting learning rate to 0.027470028954005698.\n",
      "1152/9600 [==>...........................] - ETA: 16s - loss: 0.5964 - accuracy: 0.9774\n",
      "Batch 00760: setting learning rate to 0.027395233234784974.\n",
      "1280/9600 [===>..........................] - ETA: 16s - loss: 0.5969 - accuracy: 0.9766\n",
      "Batch 00761: setting learning rate to 0.027320437515564254.\n",
      "1408/9600 [===>..........................] - ETA: 16s - loss: 0.5941 - accuracy: 0.9787\n",
      "Batch 00762: setting learning rate to 0.02724564179634353.\n",
      "1536/9600 [===>..........................] - ETA: 15s - loss: 0.5940 - accuracy: 0.9792\n",
      "Batch 00763: setting learning rate to 0.027170846077122803.\n",
      "1664/9600 [====>.........................] - ETA: 15s - loss: 0.5950 - accuracy: 0.9790\n",
      "Batch 00764: setting learning rate to 0.02709605035790208.\n",
      "1792/9600 [====>.........................] - ETA: 15s - loss: 0.5958 - accuracy: 0.9788\n",
      "Batch 00765: setting learning rate to 0.02702125463868136.\n",
      "1920/9600 [=====>........................] - ETA: 15s - loss: 0.5953 - accuracy: 0.9781\n",
      "Batch 00766: setting learning rate to 0.026946458919460632.\n",
      "2048/9600 [=====>........................] - ETA: 14s - loss: 0.5948 - accuracy: 0.9790\n",
      "Batch 00767: setting learning rate to 0.02687166320023991.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 0.5957 - accuracy: 0.9784\n",
      "Batch 00768: setting learning rate to 0.02679686748101918.\n",
      "2304/9600 [======>.......................] - ETA: 14s - loss: 0.5942 - accuracy: 0.9792\n",
      "Batch 00769: setting learning rate to 0.026722071761798465.\n",
      "2432/9600 [======>.......................] - ETA: 14s - loss: 0.5947 - accuracy: 0.9786\n",
      "Batch 00770: setting learning rate to 0.026647276042577737.\n",
      "2560/9600 [=======>......................] - ETA: 13s - loss: 0.5941 - accuracy: 0.9785\n",
      "Batch 00771: setting learning rate to 0.026572480323357014.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 0.5963 - accuracy: 0.9777\n",
      "Batch 00772: setting learning rate to 0.026497684604136287.\n",
      "2816/9600 [=======>......................] - ETA: 13s - loss: 0.5953 - accuracy: 0.9783\n",
      "Batch 00773: setting learning rate to 0.026422888884915563.\n",
      "2944/9600 [========>.....................] - ETA: 13s - loss: 0.5960 - accuracy: 0.9783\n",
      "Batch 00774: setting learning rate to 0.026348093165694843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072/9600 [========>.....................] - ETA: 12s - loss: 0.5972 - accuracy: 0.9775\n",
      "Batch 00775: setting learning rate to 0.02627329744647412.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 0.5968 - accuracy: 0.9781\n",
      "Batch 00776: setting learning rate to 0.026198501727253392.\n",
      "3328/9600 [=========>....................] - ETA: 12s - loss: 0.5956 - accuracy: 0.9787\n",
      "Batch 00777: setting learning rate to 0.02612370600803267.\n",
      "3456/9600 [=========>....................] - ETA: 11s - loss: 0.5971 - accuracy: 0.9780\n",
      "Batch 00778: setting learning rate to 0.026048910288811948.\n",
      "3584/9600 [==========>...................] - ETA: 11s - loss: 0.5986 - accuracy: 0.9774\n",
      "Batch 00779: setting learning rate to 0.02597411456959122.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 0.5979 - accuracy: 0.9779\n",
      "Batch 00780: setting learning rate to 0.025899318850370497.\n",
      "3840/9600 [===========>..................] - ETA: 11s - loss: 0.5976 - accuracy: 0.9781\n",
      "Batch 00781: setting learning rate to 0.02582452313114977.\n",
      "3968/9600 [===========>..................] - ETA: 10s - loss: 0.5967 - accuracy: 0.9783\n",
      "Batch 00782: setting learning rate to 0.025749727411929046.\n",
      "4096/9600 [===========>..................] - ETA: 10s - loss: 0.5966 - accuracy: 0.9780\n",
      "Batch 00783: setting learning rate to 0.025674931692708326.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 0.5971 - accuracy: 0.9775\n",
      "Batch 00784: setting learning rate to 0.025600135973487603.\n",
      "4352/9600 [============>.................] - ETA: 10s - loss: 0.5980 - accuracy: 0.9770\n",
      "Batch 00785: setting learning rate to 0.025525340254266875.\n",
      "4480/9600 [=============>................] - ETA: 9s - loss: 0.5976 - accuracy: 0.9775 \n",
      "Batch 00786: setting learning rate to 0.025450544535046152.\n",
      "4608/9600 [=============>................] - ETA: 9s - loss: 0.5999 - accuracy: 0.9763\n",
      "Batch 00787: setting learning rate to 0.02537574881582543.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.5998 - accuracy: 0.9761\n",
      "Batch 00788: setting learning rate to 0.025300953096604708.\n",
      "4864/9600 [==============>...............] - ETA: 9s - loss: 0.5998 - accuracy: 0.9762\n",
      "Batch 00789: setting learning rate to 0.02522615737738398.\n",
      "4992/9600 [==============>...............] - ETA: 9s - loss: 0.5994 - accuracy: 0.9764\n",
      "Batch 00790: setting learning rate to 0.025151361658163254.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 0.6001 - accuracy: 0.9760\n",
      "Batch 00791: setting learning rate to 0.025076565938942537.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.6001 - accuracy: 0.9760\n",
      "Batch 00792: setting learning rate to 0.02500177021972181.\n",
      "5376/9600 [===============>..............] - ETA: 8s - loss: 0.6012 - accuracy: 0.9754\n",
      "Batch 00793: setting learning rate to 0.024926974500501086.\n",
      "5504/9600 [================>.............] - ETA: 8s - loss: 0.6016 - accuracy: 0.9751\n",
      "Batch 00794: setting learning rate to 0.02485217878128036.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 0.6012 - accuracy: 0.9753\n",
      "Batch 00795: setting learning rate to 0.024777383062059635.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.6010 - accuracy: 0.9757\n",
      "Batch 00796: setting learning rate to 0.024702587342838915.\n",
      "5888/9600 [=================>............] - ETA: 7s - loss: 0.6013 - accuracy: 0.9759\n",
      "Batch 00797: setting learning rate to 0.02462779162361819.\n",
      "6016/9600 [=================>............] - ETA: 7s - loss: 0.6011 - accuracy: 0.9761\n",
      "Batch 00798: setting learning rate to 0.024552995904397464.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.6007 - accuracy: 0.9764\n",
      "Batch 00799: setting learning rate to 0.02447820018517674.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.6004 - accuracy: 0.9766\n",
      "Batch 00800: setting learning rate to 0.02440340446595602.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 0.6002 - accuracy: 0.9766\n",
      "Batch 00801: setting learning rate to 0.024328608746735297.\n",
      "6528/9600 [===================>..........] - ETA: 6s - loss: 0.5997 - accuracy: 0.9767\n",
      "Batch 00802: setting learning rate to 0.02425381302751457.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.5998 - accuracy: 0.9769\n",
      "Batch 00803: setting learning rate to 0.024179017308293842.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.5993 - accuracy: 0.9772\n",
      "Batch 00804: setting learning rate to 0.02410422158907312.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.5991 - accuracy: 0.9771\n",
      "Batch 00805: setting learning rate to 0.0240294258698524.\n",
      "7040/9600 [=====================>........] - ETA: 5s - loss: 0.5991 - accuracy: 0.9773\n",
      "Batch 00806: setting learning rate to 0.023954630150631675.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.5987 - accuracy: 0.9774\n",
      "Batch 00807: setting learning rate to 0.023879834431410948.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.5990 - accuracy: 0.9774\n",
      "Batch 00808: setting learning rate to 0.023805038712190224.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.5993 - accuracy: 0.9772\n",
      "Batch 00809: setting learning rate to 0.023730242992969504.\n",
      "7552/9600 [======================>.......] - ETA: 4s - loss: 0.5998 - accuracy: 0.9770\n",
      "Batch 00810: setting learning rate to 0.02365544727374878.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.5997 - accuracy: 0.9771\n",
      "Batch 00811: setting learning rate to 0.023580651554528053.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.5995 - accuracy: 0.9772\n",
      "Batch 00812: setting learning rate to 0.02350585583530733.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.5996 - accuracy: 0.9768\n",
      "Batch 00813: setting learning rate to 0.02343106011608661.\n",
      "8064/9600 [========================>.....] - ETA: 3s - loss: 0.6002 - accuracy: 0.9764\n",
      "Batch 00814: setting learning rate to 0.023356264396865885.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.6002 - accuracy: 0.9763\n",
      "Batch 00815: setting learning rate to 0.02328146867764516.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.6003 - accuracy: 0.9761\n",
      "Batch 00816: setting learning rate to 0.02320667295842443.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.6009 - accuracy: 0.9756\n",
      "Batch 00817: setting learning rate to 0.023131877239203708.\n",
      "8576/9600 [=========================>....] - ETA: 2s - loss: 0.6006 - accuracy: 0.9756\n",
      "Batch 00818: setting learning rate to 0.02305708151998298.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.6010 - accuracy: 0.9755\n",
      "Batch 00819: setting learning rate to 0.022982285800762257.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.6012 - accuracy: 0.9757\n",
      "Batch 00820: setting learning rate to 0.022907490081541543.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.6014 - accuracy: 0.9757\n",
      "Batch 00821: setting learning rate to 0.02283269436232082.\n",
      "9088/9600 [===========================>..] - ETA: 1s - loss: 0.6010 - accuracy: 0.9759\n",
      "Batch 00822: setting learning rate to 0.022757898643100093.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.6005 - accuracy: 0.9761\n",
      "Batch 00823: setting learning rate to 0.02268310292387937.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.6003 - accuracy: 0.9761\n",
      "Batch 00824: setting learning rate to 0.022608307204658642.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.6001 - accuracy: 0.9764\n",
      "Batch 00825: setting learning rate to 0.022533511485437918.\n",
      "9600/9600 [==============================] - 20s 2ms/step - loss: 0.5999 - accuracy: 0.9765 - val_loss: 0.2018 - val_accuracy: 0.9712\n",
      "Epoch 12/15\n",
      "\n",
      "Batch 00826: setting learning rate to 0.02245871576621719.\n",
      " 128/9600 [..............................] - ETA: 18s - loss: 0.5736 - accuracy: 0.9922\n",
      "Batch 00827: setting learning rate to 0.022383920046996467.\n",
      " 256/9600 [..............................] - ETA: 18s - loss: 0.6019 - accuracy: 0.9766\n",
      "Batch 00828: setting learning rate to 0.02230912432777574.\n",
      " 384/9600 [>.............................] - ETA: 17s - loss: 0.6177 - accuracy: 0.9688\n",
      "Batch 00829: setting learning rate to 0.022234328608555027.\n",
      " 512/9600 [>.............................] - ETA: 17s - loss: 0.6123 - accuracy: 0.9688\n",
      "Batch 00830: setting learning rate to 0.022159532889334303.\n",
      " 640/9600 [=>............................] - ETA: 17s - loss: 0.6134 - accuracy: 0.9703\n",
      "Batch 00831: setting learning rate to 0.022084737170113576.\n",
      " 768/9600 [=>............................] - ETA: 17s - loss: 0.6073 - accuracy: 0.9727\n",
      "Batch 00832: setting learning rate to 0.022009941450892852.\n",
      " 896/9600 [=>............................] - ETA: 16s - loss: 0.6062 - accuracy: 0.9721\n",
      "Batch 00833: setting learning rate to 0.021935145731672125.\n",
      "1024/9600 [==>...........................] - ETA: 16s - loss: 0.6053 - accuracy: 0.9727\n",
      "Batch 00834: setting learning rate to 0.0218603500124514.\n",
      "1152/9600 [==>...........................] - ETA: 16s - loss: 0.6036 - accuracy: 0.9748\n",
      "Batch 00835: setting learning rate to 0.021785554293230674.\n",
      "1280/9600 [===>..........................] - ETA: 15s - loss: 0.6012 - accuracy: 0.9766\n",
      "Batch 00836: setting learning rate to 0.02171075857400995.\n",
      "1408/9600 [===>..........................] - ETA: 15s - loss: 0.5992 - accuracy: 0.9766\n",
      "Batch 00837: setting learning rate to 0.021635962854789238.\n",
      "1536/9600 [===>..........................] - ETA: 15s - loss: 0.5999 - accuracy: 0.9753\n",
      "Batch 00838: setting learning rate to 0.021561167135568514.\n",
      "1664/9600 [====>.........................] - ETA: 15s - loss: 0.5979 - accuracy: 0.9766\n",
      "Batch 00839: setting learning rate to 0.021486371416347787.\n",
      "1792/9600 [====>.........................] - ETA: 14s - loss: 0.5981 - accuracy: 0.9777\n",
      "Batch 00840: setting learning rate to 0.021411575697127063.\n",
      "1920/9600 [=====>........................] - ETA: 14s - loss: 0.5987 - accuracy: 0.9776\n",
      "Batch 00841: setting learning rate to 0.021336779977906336.\n",
      "2048/9600 [=====>........................] - ETA: 14s - loss: 0.5977 - accuracy: 0.9771\n",
      "Batch 00842: setting learning rate to 0.02126198425868561.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 0.5985 - accuracy: 0.9775\n",
      "Batch 00843: setting learning rate to 0.021187188539464885.\n",
      "2304/9600 [======>.......................] - ETA: 13s - loss: 0.5973 - accuracy: 0.9779\n",
      "Batch 00844: setting learning rate to 0.021112392820244158.\n",
      "2432/9600 [======>.......................] - ETA: 13s - loss: 0.5979 - accuracy: 0.9774\n",
      "Batch 00845: setting learning rate to 0.021037597101023434.\n",
      "2560/9600 [=======>......................] - ETA: 13s - loss: 0.5976 - accuracy: 0.9777\n",
      "Batch 00846: setting learning rate to 0.02096280138180272.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 0.5967 - accuracy: 0.9784\n",
      "Batch 00847: setting learning rate to 0.020888005662581997.\n",
      "2816/9600 [=======>......................] - ETA: 13s - loss: 0.5959 - accuracy: 0.9787\n",
      "Batch 00848: setting learning rate to 0.02081320994336127.\n",
      "2944/9600 [========>.....................] - ETA: 12s - loss: 0.5951 - accuracy: 0.9789\n",
      "Batch 00849: setting learning rate to 0.020738414224140547.\n",
      "3072/9600 [========>.....................] - ETA: 12s - loss: 0.5968 - accuracy: 0.9782\n",
      "Batch 00850: setting learning rate to 0.02066361850491982.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 0.5969 - accuracy: 0.9772\n",
      "Batch 00851: setting learning rate to 0.020588822785699096.\n",
      "3328/9600 [=========>....................] - ETA: 12s - loss: 0.5963 - accuracy: 0.9775\n",
      "Batch 00852: setting learning rate to 0.02051402706647837.\n",
      "3456/9600 [=========>....................] - ETA: 11s - loss: 0.5957 - accuracy: 0.9780\n",
      "Batch 00853: setting learning rate to 0.020439231347257645.\n",
      "3584/9600 [==========>...................] - ETA: 11s - loss: 0.5947 - accuracy: 0.9788\n",
      "Batch 00854: setting learning rate to 0.020364435628036918.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 0.5956 - accuracy: 0.9790\n",
      "Batch 00855: setting learning rate to 0.020289639908816205.\n",
      "3840/9600 [===========>..................] - ETA: 11s - loss: 0.5945 - accuracy: 0.9794\n",
      "Batch 00856: setting learning rate to 0.02021484418959548.\n",
      "3968/9600 [===========>..................] - ETA: 10s - loss: 0.5954 - accuracy: 0.9791\n",
      "Batch 00857: setting learning rate to 0.020140048470374754.\n",
      "4096/9600 [===========>..................] - ETA: 10s - loss: 0.5951 - accuracy: 0.9790\n",
      "Batch 00858: setting learning rate to 0.02006525275115403.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 0.5951 - accuracy: 0.9792\n",
      "Batch 00859: setting learning rate to 0.019990457031933303.\n",
      "4352/9600 [============>.................] - ETA: 10s - loss: 0.5950 - accuracy: 0.9791\n",
      "Batch 00860: setting learning rate to 0.01991566131271258.\n",
      "4480/9600 [=============>................] - ETA: 10s - loss: 0.5945 - accuracy: 0.9795\n",
      "Batch 00861: setting learning rate to 0.019840865593491852.\n",
      "4608/9600 [=============>................] - ETA: 9s - loss: 0.5949 - accuracy: 0.9792 \n",
      "Batch 00862: setting learning rate to 0.01976606987427113.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.5950 - accuracy: 0.9791\n",
      "Batch 00863: setting learning rate to 0.0196912741550504.\n",
      "4864/9600 [==============>...............] - ETA: 9s - loss: 0.5961 - accuracy: 0.9786\n",
      "Batch 00864: setting learning rate to 0.01961647843582969.\n",
      "4992/9600 [==============>...............] - ETA: 9s - loss: 0.5970 - accuracy: 0.9784\n",
      "Batch 00865: setting learning rate to 0.019541682716608964.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 0.5960 - accuracy: 0.9787\n",
      "Batch 00866: setting learning rate to 0.01946688699738824.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.5956 - accuracy: 0.9787\n",
      "Batch 00867: setting learning rate to 0.019392091278167514.\n",
      "5376/9600 [===============>..............] - ETA: 8s - loss: 0.5949 - accuracy: 0.9788\n",
      "Batch 00868: setting learning rate to 0.019317295558946786.\n",
      "5504/9600 [================>.............] - ETA: 8s - loss: 0.5946 - accuracy: 0.9787\n",
      "Batch 00869: setting learning rate to 0.019242499839726063.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 0.5944 - accuracy: 0.9789\n",
      "Batch 00870: setting learning rate to 0.019167704120505336.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.5941 - accuracy: 0.9792\n",
      "Batch 00871: setting learning rate to 0.019092908401284612.\n",
      "5888/9600 [=================>............] - ETA: 7s - loss: 0.5945 - accuracy: 0.9793\n",
      "Batch 00872: setting learning rate to 0.0190181126820639.\n",
      "6016/9600 [=================>............] - ETA: 7s - loss: 0.5943 - accuracy: 0.9792\n",
      "Batch 00873: setting learning rate to 0.018943316962843175.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.5938 - accuracy: 0.9795\n",
      "Batch 00874: setting learning rate to 0.018868521243622448.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.5939 - accuracy: 0.9793\n",
      "Batch 00875: setting learning rate to 0.018793725524401724.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 0.5939 - accuracy: 0.9791\n",
      "Batch 00876: setting learning rate to 0.018718929805180997.\n",
      "6528/9600 [===================>..........] - ETA: 6s - loss: 0.5948 - accuracy: 0.9787\n",
      "Batch 00877: setting learning rate to 0.018644134085960273.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.5954 - accuracy: 0.9785\n",
      "Batch 00878: setting learning rate to 0.018569338366739546.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.5956 - accuracy: 0.9783\n",
      "Batch 00879: setting learning rate to 0.018494542647518823.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.5962 - accuracy: 0.9780\n",
      "Batch 00880: setting learning rate to 0.018419746928298095.\n",
      "7040/9600 [=====================>........] - ETA: 5s - loss: 0.5957 - accuracy: 0.9783\n",
      "Batch 00881: setting learning rate to 0.018344951209077382.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.5959 - accuracy: 0.9781\n",
      "Batch 00882: setting learning rate to 0.01827015548985666.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.5960 - accuracy: 0.9779\n",
      "Batch 00883: setting learning rate to 0.01819535977063593.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.5961 - accuracy: 0.9780\n",
      "Batch 00884: setting learning rate to 0.018120564051415208.\n",
      "7552/9600 [======================>.......] - ETA: 4s - loss: 0.5961 - accuracy: 0.9779\n",
      "Batch 00885: setting learning rate to 0.01804576833219448.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.5967 - accuracy: 0.9773\n",
      "Batch 00886: setting learning rate to 0.017970972612973757.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.5964 - accuracy: 0.9776\n",
      "Batch 00887: setting learning rate to 0.01789617689375303.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.5969 - accuracy: 0.9776\n",
      "Batch 00888: setting learning rate to 0.017821381174532306.\n",
      "8064/9600 [========================>.....] - ETA: 3s - loss: 0.5967 - accuracy: 0.9776\n",
      "Batch 00889: setting learning rate to 0.01774658545531158.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.5967 - accuracy: 0.9778\n",
      "Batch 00890: setting learning rate to 0.01767178973609087.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.5966 - accuracy: 0.9780\n",
      "Batch 00891: setting learning rate to 0.017596994016870142.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.5965 - accuracy: 0.9781\n",
      "Batch 00892: setting learning rate to 0.017522198297649415.\n",
      "8576/9600 [=========================>....] - ETA: 2s - loss: 0.5967 - accuracy: 0.9780\n",
      "Batch 00893: setting learning rate to 0.01744740257842869.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.5965 - accuracy: 0.9781\n",
      "Batch 00894: setting learning rate to 0.017372606859207964.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.5964 - accuracy: 0.9781\n",
      "Batch 00895: setting learning rate to 0.01729781113998724.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.5959 - accuracy: 0.9783\n",
      "Batch 00896: setting learning rate to 0.017223015420766513.\n",
      "9088/9600 [===========================>..] - ETA: 1s - loss: 0.5965 - accuracy: 0.9781\n",
      "Batch 00897: setting learning rate to 0.01714821970154579.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.5966 - accuracy: 0.9780\n",
      "Batch 00898: setting learning rate to 0.017073423982325062.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.5969 - accuracy: 0.9778\n",
      "Batch 00899: setting learning rate to 0.016998628263104353.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.5970 - accuracy: 0.9776\n",
      "Batch 00900: setting learning rate to 0.016923832543883625.\n",
      "9600/9600 [==============================] - 20s 2ms/step - loss: 0.5966 - accuracy: 0.9777 - val_loss: 0.1946 - val_accuracy: 0.9758\n",
      "Epoch 13/15\n",
      "\n",
      "Batch 00901: setting learning rate to 0.0168490368246629.\n",
      " 128/9600 [..............................] - ETA: 20s - loss: 0.5923 - accuracy: 0.9766\n",
      "Batch 00902: setting learning rate to 0.016774241105442175.\n",
      " 256/9600 [..............................] - ETA: 19s - loss: 0.5789 - accuracy: 0.9805\n",
      "Batch 00903: setting learning rate to 0.01669944538622145.\n",
      " 384/9600 [>.............................] - ETA: 19s - loss: 0.5821 - accuracy: 0.9818\n",
      "Batch 00904: setting learning rate to 0.016624649667000724.\n",
      " 512/9600 [>.............................] - ETA: 18s - loss: 0.5860 - accuracy: 0.9805\n",
      "Batch 00905: setting learning rate to 0.01654985394778.\n",
      " 640/9600 [=>............................] - ETA: 18s - loss: 0.5856 - accuracy: 0.9828\n",
      "Batch 00906: setting learning rate to 0.016475058228559273.\n",
      " 768/9600 [=>............................] - ETA: 17s - loss: 0.5844 - accuracy: 0.9831\n",
      "Batch 00907: setting learning rate to 0.01640026250933856.\n",
      " 896/9600 [=>............................] - ETA: 17s - loss: 0.5827 - accuracy: 0.9833\n",
      "Batch 00908: setting learning rate to 0.016325466790117836.\n",
      "1024/9600 [==>...........................] - ETA: 17s - loss: 0.5814 - accuracy: 0.9834\n",
      "Batch 00909: setting learning rate to 0.01625067107089711.\n",
      "1152/9600 [==>...........................] - ETA: 17s - loss: 0.5806 - accuracy: 0.9826\n",
      "Batch 00910: setting learning rate to 0.016175875351676385.\n",
      "1280/9600 [===>..........................] - ETA: 16s - loss: 0.5791 - accuracy: 0.9828\n",
      "Batch 00911: setting learning rate to 0.016101079632455658.\n",
      "1408/9600 [===>..........................] - ETA: 16s - loss: 0.5808 - accuracy: 0.9822\n",
      "Batch 00912: setting learning rate to 0.016026283913234934.\n",
      "1536/9600 [===>..........................] - ETA: 16s - loss: 0.5797 - accuracy: 0.9824\n",
      "Batch 00913: setting learning rate to 0.015951488194014207.\n",
      "1664/9600 [====>.........................] - ETA: 16s - loss: 0.5797 - accuracy: 0.9826\n",
      "Batch 00914: setting learning rate to 0.015876692474793484.\n",
      "1792/9600 [====>.........................] - ETA: 15s - loss: 0.5836 - accuracy: 0.9810\n",
      "Batch 00915: setting learning rate to 0.015801896755572756.\n",
      "1920/9600 [=====>........................] - ETA: 15s - loss: 0.5840 - accuracy: 0.9807\n",
      "Batch 00916: setting learning rate to 0.015727101036352047.\n",
      "2048/9600 [=====>........................] - ETA: 15s - loss: 0.5828 - accuracy: 0.9819\n",
      "Batch 00917: setting learning rate to 0.01565230531713132.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 0.5816 - accuracy: 0.9830\n",
      "Batch 00918: setting learning rate to 0.015577509597910594.\n",
      "2304/9600 [======>.......................] - ETA: 14s - loss: 0.5807 - accuracy: 0.9839\n",
      "Batch 00919: setting learning rate to 0.015502713878689869.\n",
      "2432/9600 [======>.......................] - ETA: 14s - loss: 0.5797 - accuracy: 0.9844\n",
      "Batch 00920: setting learning rate to 0.015427918159469143.\n",
      "2560/9600 [=======>......................] - ETA: 14s - loss: 0.5835 - accuracy: 0.9820\n",
      "Batch 00921: setting learning rate to 0.015353122440248418.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 0.5847 - accuracy: 0.9810\n",
      "Batch 00922: setting learning rate to 0.015278326721027692.\n",
      "2816/9600 [=======>......................] - ETA: 13s - loss: 0.5859 - accuracy: 0.9805\n",
      "Batch 00923: setting learning rate to 0.015203531001806967.\n",
      "2944/9600 [========>.....................] - ETA: 13s - loss: 0.5854 - accuracy: 0.9806\n",
      "Batch 00924: setting learning rate to 0.015128735282586242.\n",
      "3072/9600 [========>.....................] - ETA: 13s - loss: 0.5860 - accuracy: 0.9805\n",
      "Batch 00925: setting learning rate to 0.015053939563365528.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 0.5866 - accuracy: 0.9806\n",
      "Batch 00926: setting learning rate to 0.014979143844144803.\n",
      "3328/9600 [=========>....................] - ETA: 12s - loss: 0.5862 - accuracy: 0.9808\n",
      "Batch 00927: setting learning rate to 0.014904348124924078.\n",
      "3456/9600 [=========>....................] - ETA: 12s - loss: 0.5854 - accuracy: 0.9812\n",
      "Batch 00928: setting learning rate to 0.014829552405703352.\n",
      "3584/9600 [==========>...................] - ETA: 12s - loss: 0.5855 - accuracy: 0.9810\n",
      "Batch 00929: setting learning rate to 0.014754756686482627.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 0.5851 - accuracy: 0.9811\n",
      "Batch 00930: setting learning rate to 0.014679960967261901.\n",
      "3840/9600 [===========>..................] - ETA: 11s - loss: 0.5854 - accuracy: 0.9810\n",
      "Batch 00931: setting learning rate to 0.014605165248041176.\n",
      "3968/9600 [===========>..................] - ETA: 11s - loss: 0.5868 - accuracy: 0.9803\n",
      "Batch 00932: setting learning rate to 0.01453036952882045.\n",
      "4096/9600 [===========>..................] - ETA: 11s - loss: 0.5862 - accuracy: 0.9807\n",
      "Batch 00933: setting learning rate to 0.014455573809599725.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 0.5864 - accuracy: 0.9804\n",
      "Batch 00934: setting learning rate to 0.014380778090379014.\n",
      "4352/9600 [============>.................] - ETA: 10s - loss: 0.5861 - accuracy: 0.9802\n",
      "Batch 00935: setting learning rate to 0.014305982371158288.\n",
      "4480/9600 [=============>................] - ETA: 10s - loss: 0.5853 - accuracy: 0.9806\n",
      "Batch 00936: setting learning rate to 0.014231186651937563.\n",
      "4608/9600 [=============>................] - ETA: 10s - loss: 0.5847 - accuracy: 0.9809\n",
      "Batch 00937: setting learning rate to 0.014156390932716837.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.5854 - accuracy: 0.9804 \n",
      "Batch 00938: setting learning rate to 0.01408159521349611.\n",
      "4864/9600 [==============>...............] - ETA: 9s - loss: 0.5852 - accuracy: 0.9803\n",
      "Batch 00939: setting learning rate to 0.014006799494275385.\n",
      "4992/9600 [==============>...............] - ETA: 9s - loss: 0.5852 - accuracy: 0.9806\n",
      "Batch 00940: setting learning rate to 0.01393200377505466.\n",
      "5120/9600 [===============>..............] - ETA: 8s - loss: 0.5853 - accuracy: 0.9805\n",
      "Batch 00941: setting learning rate to 0.013857208055833934.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.5850 - accuracy: 0.9808\n",
      "Batch 00942: setting learning rate to 0.013782412336613223.\n",
      "5376/9600 [===============>..............] - ETA: 8s - loss: 0.5848 - accuracy: 0.9810\n",
      "Batch 00943: setting learning rate to 0.013707616617392497.\n",
      "5504/9600 [================>.............] - ETA: 8s - loss: 0.5851 - accuracy: 0.9807\n",
      "Batch 00944: setting learning rate to 0.013632820898171772.\n",
      "5632/9600 [================>.............] - ETA: 7s - loss: 0.5852 - accuracy: 0.9808\n",
      "Batch 00945: setting learning rate to 0.013558025178951046.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.5847 - accuracy: 0.9812\n",
      "Batch 00946: setting learning rate to 0.013483229459730321.\n",
      "5888/9600 [=================>............] - ETA: 7s - loss: 0.5847 - accuracy: 0.9811\n",
      "Batch 00947: setting learning rate to 0.013408433740509595.\n",
      "6016/9600 [=================>............] - ETA: 7s - loss: 0.5858 - accuracy: 0.9806\n",
      "Batch 00948: setting learning rate to 0.01333363802128887.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.5853 - accuracy: 0.9806\n",
      "Batch 00949: setting learning rate to 0.013258842302068145.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.5846 - accuracy: 0.9810\n",
      "Batch 00950: setting learning rate to 0.01318404658284742.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 0.5844 - accuracy: 0.9812\n",
      "Batch 00951: setting learning rate to 0.013109250863626706.\n",
      "6528/9600 [===================>..........] - ETA: 6s - loss: 0.5849 - accuracy: 0.9812\n",
      "Batch 00952: setting learning rate to 0.01303445514440598.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.5849 - accuracy: 0.9811\n",
      "Batch 00953: setting learning rate to 0.012959659425185255.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.5853 - accuracy: 0.9810\n",
      "Batch 00954: setting learning rate to 0.01288486370596453.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.5861 - accuracy: 0.9805\n",
      "Batch 00955: setting learning rate to 0.012810067986743804.\n",
      "7040/9600 [=====================>........] - ETA: 5s - loss: 0.5855 - accuracy: 0.9808\n",
      "Batch 00956: setting learning rate to 0.012735272267523079.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.5851 - accuracy: 0.9810\n",
      "Batch 00957: setting learning rate to 0.012660476548302354.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.5849 - accuracy: 0.9811\n",
      "Batch 00958: setting learning rate to 0.012585680829081628.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.5856 - accuracy: 0.9809\n",
      "Batch 00959: setting learning rate to 0.012510885109860903.\n",
      "7552/9600 [======================>.......] - ETA: 4s - loss: 0.5855 - accuracy: 0.9808\n",
      "Batch 00960: setting learning rate to 0.012436089390640191.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.5859 - accuracy: 0.9806\n",
      "Batch 00961: setting learning rate to 0.012361293671419466.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.5858 - accuracy: 0.9803\n",
      "Batch 00962: setting learning rate to 0.01228649795219874.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.5857 - accuracy: 0.9802\n",
      "Batch 00963: setting learning rate to 0.012211702232978013.\n",
      "8064/9600 [========================>.....] - ETA: 3s - loss: 0.5858 - accuracy: 0.9803\n",
      "Batch 00964: setting learning rate to 0.012136906513757288.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.5861 - accuracy: 0.9800\n",
      "Batch 00965: setting learning rate to 0.012062110794536562.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.5862 - accuracy: 0.9799\n",
      "Batch 00966: setting learning rate to 0.011987315075315837.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.5861 - accuracy: 0.9800\n",
      "Batch 00967: setting learning rate to 0.011912519356095112.\n",
      "8576/9600 [=========================>....] - ETA: 2s - loss: 0.5863 - accuracy: 0.9798\n",
      "Batch 00968: setting learning rate to 0.011837723636874386.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.5862 - accuracy: 0.9800\n",
      "Batch 00969: setting learning rate to 0.011762927917653675.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.5869 - accuracy: 0.9796\n",
      "Batch 00970: setting learning rate to 0.01168813219843295.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.5865 - accuracy: 0.9798\n",
      "Batch 00971: setting learning rate to 0.011613336479212224.\n",
      "9088/9600 [===========================>..] - ETA: 1s - loss: 0.5862 - accuracy: 0.9800\n",
      "Batch 00972: setting learning rate to 0.011538540759991498.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.5867 - accuracy: 0.9797\n",
      "Batch 00973: setting learning rate to 0.011463745040770773.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.5870 - accuracy: 0.9795\n",
      "Batch 00974: setting learning rate to 0.011388949321550048.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.5871 - accuracy: 0.9795\n",
      "Batch 00975: setting learning rate to 0.011314153602329322.\n",
      "9600/9600 [==============================] - 20s 2ms/step - loss: 0.5870 - accuracy: 0.9795 - val_loss: 0.1842 - val_accuracy: 0.9725\n",
      "Epoch 14/15\n",
      "\n",
      "Batch 00976: setting learning rate to 0.011239357883108597.\n",
      " 128/9600 [..............................] - ETA: 20s - loss: 0.5656 - accuracy: 0.9844\n",
      "Batch 00977: setting learning rate to 0.011164562163887884.\n",
      " 256/9600 [..............................] - ETA: 19s - loss: 0.5766 - accuracy: 0.9805\n",
      "Batch 00978: setting learning rate to 0.011089766444667158.\n",
      " 384/9600 [>.............................] - ETA: 19s - loss: 0.5692 - accuracy: 0.9870\n",
      "Batch 00979: setting learning rate to 0.011014970725446433.\n",
      " 512/9600 [>.............................] - ETA: 18s - loss: 0.5741 - accuracy: 0.9863\n",
      "Batch 00980: setting learning rate to 0.010940175006225707.\n",
      " 640/9600 [=>............................] - ETA: 18s - loss: 0.5748 - accuracy: 0.9875\n",
      "Batch 00981: setting learning rate to 0.010865379287004982.\n",
      " 768/9600 [=>............................] - ETA: 17s - loss: 0.5799 - accuracy: 0.9844\n",
      "Batch 00982: setting learning rate to 0.010790583567784257.\n",
      " 896/9600 [=>............................] - ETA: 17s - loss: 0.5854 - accuracy: 0.9810\n",
      "Batch 00983: setting learning rate to 0.010715787848563531.\n",
      "1024/9600 [==>...........................] - ETA: 17s - loss: 0.5831 - accuracy: 0.9824\n",
      "Batch 00984: setting learning rate to 0.010640992129342806.\n",
      "1152/9600 [==>...........................] - ETA: 17s - loss: 0.5816 - accuracy: 0.9818\n",
      "Batch 00985: setting learning rate to 0.01056619641012208.\n",
      "1280/9600 [===>..........................] - ETA: 16s - loss: 0.5827 - accuracy: 0.9820\n",
      "Batch 00986: setting learning rate to 0.010491400690901369.\n",
      "1408/9600 [===>..........................] - ETA: 16s - loss: 0.5824 - accuracy: 0.9815\n",
      "Batch 00987: setting learning rate to 0.010416604971680643.\n",
      "1536/9600 [===>..........................] - ETA: 16s - loss: 0.5806 - accuracy: 0.9831\n",
      "Batch 00988: setting learning rate to 0.010341809252459918.\n",
      "1664/9600 [====>.........................] - ETA: 15s - loss: 0.5819 - accuracy: 0.9826\n",
      "Batch 00989: setting learning rate to 0.01026701353323919.\n",
      "1792/9600 [====>.........................] - ETA: 15s - loss: 0.5817 - accuracy: 0.9821\n",
      "Batch 00990: setting learning rate to 0.010192217814018465.\n",
      "1920/9600 [=====>........................] - ETA: 15s - loss: 0.5824 - accuracy: 0.9828\n",
      "Batch 00991: setting learning rate to 0.01011742209479774.\n",
      "2048/9600 [=====>........................] - ETA: 15s - loss: 0.5830 - accuracy: 0.9824\n",
      "Batch 00992: setting learning rate to 0.010042626375577015.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 0.5819 - accuracy: 0.9830\n",
      "Batch 00993: setting learning rate to 0.00996783065635629.\n",
      "2304/9600 [======>.......................] - ETA: 14s - loss: 0.5813 - accuracy: 0.9831\n",
      "Batch 00994: setting learning rate to 0.009893034937135564.\n",
      "2432/9600 [======>.......................] - ETA: 14s - loss: 0.5817 - accuracy: 0.9827\n",
      "Batch 00995: setting learning rate to 0.009818239217914852.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560/9600 [=======>......................] - ETA: 14s - loss: 0.5830 - accuracy: 0.9816\n",
      "Batch 00996: setting learning rate to 0.009743443498694127.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 0.5830 - accuracy: 0.9818\n",
      "Batch 00997: setting learning rate to 0.009668647779473401.\n",
      "2816/9600 [=======>......................] - ETA: 13s - loss: 0.5838 - accuracy: 0.9819\n",
      "Batch 00998: setting learning rate to 0.009593852060252676.\n",
      "2944/9600 [========>.....................] - ETA: 13s - loss: 0.5837 - accuracy: 0.9820\n",
      "Batch 00999: setting learning rate to 0.00951905634103195.\n",
      "3072/9600 [========>.....................] - ETA: 13s - loss: 0.5856 - accuracy: 0.9811\n",
      "Batch 01000: setting learning rate to 0.009444260621811225.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 0.5865 - accuracy: 0.9803\n",
      "Batch 01001: setting learning rate to 0.0093694649025905.\n",
      "3328/9600 [=========>....................] - ETA: 12s - loss: 0.5859 - accuracy: 0.9808\n",
      "Batch 01002: setting learning rate to 0.009294669183369774.\n",
      "3456/9600 [=========>....................] - ETA: 12s - loss: 0.5850 - accuracy: 0.9812\n",
      "Batch 01003: setting learning rate to 0.009219873464149047.\n",
      "3584/9600 [==========>...................] - ETA: 12s - loss: 0.5849 - accuracy: 0.9810\n",
      "Batch 01004: setting learning rate to 0.009145077744928336.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 0.5845 - accuracy: 0.9811\n",
      "Batch 01005: setting learning rate to 0.00907028202570761.\n",
      "3840/9600 [===========>..................] - ETA: 11s - loss: 0.5834 - accuracy: 0.9815\n",
      "Batch 01006: setting learning rate to 0.008995486306486885.\n",
      "3968/9600 [===========>..................] - ETA: 11s - loss: 0.5837 - accuracy: 0.9816\n",
      "Batch 01007: setting learning rate to 0.00892069058726616.\n",
      "4096/9600 [===========>..................] - ETA: 11s - loss: 0.5832 - accuracy: 0.9819\n",
      "Batch 01008: setting learning rate to 0.008845894868045434.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 0.5832 - accuracy: 0.9818\n",
      "Batch 01009: setting learning rate to 0.008771099148824709.\n",
      "4352/9600 [============>.................] - ETA: 10s - loss: 0.5843 - accuracy: 0.9809\n",
      "Batch 01010: setting learning rate to 0.008696303429603983.\n",
      "4480/9600 [=============>................] - ETA: 10s - loss: 0.5844 - accuracy: 0.9808\n",
      "Batch 01011: setting learning rate to 0.008621507710383258.\n",
      "4608/9600 [=============>................] - ETA: 10s - loss: 0.5838 - accuracy: 0.9807\n",
      "Batch 01012: setting learning rate to 0.008546711991162546.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.5841 - accuracy: 0.9804 \n",
      "Batch 01013: setting learning rate to 0.008471916271941821.\n",
      "4864/9600 [==============>...............] - ETA: 9s - loss: 0.5845 - accuracy: 0.9803\n",
      "Batch 01014: setting learning rate to 0.008397120552721094.\n",
      "4992/9600 [==============>...............] - ETA: 9s - loss: 0.5840 - accuracy: 0.9806\n",
      "Batch 01015: setting learning rate to 0.008322324833500368.\n",
      "5120/9600 [===============>..............] - ETA: 9s - loss: 0.5846 - accuracy: 0.9807\n",
      "Batch 01016: setting learning rate to 0.008247529114279643.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.5843 - accuracy: 0.9806\n",
      "Batch 01017: setting learning rate to 0.008172733395058918.\n",
      "5376/9600 [===============>..............] - ETA: 8s - loss: 0.5842 - accuracy: 0.9805\n",
      "Batch 01018: setting learning rate to 0.008097937675838192.\n",
      "5504/9600 [================>.............] - ETA: 8s - loss: 0.5836 - accuracy: 0.9807\n",
      "Batch 01019: setting learning rate to 0.008023141956617467.\n",
      "5632/9600 [================>.............] - ETA: 8s - loss: 0.5849 - accuracy: 0.9803\n",
      "Batch 01020: setting learning rate to 0.007948346237396741.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.5857 - accuracy: 0.9799\n",
      "Batch 01021: setting learning rate to 0.00787355051817603.\n",
      "5888/9600 [=================>............] - ETA: 7s - loss: 0.5863 - accuracy: 0.9796\n",
      "Batch 01022: setting learning rate to 0.0077987547989553044.\n",
      "6016/9600 [=================>............] - ETA: 7s - loss: 0.5864 - accuracy: 0.9797\n",
      "Batch 01023: setting learning rate to 0.007723959079734579.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.5859 - accuracy: 0.9800\n",
      "Batch 01024: setting learning rate to 0.007649163360513853.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.5854 - accuracy: 0.9804\n",
      "Batch 01025: setting learning rate to 0.007574367641293127.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 0.5847 - accuracy: 0.9808\n",
      "Batch 01026: setting learning rate to 0.007499571922072402.\n",
      "6528/9600 [===================>..........] - ETA: 6s - loss: 0.5851 - accuracy: 0.9802\n",
      "Batch 01027: setting learning rate to 0.0074247762028516765.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.5863 - accuracy: 0.9799\n",
      "Batch 01028: setting learning rate to 0.007349980483630951.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.5859 - accuracy: 0.9801\n",
      "Batch 01029: setting learning rate to 0.007275184764410226.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.5859 - accuracy: 0.9800\n",
      "Batch 01030: setting learning rate to 0.007200389045189513.\n",
      "7040/9600 [=====================>........] - ETA: 5s - loss: 0.5866 - accuracy: 0.9795\n",
      "Batch 01031: setting learning rate to 0.007125593325968788.\n",
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.5871 - accuracy: 0.9792\n",
      "Batch 01032: setting learning rate to 0.0070507976067480625.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.5871 - accuracy: 0.9790\n",
      "Batch 01033: setting learning rate to 0.006976001887527337.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.5872 - accuracy: 0.9789\n",
      "Batch 01034: setting learning rate to 0.006901206168306612.\n",
      "7552/9600 [======================>.......] - ETA: 4s - loss: 0.5875 - accuracy: 0.9784\n",
      "Batch 01035: setting learning rate to 0.006826410449085886.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.5879 - accuracy: 0.9784\n",
      "Batch 01036: setting learning rate to 0.006751614729865161.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.5881 - accuracy: 0.9784\n",
      "Batch 01037: setting learning rate to 0.0066768190106444355.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.5878 - accuracy: 0.9785\n",
      "Batch 01038: setting learning rate to 0.006602023291423709.\n",
      "8064/9600 [========================>.....] - ETA: 3s - loss: 0.5875 - accuracy: 0.9785\n",
      "Batch 01039: setting learning rate to 0.006527227572202998.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.5876 - accuracy: 0.9784\n",
      "Batch 01040: setting learning rate to 0.006452431852982272.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.5875 - accuracy: 0.9787\n",
      "Batch 01041: setting learning rate to 0.006377636133761547.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.5872 - accuracy: 0.9789\n",
      "Batch 01042: setting learning rate to 0.0063028404145408215.\n",
      "8576/9600 [=========================>....] - ETA: 2s - loss: 0.5872 - accuracy: 0.9790\n",
      "Batch 01043: setting learning rate to 0.006228044695320096.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.5868 - accuracy: 0.9792\n",
      "Batch 01044: setting learning rate to 0.00615324897609937.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.5872 - accuracy: 0.9791\n",
      "Batch 01045: setting learning rate to 0.006078453256878644.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.5871 - accuracy: 0.9791\n",
      "Batch 01046: setting learning rate to 0.006003657537657919.\n",
      "9088/9600 [===========================>..] - ETA: 1s - loss: 0.5871 - accuracy: 0.9793\n",
      "Batch 01047: setting learning rate to 0.0059288618184372074.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.5867 - accuracy: 0.9795\n",
      "Batch 01048: setting learning rate to 0.005854066099216482.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.5866 - accuracy: 0.9795\n",
      "Batch 01049: setting learning rate to 0.005779270379995756.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.5864 - accuracy: 0.9797\n",
      "Batch 01050: setting learning rate to 0.00570447466077503.\n",
      "9600/9600 [==============================] - 21s 2ms/step - loss: 0.5859 - accuracy: 0.9798 - val_loss: 0.1784 - val_accuracy: 0.9787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15\n",
      "\n",
      "Batch 01051: setting learning rate to 0.005629678941554305.\n",
      " 128/9600 [..............................] - ETA: 20s - loss: 0.5705 - accuracy: 0.9688\n",
      "Batch 01052: setting learning rate to 0.0055548832223335795.\n",
      " 256/9600 [..............................] - ETA: 19s - loss: 0.5817 - accuracy: 0.9766\n",
      "Batch 01053: setting learning rate to 0.005480087503112854.\n",
      " 384/9600 [>.............................] - ETA: 18s - loss: 0.5886 - accuracy: 0.9792\n",
      "Batch 01054: setting learning rate to 0.005405291783892129.\n",
      " 512/9600 [>.............................] - ETA: 18s - loss: 0.5877 - accuracy: 0.9805\n",
      "Batch 01055: setting learning rate to 0.005330496064671403.\n",
      " 640/9600 [=>............................] - ETA: 17s - loss: 0.5803 - accuracy: 0.9844\n",
      "Batch 01056: setting learning rate to 0.005255700345450691.\n",
      " 768/9600 [=>............................] - ETA: 17s - loss: 0.5838 - accuracy: 0.9792\n",
      "Batch 01057: setting learning rate to 0.0051809046262299655.\n",
      " 896/9600 [=>............................] - ETA: 17s - loss: 0.5834 - accuracy: 0.9810\n",
      "Batch 01058: setting learning rate to 0.00510610890700924.\n",
      "1024/9600 [==>...........................] - ETA: 17s - loss: 0.5794 - accuracy: 0.9834\n",
      "Batch 01059: setting learning rate to 0.005031313187788515.\n",
      "1152/9600 [==>...........................] - ETA: 16s - loss: 0.5813 - accuracy: 0.9818\n",
      "Batch 01060: setting learning rate to 0.004956517468567789.\n",
      "1280/9600 [===>..........................] - ETA: 16s - loss: 0.5836 - accuracy: 0.9820\n",
      "Batch 01061: setting learning rate to 0.004881721749347064.\n",
      "1408/9600 [===>..........................] - ETA: 16s - loss: 0.5821 - accuracy: 0.9830\n",
      "Batch 01062: setting learning rate to 0.0048069260301263385.\n",
      "1536/9600 [===>..........................] - ETA: 16s - loss: 0.5835 - accuracy: 0.9824\n",
      "Batch 01063: setting learning rate to 0.004732130310905613.\n",
      "1664/9600 [====>.........................] - ETA: 15s - loss: 0.5815 - accuracy: 0.9826\n",
      "Batch 01064: setting learning rate to 0.004657334591684887.\n",
      "1792/9600 [====>.........................] - ETA: 15s - loss: 0.5852 - accuracy: 0.9810\n",
      "Batch 01065: setting learning rate to 0.004582538872464175.\n",
      "1920/9600 [=====>........................] - ETA: 15s - loss: 0.5829 - accuracy: 0.9823\n",
      "Batch 01066: setting learning rate to 0.00450774315324345.\n",
      "2048/9600 [=====>........................] - ETA: 15s - loss: 0.5802 - accuracy: 0.9834\n",
      "Batch 01067: setting learning rate to 0.0044329474340227245.\n",
      "2176/9600 [=====>........................] - ETA: 14s - loss: 0.5803 - accuracy: 0.9830\n",
      "Batch 01068: setting learning rate to 0.004358151714801999.\n",
      "2304/9600 [======>.......................] - ETA: 14s - loss: 0.5812 - accuracy: 0.9822\n",
      "Batch 01069: setting learning rate to 0.004283355995581273.\n",
      "2432/9600 [======>.......................] - ETA: 14s - loss: 0.5807 - accuracy: 0.9823\n",
      "Batch 01070: setting learning rate to 0.004208560276360547.\n",
      "2560/9600 [=======>......................] - ETA: 14s - loss: 0.5828 - accuracy: 0.9812\n",
      "Batch 01071: setting learning rate to 0.004133764557139822.\n",
      "2688/9600 [=======>......................] - ETA: 13s - loss: 0.5822 - accuracy: 0.9818\n",
      "Batch 01072: setting learning rate to 0.0040589688379190966.\n",
      "2816/9600 [=======>......................] - ETA: 13s - loss: 0.5826 - accuracy: 0.9819\n",
      "Batch 01073: setting learning rate to 0.003984173118698371.\n",
      "2944/9600 [========>.....................] - ETA: 13s - loss: 0.5832 - accuracy: 0.9817\n",
      "Batch 01074: setting learning rate to 0.00390937739947766.\n",
      "3072/9600 [========>.....................] - ETA: 13s - loss: 0.5842 - accuracy: 0.9811\n",
      "Batch 01075: setting learning rate to 0.0038345816802569338.\n",
      "3200/9600 [=========>....................] - ETA: 12s - loss: 0.5832 - accuracy: 0.9819\n",
      "Batch 01076: setting learning rate to 0.0037597859610362084.\n",
      "3328/9600 [=========>....................] - ETA: 12s - loss: 0.5836 - accuracy: 0.9814\n",
      "Batch 01077: setting learning rate to 0.0036849902418154825.\n",
      "3456/9600 [=========>....................] - ETA: 12s - loss: 0.5839 - accuracy: 0.9809\n",
      "Batch 01078: setting learning rate to 0.003610194522594757.\n",
      "3584/9600 [==========>...................] - ETA: 12s - loss: 0.5849 - accuracy: 0.9802\n",
      "Batch 01079: setting learning rate to 0.0035353988033740317.\n",
      "3712/9600 [==========>...................] - ETA: 11s - loss: 0.5834 - accuracy: 0.9806\n",
      "Batch 01080: setting learning rate to 0.0034606030841533063.\n",
      "3840/9600 [===========>..................] - ETA: 11s - loss: 0.5829 - accuracy: 0.9807\n",
      "Batch 01081: setting learning rate to 0.003385807364932581.\n",
      "3968/9600 [===========>..................] - ETA: 11s - loss: 0.5834 - accuracy: 0.9803\n",
      "Batch 01082: setting learning rate to 0.003311011645711869.\n",
      "4096/9600 [===========>..................] - ETA: 11s - loss: 0.5831 - accuracy: 0.9807\n",
      "Batch 01083: setting learning rate to 0.003236215926491143.\n",
      "4224/9600 [============>.................] - ETA: 10s - loss: 0.5822 - accuracy: 0.9813\n",
      "Batch 01084: setting learning rate to 0.0031614202072704177.\n",
      "4352/9600 [============>.................] - ETA: 10s - loss: 0.5824 - accuracy: 0.9812\n",
      "Batch 01085: setting learning rate to 0.0030866244880496923.\n",
      "4480/9600 [=============>................] - ETA: 10s - loss: 0.5828 - accuracy: 0.9810\n",
      "Batch 01086: setting learning rate to 0.003011828768828967.\n",
      "4608/9600 [=============>................] - ETA: 10s - loss: 0.5836 - accuracy: 0.9809\n",
      "Batch 01087: setting learning rate to 0.002937033049608241.\n",
      "4736/9600 [=============>................] - ETA: 9s - loss: 0.5835 - accuracy: 0.9808 \n",
      "Batch 01088: setting learning rate to 0.0028622373303875156.\n",
      "4864/9600 [==============>...............] - ETA: 9s - loss: 0.5831 - accuracy: 0.9811\n",
      "Batch 01089: setting learning rate to 0.0027874416111667902.\n",
      "4992/9600 [==============>...............] - ETA: 9s - loss: 0.5828 - accuracy: 0.9810\n",
      "Batch 01090: setting learning rate to 0.002712645891946065.\n",
      "5120/9600 [===============>..............] - ETA: 9s - loss: 0.5824 - accuracy: 0.9812\n",
      "Batch 01091: setting learning rate to 0.002637850172725353.\n",
      "5248/9600 [===============>..............] - ETA: 8s - loss: 0.5824 - accuracy: 0.9809\n",
      "Batch 01092: setting learning rate to 0.0025630544535046275.\n",
      "5376/9600 [===============>..............] - ETA: 8s - loss: 0.5821 - accuracy: 0.9810\n",
      "Batch 01093: setting learning rate to 0.0024882587342839016.\n",
      "5504/9600 [================>.............] - ETA: 8s - loss: 0.5821 - accuracy: 0.9811\n",
      "Batch 01094: setting learning rate to 0.002413463015063176.\n",
      "5632/9600 [================>.............] - ETA: 8s - loss: 0.5820 - accuracy: 0.9810\n",
      "Batch 01095: setting learning rate to 0.002338667295842451.\n",
      "5760/9600 [=================>............] - ETA: 7s - loss: 0.5821 - accuracy: 0.9807\n",
      "Batch 01096: setting learning rate to 0.0022638715766217254.\n",
      "5888/9600 [=================>............] - ETA: 7s - loss: 0.5819 - accuracy: 0.9808\n",
      "Batch 01097: setting learning rate to 0.0021890758574009996.\n",
      "6016/9600 [=================>............] - ETA: 7s - loss: 0.5811 - accuracy: 0.9812\n",
      "Batch 01098: setting learning rate to 0.002114280138180274.\n",
      "6144/9600 [==================>...........] - ETA: 6s - loss: 0.5807 - accuracy: 0.9814\n",
      "Batch 01099: setting learning rate to 0.0020394844189595487.\n",
      "6272/9600 [==================>...........] - ETA: 6s - loss: 0.5805 - accuracy: 0.9815\n",
      "Batch 01100: setting learning rate to 0.0019646886997388368.\n",
      "6400/9600 [===================>..........] - ETA: 6s - loss: 0.5804 - accuracy: 0.9816\n",
      "Batch 01101: setting learning rate to 0.0018898929805181112.\n",
      "6528/9600 [===================>..........] - ETA: 6s - loss: 0.5804 - accuracy: 0.9816\n",
      "Batch 01102: setting learning rate to 0.0018150972612973857.\n",
      "6656/9600 [===================>..........] - ETA: 5s - loss: 0.5801 - accuracy: 0.9818\n",
      "Batch 01103: setting learning rate to 0.0017403015420766603.\n",
      "6784/9600 [====================>.........] - ETA: 5s - loss: 0.5803 - accuracy: 0.9819\n",
      "Batch 01104: setting learning rate to 0.0016655058228559347.\n",
      "6912/9600 [====================>.........] - ETA: 5s - loss: 0.5802 - accuracy: 0.9821\n",
      "Batch 01105: setting learning rate to 0.0015907101036352093.\n",
      "7040/9600 [=====================>........] - ETA: 5s - loss: 0.5803 - accuracy: 0.9821\n",
      "Batch 01106: setting learning rate to 0.0015159143844144837.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7168/9600 [=====================>........] - ETA: 4s - loss: 0.5804 - accuracy: 0.9821\n",
      "Batch 01107: setting learning rate to 0.0014411186651937583.\n",
      "7296/9600 [=====================>........] - ETA: 4s - loss: 0.5807 - accuracy: 0.9819\n",
      "Batch 01108: setting learning rate to 0.0013663229459730327.\n",
      "7424/9600 [======================>.......] - ETA: 4s - loss: 0.5807 - accuracy: 0.9820\n",
      "Batch 01109: setting learning rate to 0.0012915272267523207.\n",
      "7552/9600 [======================>.......] - ETA: 4s - loss: 0.5807 - accuracy: 0.9820\n",
      "Batch 01110: setting learning rate to 0.0012167315075315953.\n",
      "7680/9600 [=======================>......] - ETA: 3s - loss: 0.5807 - accuracy: 0.9819\n",
      "Batch 01111: setting learning rate to 0.0011419357883108697.\n",
      "7808/9600 [=======================>......] - ETA: 3s - loss: 0.5802 - accuracy: 0.9822\n",
      "Batch 01112: setting learning rate to 0.0010671400690901443.\n",
      "7936/9600 [=======================>......] - ETA: 3s - loss: 0.5801 - accuracy: 0.9822\n",
      "Batch 01113: setting learning rate to 0.0009923443498694188.\n",
      "8064/9600 [========================>.....] - ETA: 3s - loss: 0.5805 - accuracy: 0.9820\n",
      "Batch 01114: setting learning rate to 0.0009175486306486932.\n",
      "8192/9600 [========================>.....] - ETA: 2s - loss: 0.5802 - accuracy: 0.9823\n",
      "Batch 01115: setting learning rate to 0.0008427529114279677.\n",
      "8320/9600 [=========================>....] - ETA: 2s - loss: 0.5804 - accuracy: 0.9822\n",
      "Batch 01116: setting learning rate to 0.0007679571922072423.\n",
      "8448/9600 [=========================>....] - ETA: 2s - loss: 0.5806 - accuracy: 0.9820\n",
      "Batch 01117: setting learning rate to 0.0006931614729865302.\n",
      "8576/9600 [=========================>....] - ETA: 2s - loss: 0.5808 - accuracy: 0.9818\n",
      "Batch 01118: setting learning rate to 0.0006183657537658048.\n",
      "8704/9600 [==========================>...] - ETA: 1s - loss: 0.5811 - accuracy: 0.9816\n",
      "Batch 01119: setting learning rate to 0.0005435700345450793.\n",
      "8832/9600 [==========================>...] - ETA: 1s - loss: 0.5809 - accuracy: 0.9818\n",
      "Batch 01120: setting learning rate to 0.0004687743153243538.\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.5810 - accuracy: 0.9817\n",
      "Batch 01121: setting learning rate to 0.0003939785961036283.\n",
      "9088/9600 [===========================>..] - ETA: 1s - loss: 0.5808 - accuracy: 0.9818\n",
      "Batch 01122: setting learning rate to 0.00031918287688290276.\n",
      "9216/9600 [===========================>..] - ETA: 0s - loss: 0.5809 - accuracy: 0.9818\n",
      "Batch 01123: setting learning rate to 0.00024438715766217725.\n",
      "9344/9600 [============================>.] - ETA: 0s - loss: 0.5810 - accuracy: 0.9819\n",
      "Batch 01124: setting learning rate to 0.00016959143844145176.\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.5812 - accuracy: 0.9818\n",
      "Batch 01125: setting learning rate to 9.479571922072626e-05.\n",
      "9600/9600 [==============================] - 21s 2ms/step - loss: 0.5817 - accuracy: 0.9817 - val_loss: 0.1654 - val_accuracy: 0.9742\n",
      "Acc -> Val acc: 0.0213,Test (last) acc: 0.0208\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0213 in 6.2 min\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.021250009536743164"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clases = 100 if dataset == 'cifar100' else 10\n",
    "dm = DataManager(dataset, clases=classes, folder_var_mnist=data_folder, num_clases=num_clases) #, max_examples=8000)\n",
    "data = dm.load_data()\n",
    "fitness_cnn.set_params(data=data, verbose=True, batch_size=batch_size, reduce_plateau=redu_plat,\n",
    "               epochs=epochs, cosine_decay=cosine_dec, early_stop=early_stop, \n",
    "               warm_epochs=warm_up_epochs, base_lr=base_lr, smooth_label=smooth, find_lr=lr_find,\n",
    "               precise_epochs=precise_eps, include_time=include_time, test_eps=test_eps, augment=augment)\n",
    "\n",
    "fitness_cnn.calc(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file ../../exp_mnist_grow_v2/cifar10/genetic/0_2020-01-31-15:39/GA_experiment\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "# experiments_folder = '../../exp_cifar10_time'\n",
    "experiments_folder = '../../exp_mnist_grow_v2'\n",
    "dataset = datasets[0]\n",
    "exp_folder = os.path.join(experiments_folder, dataset)\n",
    "folder = os.path.join(exp_folder, 'genetic')\n",
    "\n",
    "\n",
    "generational = TwoLevelGA.load_genetic_algorithm(folder=folder)\n",
    "# generational.finishing_evolution(show=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||CNN|F:1.1|K:5|A:relu|D:0.05||woCAT||1||\n",
      "||CNN|F:1.0|K:5|A:relu|D:0.05||CAT||11||\n",
      "||CNN|F:0.9|K:3|A:prelu|D:0.10||CAT||101||\n",
      "HP->|GR:4.20|CELL:2|BLOCK:2|STEM:45|LR:0.0112|WU:0.2\n",
      "\n",
      "0.9375290000000001\n",
      "0.9523 0.0477\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "winner = generational.best_individual['winner']\n",
    "fit = generational.best_individual['best_fit']\n",
    "test = generational.best_individual[\"test\"]\n",
    "print(winner)\n",
    "print(1 - fit)\n",
    "print(1 - test, test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 32, 32, 3) train samples\n",
      "(10000, 32, 32, 3) validation samples\n",
      "(10000, 32, 32, 3) test samples\n",
      "cutout\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "num_clases = 100 if dataset == 'cifar100' else 10\n",
    "dm = DataManager(dataset, clases=classes, folder_var_mnist=data_folder, num_clases=num_clases) #, max_examples=8000)\n",
    "data = dm.load_data()\n",
    "fitness_cnn.set_params(data=data, verbose=verbose, batch_size=batch_size, reduce_plateau=redu_plat,\n",
    "               epochs=epochs, cosine_decay=cosine_dec, early_stop=early_stop, \n",
    "               warm_epochs=warm_up_epochs, base_lr=base_lr, smooth_label=smooth, find_lr=lr_find,\n",
    "               precise_epochs=precise_eps, include_time=include_time, test_eps=test_eps, augment=augment)\n",
    "print(fitness_cnn.augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 45)   1260        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 45)   180         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 32, 45)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 63)   70938       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 108)  0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 108)  432         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 32, 108)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 76)   205276      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 121)  0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 121)  484         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32, 32, 121)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 80)   87200       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 32, 32, 80)   81920       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 80)   320         p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32, 32, 80)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 129)  258129      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 209)  0           p_re_lu_1[0][0]                  \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 209)  836         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32, 32, 209)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 155)  810030      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 235)  0           p_re_lu_1[0][0]                  \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 235)  940         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32, 32, 235)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 165)  349140      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_2 (PReLU)               (None, 32, 32, 165)  168960      conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 165)  0           p_re_lu_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 165)  660         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 16, 16, 165)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 265)  1093390     dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 16, 16, 430)  0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 430)  1720        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 16, 16, 430)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 319)  3429569     dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 16, 16, 484)  0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 484)  1936        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 16, 16, 484)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 339)  1477023     dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_3 (PReLU)               (None, 16, 16, 339)  86784       conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 339)  1356        p_re_lu_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 16, 16, 339)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 544)  4610944     dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 16, 16, 883)  0           p_re_lu_3[0][0]                  \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 883)  3532        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 16, 16, 883)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 653)  14415628    dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 16, 16, 992)  0           p_re_lu_3[0][0]                  \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 992)  3968        concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 16, 16, 992)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 695)  6205655     dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_4 (PReLU)               (None, 16, 16, 695)  177920      conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 695)          0           p_re_lu_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           6960        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 33,553,090\n",
      "Trainable params: 33,544,908\n",
      "Non-trainable params: 8,182\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 1.5698 - accuracy: 0.5159 - val_loss: 2.3153 - val_accuracy: 0.2674\n",
      "Epoch 2/200\n",
      "50000/50000 [==============================] - 144s 3ms/step - loss: 1.2089 - accuracy: 0.7019 - val_loss: 0.8119 - val_accuracy: 0.7321\n",
      "Epoch 3/200\n",
      "50000/50000 [==============================] - 144s 3ms/step - loss: 1.0332 - accuracy: 0.7871 - val_loss: 0.6999 - val_accuracy: 0.7789\n",
      "Epoch 4/200\n",
      "50000/50000 [==============================] - 144s 3ms/step - loss: 0.8998 - accuracy: 0.8480 - val_loss: 0.5612 - val_accuracy: 0.8155\n",
      "Epoch 5/200\n",
      "50000/50000 [==============================] - 144s 3ms/step - loss: 0.8045 - accuracy: 0.8878 - val_loss: 0.5582 - val_accuracy: 0.8309\n",
      "Epoch 6/200\n",
      "50000/50000 [==============================] - 144s 3ms/step - loss: 0.7263 - accuracy: 0.9231 - val_loss: 0.6148 - val_accuracy: 0.8140\n",
      "Epoch 7/200\n",
      "  640/50000 [..............................] - ETA: 2:16 - loss: 0.6863 - accuracy: 0.9359"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ed43e591233e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfitness_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfitness_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitness_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwinner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Daniel/NeuroEvolution/utils/codification_cnn.py\u001b[0m in \u001b[0;36mcalc\u001b[0;34m(self, chromosome, test, file_model, fp, precise_mode)\u001b[0m\n\u001b[1;32m    565\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m                               verbose=self.verb)\n\u001b[0m\u001b[1;32m    568\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m                 \u001b[0;31m# model = load_model(file_model, {'BatchNormalizationF16': BatchNormalizationF16})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/genetic/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/genetic/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/genetic/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/genetic/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/genetic/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/genetic/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/genetic/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fitness_cnn.verb = True\n",
    "fitness_cnn.augment = False\n",
    "score = fitness_cnn.calc(winner, test=True, precise_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
